\RequirePackage[hyphens]{url}

\documentclass{elsarticle}


%
% Packages
%

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage{listings}
\usepackage{amsmath}
\allowdisplaybreaks
\usepackage{amsfonts}
\usepackage{amstext}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{empheq}
\usepackage{cases}
\usepackage{anyfontsize}
\usepackage{enumitem}
\usepackage{pdfpages}
\usepackage{fourier}	% Style
\usepackage{bm}
\usepackage{epstopdf}
\usepackage{lipsum}
%\usepackage{authblk}
\usepackage[top=3cm, bottom=3cm, left=2cm, right=2cm, scale=0.75]{geometry}	% Set the margins
%\usepackage[inner=3cm, textwidth=445pt, scale=0.75, top=3cm, bottom=3cm]{geometry}	% Set the margins
\usepackage{fancyhdr}
\usepackage[letterspace=150]{microtype}
\usepackage{textcomp}
\usepackage{gensymb}
\usepackage{booktabs}
\usepackage{amsmath,etoolbox}
\usepackage{mathtools}
\usepackage{anyfontsize}
%\usepackage{enumerate}
\usepackage{graphicx}
\usepackage{epstopdf}
\usepackage{float}
\usepackage{subfig}
\usepackage[labelfont=bf,labelsep=period,font=small]{caption}
%\usepackage{subcaption}
\usepackage{newunicodechar}
\usepackage{nicefrac}	% For diagonal fractions
\usepackage{bbm}
\usepackage{csvsimple}
%\usepackage{floatrow}	% For notes below a figure

% Set header and footer
\usepackage{fancyhdr}

% Packages needed for tables
\usepackage{longtable}
\usepackage{multicol}
\usepackage{multirow}
\usepackage{array}

\PassOptionsToPackage{hyphens}{url}\usepackage{hyperref}
\usepackage{breakurl}
\usepackage{url}

% To put footnotes at the bottom of the page
\usepackage[bottom]{footmisc}

\usepackage{empheq}

\usepackage{tikz}

% Default fixed font does not support bold face
\DeclareFixedFont{\ttb}{T1}{txtt}{bx}{n}{10.25} % for bold
\DeclareFixedFont{\ttm}{T1}{txtt}{m}{n}{10.25}  % for normal

% Custom colors
\usepackage{color}
\definecolor{deepblue}{rgb}{0,0,0.5}
\definecolor{deepred}{rgb}{0.6,0,0}
\definecolor{deepgreen}{rgb}{0,0.5,0}

% To insert code snippets
\usepackage{listings}

% For argmin
\DeclareMathOperator*{\argmin}{arg\,min}

% To insert verbatim within a command
\usepackage{fancyvrb}

% For pseudocode
\usepackage[section]{algorithm}
\usepackage{algpseudocode}

\usepackage[many]{tcolorbox}

\usepackage{stackengine}

% Equations enumeration options
\numberwithin{equation}{section}

% Set interline
\usepackage{setspace}
%\onehalfspacing

% Path to images
\graphicspath{{./img/}}


%
% Definitions
%

% To enumerate subequations with arabic numbers (e.g. 1.1, 1.2, ecc)
%\numberwithin{equation}{chapter}

\DeclarePairedDelimiter\abs{\lvert}{\rvert}
\makeatletter
\let\oldabs\abs
\def\abs{\@ifstar{\oldabs}{\oldabs*}}

% Theorem and definition environment
\theoremstyle{theorem}
\newtheorem{theorem}{Theorem}[section]
\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]
\theoremstyle{remark}
\newtheorem{remark}{Remark}[section]
\theoremstyle{proposition}
\newtheorem{proposition}{Proposition}[section]
%\newenvironment{definition}[1][Definition]{\begin{trivlist}
%\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}

% To enumerate the equations and the figures according to the section they are in
%\numberwithin{equation}{section}
\numberwithin{figure}{section}

% To modify the space between figure and caption
%\setlength{\abovecaptionskip}{-4pt}
%\setlength{\belowcaptionskip}{3pt}

\renewcommand{\textfraction}{0.1}
\renewcommand{\topfraction}{0.9}

\makeatletter
	\renewcommand*\l@figure{\@dottedtocline{1}{1em}{3.2em}}
\makeatother

% Define norm symbol
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}

% Define mod symbol
\newcommand{\Mod}[1]{\ (\mathrm{mod}\ #1)}

% Aliases for \boldsymbol and \widetilde
\newcommand{\wt}[1]{\widetilde{#1}}
\newcommand{\bg}[1]{\boldsymbol{#1}}

% Redefine \Require and \Ensure for algorithm environment
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}

% Make \big| adapt to the context
\makeatletter
\let\amstexbig\big
\def\newbig#1{%
  \ifx#1|%
    \expandafter\@firstoftwo
  \else
    \expandafter\@secondoftwo
  \fi
  {\big@bar}%
  {\amstexbig{#1}}%
}
\AtBeginDocument{\let\big\newbig}
\def\big@bar{\bBigg@{1.1}|}
\makeatother

% Define the do-while loop
\algdef{SE}[DOWHILE]{DoWhile}{EndDoWhile}{\algorithmicdo}[1]{\algorithmicwhile\ #1}

%\newcommand{\NewPage}{\newpage\null\thispagestyle{empty}\newpage}


%
% Document
%

\begin{document}

	%
	% Frontmatter
	%
	
	\begin{frontmatter}
		\title{Reduced order modeling of nonlinear problems using neural networks}
		
		\author{J.~S.~Hesthaven}
		\ead{jan.hesthaven@epfl.ch}
		
		\author{S.~Ubbiali}
		\ead{stefano.ubbiali@epfl.ch}
		
		\address{\'Ecole Polytechnique F\'ed\'erale de Lausanne (EPFL), CH-1015 Lausanne, Switzerland}
		
		\begin{abstract}
			In this work, we develop a non-intrusive reduced basis (RB) method for pa\-ra\-me\-tri\-zed time-indepedent partial differential equations (PDEs). The proposed method extracts a reduced basis from a collection of high-fidelity solutions via proper orthogonal decomposition (POD) and employs artificial neural networks (ANNs), particularly multi-layer perceptrons (MLPs), to accurately approximate the coefficients of the reduced model. The search for the optimal number of inner neurons and the minimum amount of training samples which avoids overfitting is carried out in the offline phase through an automatic routine, relying upon a joint use of the latin hypercube sampling (LHS) and the Levenberg-Marquardt training algorithm. This guarantees a complete offline-online decoupling, leading to an efficient RB method - referred to as POD-NN - suitable also for nonlinear problems featuring a non-affine parametric dependence. Numerical studies are presented for the nonlinear Poisson equation and for driven cavity viscous flows, modeled through the steady uncompressible Navier-Stokes equations. Both physical and geometrical parametrizations are considered. Several results confirm the accuracy of the POD-NN method and show the substantial speed-up enabled at the online stage with respect to a traditional RB strategy based on the Galerkin projection process.
		\end{abstract}
		
		\begin{keyword}
			non-intrusive reduced basis method \sep proper orthogonal decomposition \sep multi-layer perceptron \sep Levenberg-Marquardt algorithm \sep Poisson equation \sep driven cavity flow
		\end{keyword}
	\end{frontmatter}
	
	
	%
	% Introduction
	%
	
	\section{Introduction}
	\label{section:Introduction}
		
		Several applications arising in engineering and applied sciences involve mathematical models expressed as parametrized partial differential equations (PDEs), in which boundary conditions, material properties, source terms, loads or geometrical factors of the underlying physical problem are addressed to a parameter $\bg{\mu}$ \cite{Eft08, HSR16, JIR14}. A list of notable examples includes parameter estimation \cite{Bro93}, topology optimization \cite{Ben04}, optimal control \cite{LM67} and uncertainty quantification \cite{LeM10}. In these contexts, one is typically interested in a real-time evaluation of an \emph{output of interest} (defined as a functional of the state variable \cite{Dep08}) for many parameter entries, i.e., for many configurations of the problem.  
		
		The continuously growing availability of computational power and the simultaneous algorithmic improvements make possible nowadays the \emph{high-fidelity} numerical resolution of complex problems via standard discretization procedures, such as finite difference (FD), finite volume (FV), finite element (FE), or spectral methods \cite{QMN15}. However, these schemes remain prohibitevely expensive in many-query and real-time contexts, both in terms of CPU time and memory demand. This follows from the large amount of degrees of freedom (DOFs) they imply, resulting from the (fine) spatial discretization needed to accurately solve the underpinning PDE \cite{Ams10}. In light of this, \emph{reduced order modeling} (ROM) methods have received a significant attention in the last decades. The objective of these methods is to replace the full-order system by one of significant smaller dimension, thus to decrease the computational burden while leading to a reasonable loss of accuracy \cite{Chen17}. 
		
		\emph{Reduced basis} (RB) methods constitute a well-known and widely-used instance of reduced order modeling techniques. They are generally implemented pursuing an offline-online paradigm \cite{Mad06}. Based upon an ensemble of \emph{snapshots} (i.e., high-fidelity solutions to the parametrized differential problem), the goal of the \emph{offline} step is to construct a solution-dependent basis, yielding a reduced space of globally approximating functions capable of representing the main dynamics of the full-order model \cite{Bal14, Chen17}. For this, two major approaches have been proposed in the literature: proper orthogonal decomposition (POD) \cite{Lia02, Vol08} and greedy algorithms \cite{HSZ14}. The former relies on a deterministic or random sample to generate the snapshots and then employs a singular value decomposition (SVD) to recover the reduced basis. Whereas, in the latter approach the basis vectors coincide with the snapshots themselves, carefully selected according to some optimality criterion. As a result, a greedy strategy is typically more effective and efficient than POD, as it enables the exploration of a wider region of the parameter space while entailing the computation of many fewer high-fidelity solutions \cite{HSR16}. However, there may exist problems for which a greedy approach is not feasible, simply because a natural criterion for the choice of the snapshots is not available \cite{Bal14}.
		
		Once a reduced-order framework has been properly set up, given a new parameter input, an approximation to the \emph{truth} solution is sought \emph{online} as a linear combination of the RB functions, with the expansion coefficients determined via projection of the full-order system onto the reduced space \cite{Buf12}. To this end, a Galerkin procedure is the most popular choice. 
		
		Despite their established effectiveness, for complex nonlinear problems with a non-affine dependence on the parameters, projection-based RB methods do not provide any computational gain with respect to a direct (expensive) approach, as the cost to compute the projection coefficients depends on the dimension of the full-order model. In fact, a full decoupling between the online stage and the high-fidelity scheme is the ultimate secret for the success of any RB procedure \cite{QMN15}. For this purpose, one may recover an affine expansion of the differential operator through the empirical interpolation method (EIM) \cite{Bar04} or its discrete variants \cite{Cha10, NMA15}. 
		
		A valuable alternative to address this concern is represented by \emph{non-intrusive} RB methods, which refer to the high-fidelity model solely to generate the snapshots, without involving it in the projection process \cite{Chen17}. The projection coefficients are then obtained via interpolation over the parameter domain of a database of reduced-order information \cite{Cas15}. However, since reduced bases generally belong to nonlinear, matrix manifolds, standard interpolation techniques may fail, as they cannot enforce the constraints characterizing those manifolds, unless using a large amount of samples \cite{Ams10, BNR00}. 
		
		In this work, we develop a non-intrusive RB method employing POD for the generation of the reduced basis and resorting to (artificial) neural networks, in particular multi-layer perceptrons, in the interpolation step. Hence, in the following we refer to the proposed RB procedure as the POD-NN method. Being of non-intrusive nature, POD-NN is suitable for a fast and reliable resolution of complex nonlinear PDEs featuring a non-affine parametric dependence. To test this assertion, the POD-NN method is applied to the one- and two-dimensional nonlinear Poisson equation and to the steady uncompressible Navier-Stokes equations. Both physical and geometrical parametrizations are considered.
		
		\iffalse
		As their biological counterpart, \emph{artificial neural networks} (ANNs) \cite{Hay05} are computing systems consisting of information-processing units, called \emph{neurons}, interconnected through \emph{weighted synapses}. The attractive feature of ANNs lies in their capability of \emph{learning} from experience \cite{Kri07}. As a result, ANNs have found most used in those applications, such as cluster detection, image identification or speech recognition, which are difficult to address via computer algorithms using rule-based programming \cite{Mat16}. However, ANNs have been successfully applied also in more traditional contexts, e.g., continuous function approximation, as a valid alternative to the programming approach to investigate complexity, nonlinearity and uncertainties of a high order \cite{Nie15}. 
		
		The learning of an ANN is accomplished through a \emph{training} process, during which the network is exposed to a collection of examples and its weights are properly adjusted so that it can provide reasonable responses in similar (yet not identical) situations. As shown later in the paper, the training and the subsequent evaluation of a neural network perfectly fit the offline-online framework, offering the POD-NN method a significant online speed-up with respect to the standard projection-based POD-Garlerkin (POD-G) RB procedure.
		\fi
		
		The paper is organized as follows. Section \ref{section:Parametrized partial differential equations} defines the (parametrized) functional and variational framework which is required to develop a finite element solver, briefly outlined in Subsection \ref{section:Discrete full-order model}. The standard projection-based POD-Galerkin (POD-G) RB method is then derived in Section \ref{section:Projection-based reduced basis method}. Section \ref{section:Artificial neural networks} discusses components, topology and learning process for the artificial neural networks used in the simulations. This is preparatory for the subsequent Section \ref{section:A non-intrusive RB method using neural networks}, which details the non-intrusive POD-NN RB procedure; both theoretical and practical aspects are addressed. Several numerical results, aiming to show the reliability and efficiency of the proposed RB technique, are offered in Section \ref{section:Numerical results} for the Poisson equation (Subsection \ref{section:Nonlinear Poisson equation}) and the lid-driven cavity problem for the steady Navier-Stokes equations (Subsection \ref{section:Steady uncompressible Navier-Stokes equations}). Finally, Section \ref{section:Conclusion} gathers some relevant conclusions and suggests future developments.
		
		
	%
	% Section 2 : Parametrized partial differential equations
	%
		
	\section{Parametrized partial differential equations}
	\label{section:Parametrized partial differential equations}
	
		Assume $\mathcal{P}_{ph} \subset \mathbb{R}^{P_{ph}}$ and $\mathcal{P}_g \subset \mathbb{R}^{P_{g}}$ be compact sets, and let $\boldsymbol{\mu}_{ph} \in \mathcal{P}_{ph}$ and $\boldsymbol{\mu}_{g} \in \mathcal{P}_{g}$ be respectively the \emph{physical} and \emph{geometrical} parameters characterizing the differential problem, so that $\boldsymbol{\mu} = (\boldsymbol{\mu}_{ph} \, , \boldsymbol{\mu}_{g}) \in \mathcal{P} = \mathcal{P}_{ph} \times \mathcal{P}_g \subset \mathbb{R}^P$, $P = P_{ph} + P_g$, represents the overall \emph{input vector parameter}. While $\bg{\mu}_{ph}$ addresses material properties, source terms and boundary conditions, $\bg{\mu}_g$ defines the shape of the computational domain $\wt{\Omega} = \wt{\Omega}(\boldsymbol{\mu}_g) \subset \mathbb{R}^d$, $d = 1,2$. We denote by $\wt{\Gamma}(\boldsymbol{\mu}_g) = \partial \wt{\Omega}(\boldsymbol{\mu}_g)$ the (Lipschitz) boundary of $\wt{\Omega}(\boldsymbol{\mu}_g)$, and by $\wt{\Gamma}_D(\boldsymbol{\mu}_g)$ and $\wt{\Gamma}_N(\boldsymbol{\mu}_g)$ the portions of $\wt{\Gamma}(\boldsymbol{\mu}_g)$ where Dirichlet and Neumann boundary conditions are enforced, respectively, with $\wt{\Gamma}_D \cup \wt{\Gamma}_N = \wt{\Gamma}$ and $\mathring{\wt{\Gamma}}_D \cap \mathring{\wt{\Gamma}}_N = \emptyset$.
		
		Consider then a Hilbert space $\wt{V} = \wt{V}(\bg{\mu}_g) = \wt{V}(\wt{\Omega}(\boldsymbol{\mu}_g))$ defined over the domain $\wt{\Omega}(\boldsymbol{\mu}_g)$, equipped with the scalar product $(\cdot, \cdot)_{\wt{V}}$ and the induced norm $\norm{\cdot}_{\wt{V}} = \sqrt{(\cdot, \cdot)_{\wt{V}}}$. Furthermore, let $\wt{V}' = \wt{V}'(\boldsymbol{\mu}_g)$ be the dual space of $\wt{V}$. Denoting by $\wt{G} ~ : ~ \wt{V} \times \mathcal{P}_{ph} \rightarrow \wt{V}'$ the map representing a parametrized nonlinear second-order PDE, the differential (strong) form of the problem of interest reads: given $\bg{\mu} = (\bg{\mu}_{ph}, \, \bg{\mu}_g) \in \mathcal{P}$, find $\wt{u}(\boldsymbol{\mu}) \in \wt{V}(\bg{\mu}_g)$ such that
		\begin{equation}
			\label{eq:pde-differential-form}
			\wt{G}(\wt{u}(\bg{\mu}); \, \bg{\mu}_{ph}) = 0 \hspace*{0.3cm} \text{in $\wt{V}'(\bg{\mu}_g)$} \, ,
		\end{equation}
		namely
		\begin{equation*}
			\langle \wt{G}(\wt{u}(\bg{\mu}); \, \bg{\mu}_{ph}), \, v \rangle_{\wt{V}',\wt{V}} = 0 \hspace*{0.3cm} \forall v \in \wt{V}(\bg{\mu}_g) \, ,
		\end{equation*}
		with $\langle \cdot, \, \cdot \rangle_{\wt{V}',\wt{V}} ~ : ~ \wt{V}' \times \wt{V} \rightarrow \mathbb{R}$ the duality pairing between $\wt{V}'$ and $\wt{V}$. %, encoding the action of any functional of $\wt{V}'$ onto elements of $\wt{V}$.
		
		The finite element method requires problem \eqref{eq:pde-differential-form} to be stated in a weak (or variational) form \cite{Qua10}. To this end, let us introduce the form $\wt{g} ~ : ~ \wt{V} \times \wt{V} \times \mathcal{P} \rightarrow \mathbb{R}$, with $\wt{g}(\cdot, \, \cdot; \, \bg{\mu})$ defined as:
		\begin{equation*}
			\wt{g}(w, \, v; \, \bg{\mu}) = \langle \wt{G}(w; \, \bg{\mu}_{ph}), \, v \rangle_{\wt{V}',\wt{V}} \hspace*{0.3cm} \forall w, \, v \in \wt{V} \, .
		\end{equation*}
		The variational formulation of \eqref{eq:pde-differential-form} then reads: given $\bg{\mu} = (\bg{\mu}_{ph}, \, \bg{\mu}_g) \in \mathcal{P}$, find $\wt{u}(\bg{\mu}) \in \wt{V}(\bg{\mu}_g)$ such that
		\begin{equation*}
			\label{eq:pde-variational-form}
			\wt{g}(\wt{u}(\bg{\mu}), \, v; \, \bg{\mu}) = 0 \hspace*{0.3cm} \forall v \in \wt{V}(\bg{\mu}_g) \, .
		\end{equation*}
		%Note that the definition of $\wt{g}(\cdot, \, \cdot; \, \bg{\mu})$ relies on the duality pairing $\langle \cdot, \, \cdot \rangle_{\wt{V}',\wt{V}}$ between $\wt{V}$ and $\wt{V}'$. Hence, from the nonlinearity of $\wt{G}(\cdot; \, \bg{\mu}_{ph})$ follows the nonlinearity of $\wt{g}(\cdot, \, \cdot; \, \bg{\mu})$ with respect to its first argument.
		
		
	%
	% Subsection 2.1 : From physical to reference domain
	%
		
	\subsection{From physical to reference domain}
	\label{section:From physical to reference domain}
	
		As anticipated in the Introduction, any reduced basis method seeks an approximated solution to a differential problem as a combination of (few) well-chosen basis vectors. These typically result from a suitable combination of a collection of high-fidelity approximations, called \emph{snapshots}. Therefore, when addressing problems defined on variable shape domains, ensuring the \emph{compatibility} among snapshots is crucial. To this end, it is common practice to formulate and solve the differential problem over a fixed, \emph{parameter-independent} domain $\Omega \subset \mathbb{R}^d$ \cite{MN16}. This can be accomplished upon introducing a parametrized map $\bg{\Phi} ~ : ~ \Omega \times \mathcal{P}_g \rightarrow \wt{\Omega}$ such that
		\begin{equation*}
			\label{eq:parametrized-map}
			\wt{\Omega}(\bg{\mu}_g) = \bg{\Phi}(\Omega; \, \bg{\mu}_g) \, .
		\end{equation*}
		The transformation $\bg{\Phi}(\cdot; \bg{\mu}_g)$ allows to restate the general problem \eqref{eq:pde-differential-form}. Let $V$ be a suitable Hilbert space over $\Omega$ and $V'$ be its dual. Suppose $V$ is equipped with the scalar product $(\cdot, \cdot)_V$ and the induced norm $\norm{\cdot}_V = \sqrt{(\cdot, \cdot)_V}$. Given the parametrized map $G ~ : V \times \mathcal{P} \rightarrow V'$ representing the (nonlinear) PDE over the reference domain $\Omega$, we focus on differential problems of the form: given $\bg{\mu} \in \mathcal{P}$, find $u(\bg{\mu}) \in V$ such that
		\begin{equation}
			\label{eq:pde-differential-reference}
			G(u(\bg{\mu}); \, \bg{\mu}) = 0 \hspace*{0.3cm} \text{in $V'$} \, .
		\end{equation}
		The weak formulation of problem \eqref{eq:pde-differential-reference} reads: given $\bg{\mu} \in \mathcal{P}$, seek $u(\bg{\mu}) \in V$ such that
		\begin{equation}
			\label{eq:pde-weak-reference}
			g(u(\bg{\mu}), \, v; \, \bg{\mu}) = 0 \hspace*{0.3cm} \forall v \in V \, ,
		\end{equation}
		where $g ~ : ~ V \times V \times \mathcal{P} \rightarrow \mathbb{R}$ is defined as
		\begin{equation*}
			g(w, \, v; \, \bg{\mu}) = \langle G(w; \, \bg{\mu}), \, v \rangle_{V',V} \hspace*{0.3cm} \forall w, \, v \in V \, , ~ \forall \bg{\mu} \in \mathcal{P} \, ,
		\end{equation*}
		with $\langle \cdot, \, \cdot \rangle_{V',V} ~ : ~ V' \times V \rightarrow \mathbb{R}$ the dual pairing between $V$ and $V'$. Observe that the explicit expression of $g(\cdot, \, \cdot; \, \bg{\mu})$ involves the map $\bg{\Phi}(\cdot; \bg{\mu}_g)$, thus keeping track of the original domain $\wt{\Omega}(\bg{\mu}_g)$. Then, the solution $\wt{u}(\bg{\mu})$ over the original domain $\wt{\Omega}(\bg{\mu}_g)$ can be recovered as
		\begin{equation*}
			\wt{u}(\bg{\mu}) = u(\bg{\mu}) \circ \bg{\Phi}(\bg{\mu}_g) \, .
		\end{equation*}
		%For any $\bg{\mu} \in \mathcal{P}$ we seek a discrete solution $u_h(\bg{\mu})$ to the problem \eqref{eq:pde-differential-reference} on a \emph{parameter-independent} cover $\Omega_h$ of the domain $\Omega$. Provided a convenient choice for $\Omega$, this makes the mesh generation process easier. In addition, we note that discretizing the problem \eqref{eq:pde-differential-reference} over $\Omega_h$ is equivalent to approximating the original problem \eqref{eq:pde-differential-form} over the mesh $\wt{\Omega}_h(\bg{\mu}_g)$, given by
		%\begin{equation*}
		%	\label{eq:parametrized-map-discrete}
		%	\wt{\Omega}_h(\bg{\mu}_g) = \bg{\Phi}(\Omega_h; \, \bg{\mu}_g) \, .
		%\end{equation*}
		In our numerical tests, we employ the squared reference domain shown on the right in Fig. \ref{fig:poisson2d-fig1} and we resort to a particular choice for $\bg{\Phi}(\cdot; \bg{\mu}_g)$ - the boundary displacement-dependent transfinite map (BDD TM) proposed by Jaggli \emph{et al}. \cite{JIR14}.
		
		
	%
	% Subsection 2.2 : Discrete full-order model
	%
	
	\subsection{Discrete full-order model}
	\label{section:Discrete full-order model}
	
		Let $V_h \subset V$ be a suitable FE subspace of $V$ of (finite) dimension $N_h$, with $h \geq 0$ being the characteristic size of the underlying mesh $\Omega_h$ which discretizes the domain $\Omega$. The FE approximation of the weak problem \eqref{eq:pde-weak-reference} can be cast in the form: given $\bg{\mu} \in \mathcal{P}$, find $u_h(\bg{\mu}) \in V_h$ such that 
		\begin{equation}
			\label{eq:galerkin}
			g(u_h(\bg{\mu}), \, v_h; \, \bg{\mu}) = 0 \hspace*{0.3cm} \forall v_h \in V_h \, .
		\end{equation}
		The discretization \eqref{eq:galerkin} is known as \emph{Galerkin approximation}, and therefore $u_h(\bg{\mu})$ is referred to as the \emph{Galerkin solution} to the problem \eqref{eq:pde-differential-reference}. Due to the nonlinearity of $g(\cdot, \, \cdot; \, \bg{\mu})$ in its first argument, one has to resort to some iterative method, e.g., Newton's method, to solve the Galerkin problem \eqref{eq:galerkin}. In this regard, let \[ dg[z](\cdot, \, \cdot; \, \bg{\mu}) ~ : ~ V \times V \rightarrow \mathbb{R} \]
		be the partial Frech\'et derivative of $g(\cdot, \, \cdot; \, \bg{\mu})$ with respect to its first argument, evaluated at $z \in V$. Starting from an initial guess $u_h^0(\bg{\mu})$, we construct a collection of approximations $\big\lbrace u_h^k(\bg{\mu}) \big\rbrace_{k \geq 0}$ to the Galerkin solution $u_h(\bg{\mu})$ by iteratively solving the linearized problems
		\begin{equation*}
			\label{eq:newton-linearized-problem}
			dg \big[ u_h^k(\bg{\mu}) \big](\delta u_h^k(\bg{\mu}), \, v_h; \, \bg{\mu}) = - g(u_h^k(\bg{\mu}), \, v_h; \, \bg{\mu}) \hspace*{0.3cm} \forall v_h \in V_h
		\end{equation*}
		in the unknown $\delta u_h^k(\bg{\mu}) \in V_h$, and then setting $u_h^{k+1}(\bg{\mu}) = u_h^k(\bg{\mu}) + \delta u_h^k(\bg{\mu})$.
		
		To derive the algebraic counterpart of the Galerkin-Newton method, let $\big\lbrace \phi_1, \, \ldots \, , \phi_{N_h} \big\rbrace$ be a basis for the ${N_h}$-dimensional space $V_h$, so that the solution $u_h(\bg{\mu})$ can be expressed as a linear combination of the basis functions, i.e.,
		\begin{equation*}
			\label{eq:galerkin-solution}
			u_h(\bg{x}; \, \bg{\mu}) = \sum_{j = 1}^{N_h} u_h^{(j)}(\bg{\mu}) ~ \phi_j(\bg{x}) \, .
		\end{equation*} 
		Hence, denoting by $\mathbf{u}_h(\bg{\mu}) \in \mathbb{R}^{N_h}$ the vector collecting the \emph{degrees} \emph{of} \emph{freedom} $\big\lbrace u_h^{(j)} \big\rbrace_{1 \leq j \leq N_h}$ and exploiting the linearity of $g(\cdot, \, \cdot; \, \bg{\mu})$ in the second argument, the problem \eqref{eq:galerkin} is equivalent to: given $\bg{\mu} \in \mathcal{P}$, find $\mathbf{u}_h(\bg{\mu}) \in \mathbb{R}^{N_h}$ such that
		\begin{equation}
			\label{eq:galerkin-nonlinear-system}
			\mathbf{G}_h (\mathbf{u}_h(\bg{\mu}); \, \bg{\mu}) = \bg{0} \in \mathbb{R}^{N_h} \, ,
		\end{equation}
		where the $i$-th component of the \emph{residual vector} $\mathbf{G}(\cdot; \, \bg{\mu}) \in \mathbb{R}^{N_h}$ is given by
		\begin{equation}
			\label{eq:galerkin-nonlinear-system-equation}
			\left( \mathbf{G}_h(\mathbf{u}_h(\bg{\mu}); \, \bg{\mu}) \right)_i = g \left( \sum_{j = 1}^{N_h} u_h^{(j)}(\bg{\mu}) ~ \phi_j, \, \phi_i; \, \bg{\mu} \right) \, , \hspace*{0.3cm} i = 1, \, \ldots \, , {N_h} \, .
		\end{equation}
		Then, for $k \geq 0$, the $k$-th iteration of Newton's method applied to the system \eqref{eq:galerkin-nonlinear-system} entails the resolution of the \emph{linear} system
		\begin{equation}
			\label{eq:galerkin-linear-system}
			\mathbb{J}_h \big( \mathbf{u}^k_h(\bg{\mu}); \, \bg{\mu} \big) ~ \delta \mathbf{u}^k_h(\bg{\mu}) = - \mathbf{G}_h \big( \mathbf{u}^k_h(\bg{\mu}); \, \bg{\mu} \big) \, , \hspace*{0.3cm} \delta \mathbf{u}^k_h(\bg{\mu}) \in \mathbb{R}^{N_h} \, ,
		\end{equation}
		so that $\mathbf{u}^{k+1}_h(\bg{\mu}) = \mathbf{u}^k_h(\bg{\mu}) + \delta \mathbf{u}^k_h(\bg{\mu})$. Here, $\mathbb{J}_h(\cdot; \, \bg{\mu}) \in \mathbb{R}^{{N_h} \times {N_h}}$ denotes the Jacobian of the residual vector $\mathbf{G}_h(\cdot; \, \bg{\mu})$; exploiting the bilinearity of $dg[z](\cdot, \, \cdot; \, \bg{\mu})$, $\mathbb{J}_h(\cdot; \, \bg{\mu})$ is defined as
		\begin{equation*}
			\left( \mathbb{J}_h \big( \mathbf{u}^k_h(\bg{\mu}); \, \bg{\mu} \big) \right)_{i,j} = dg\big[u^k_h(\bg{\mu})\big](\phi_j, \, \phi_i; \, \bg{\mu}) \, , \hspace*{0.3cm} i, \, j = 1, \, \ldots \, , {N_h} \, .
		\end{equation*} 
		
		
	%
	% Section 3 : Projection-based RB method
	%
	
	\section{Projection-based reduced basis method}
	\label{section:Projection-based reduced basis method}
	
		As detailed in the previous section, the finite element discretization of the $\bg{\mu}$-dependent nonlinear differential problem \eqref{eq:pde-weak-reference}, combined with Newton's method, entails the assembly and resolution of (possibly) many linear systems of the form \eqref{eq:galerkin-linear-system}, whose dimension $N_h$ is directly related to $(i)$ the size of the underlying grid and $(ii)$ the order of the polynomial FE space adopted. Since the accuracy of the resulting discretization heavily relies on these two factors, a direct numerical approximation of the full-order model implies severe computational costs. Therefore, this approach is hardly affordable in \emph{many-query} and \emph{real-time} contexts, even resorting to high-performance parallel workstations. %where one is interested in a fast and reliable prediction of an \emph{output of interest}, i.e, a functional of the field variable $u(\bg{\mu})$, for many instances of $\bg{\mu} \in \mathcal{P}$ \cite{Dep08}. 
		This motivates the broad use of \emph{reduced-order} models across several inter-disciplinary areas, e.g., parameter estimation, optimal control, shape optimization and uncertainty quantification \cite{HSR16, QMN15}. 
						
		Reduced basis methods seek an approximated solution to problem \eqref{eq:pde-weak-reference} as a linear combination of parameter-independent functions $\big\lbrace \psi_1, \, \ldots \, , \psi_L \big\rbrace \subset V_h$, called \emph{reduced basis functions}, built from a collection of high-fidelity snapshots $\big\lbrace u_h \left( \bg{\mu}^{(1)} \right), \, \ldots \, , u_h \left( \bg{\mu}^{(N)} \right) \big\rbrace$, where the discrete and finite set $\Xi_N = \big\lbrace \bg{\mu}^{(1)}, \, \ldots \, , \bg{\mu}^{(N)} \big\rbrace \subset \mathcal{P}$ may consist of either a uniform lattice or randomly generated points over the parameter domain $\mathcal{P}$ \cite{HSR16}. The basis functions \smash{$\big\lbrace \psi_l \big\rbrace_{1 \leq l \leq L}$} generally follow from a principal component analysis (PCA) of the set of snapshots (in that case, $N > L$), or they might coincide with the snapshots themselves (in that case, $N = L$). In the latter approach, typical of any \emph{greedy} method, the parameters \smash{$\big\lbrace \bg{\mu}^{(n)} \big\rbrace_{1 \leq n \leq N}$} must be carefully chosen according to some optimality criterium (see, e.g., \cite{Chen17}). Here, we pursue the first approach, employing the well-known proper orthogonal decomposition (POD) method \cite{Lia02, Vol08}, detailed in the following subsection.
		
		Assume now that a reduced basis is available and let $V_{\texttt{rb}} \subset V_h$ be the associated \emph{reduced basis space}, i.e.,
		\begin{equation*}
			V_{\texttt{rb}} = \text{span} \big\lbrace \psi_1, \, \ldots \, , \psi_L \big\rbrace \, .
		\end{equation*} 
		A \emph{reduced basis solution} $u_{L}(\bg{\mu})$ is sought in the form
		\begin{equation*}
			\label{eq:rb-solution}
			u_{L}(\bg{x}; \, \bg{\mu}) = \sum_{l = 1}^L u_{\texttt{rb}}^{(l)}(\bg{\mu}) ~ \psi_l(\bg{x}) ~ \in ~ V_{\texttt{rb}} \, ,
		\end{equation*}
		with \[ \mathbf{u}_{\texttt{rb}}(\bg{\mu}) = \big[ u_{\texttt{rb}}^{(1)}(\bg{\mu}), \, \ldots \, , u_{\texttt{rb}}^{(L)}(\bg{\mu}) \big]^T \in \mathbb{R}^L \] be the \emph{reduced coefficients} (also called \emph{generalized coordinates}) for the expansion of the RB solution in the RB basis functions. To unearth $u_L(\bg{\mu})$, we proceed to project the variational problem \eqref{eq:pde-weak-reference} onto the RB space $V_{\texttt{rb}}$ by pursuing a standard Galerkin approach, leading to the following \emph{reduced basis problem}: given $\bg{\mu} \in \mathcal{P}$, find $u_L(\bg{\mu}) \in V_{\texttt{rb}}$ so that
		\begin{equation}
			\label{eq:pde-rb}
			g(u_L(\bg{\mu}), \, v_L; \, \bg{\mu}) = 0 \hspace*{0.3cm} \forall v_L \in V_{\texttt{rb}} \, .
		\end{equation}
		Then, Newton's method applied to \eqref{eq:pde-rb} entails, at each iteration $k \geq 0$, the solution of the linearized problem: given $\bg{\mu} \in \mathcal{P}$, seek $\delta u_{L}^k(\bg{\mu})$ such that
		\begin{equation*}
			\label{eq:pde-rb-newton}
			dg \big[ u_{L}^k(\bg{\mu}) \big] \big( \delta u_{L}^k(\bg{\mu}), \, v_{L}; \, \bg{\mu} \big) = - g \big( u_{L}^k(\bg{\mu}), \, v_{L}; \, \bg{\mu} \big) \hspace*{0.3cm} \forall v_{L} \in V_{\texttt{rb}} \, ,
		\end{equation*}
		with $u_{L}^{k+1}(\bg{\mu}) = u_{L}^k(\bg{\mu}) + \delta u_{L}^k(\bg{\mu})$. At this stage, it is worth pointing out that the RB functions $\big\lbrace \psi_l \big\rbrace_{1 \leq l \leq L}$ belong to $V_h$, i.e., they are actual FE functions. Hence, denoting by $\bg{\psi}_l \in \mathbb{R}^{N_h}$ the vector collecting the nodal values of $\psi_l$, for $l = 1, \, \ldots \, , L$, let us introduce the matrix \[ \mathbb{V} = \big[ \bg{\psi}_1 \, \big| \, \ldots \, \big| \, \bg{\psi}_L \big] \in \mathbb{R}^{{N_h} \times L} \, .\] For any $v_{L} \in V_{\texttt{rb}}$, the matrix $\mathbb{V}$ encodes the change of variables from the RB basis to the standard (Lagrangian) FE basis, i.e.,
		\begin{equation}
			\label{eq:rb-fe-coefficients}
			\mathbf{v}_L = \mathbb{V} ~ \mathbf{v}_{\texttt{rb}} \, .
		\end{equation}
		Therefore, each element $v_{L}$ of the reduced space admits two (algebraic) representations:
		\begin{itemize}
			\item $\mathbf{v}_{\texttt{rb}} \in \mathbb{R}^L$, collecting the coefficients for the expansion of $v_{L}$ in terms of the RB basis $\big\lbrace \psi_1, \, \ldots \, , \psi_L \big\rbrace$;
			\item $\mathbf{v}_{L} \in \mathbb{R}^{N_h}$, collecting the coefficients for the expansion of $v_{L}$ in terms of the FE basis $\big\lbrace \phi_1, \, \ldots \, , \phi_{N_h} \big\rbrace$.
		\end{itemize}
		Note that the latter is also available for any $v_h \in V_h$, while the former characterizes the element in the subspace $V_{\texttt{rb}}$. Then, due to \eqref{eq:galerkin-nonlinear-system-equation}, \eqref{eq:rb-fe-coefficients} and the linearity of $g(\cdot,\cdot; \bg{\mu})$ in the second argument, the algebraic formulation of the reduced basis problem \eqref{eq:pde-rb} can be written in compact form as: given $\bg{\mu} \in \mathcal{P}$, seek $\mathbf{u}_{\texttt{rb}} \in \mathbb{R}^L$ such that
		\begin{equation}
			\label{eq:rb-nonlinear-system}
			\mathbf{G}_{\texttt{rb}}(\mathbf{u}_{\texttt{rb}}(\bg{\mu}); \, \bg{\mu}) = \mathbb{V}^T \mathbf{G}_h(\mathbf{u}_L(\bg{\mu}); \, \bg{\mu}) = \mathbb{V}^T \mathbf{G}_h(\mathbb{V} ~ \mathbf{u}_{\texttt{rb}}(\bg{\mu}); \, \bg{\mu}) = \bg{0} ~ \in ~ \mathbb{R}^L \, .
		\end{equation}
		Observe that this \emph{reduced nonlinear system} imposes the orthogonality (in the Euclidean scalar product) of the residual vector $\mathbf{G}_h(\cdot; \bg{\mu})$, evaluated in $\mathbb{V} \mathbf{u}_{\texttt{rb}}(\bg{\mu})$, to the columns of $\mathbb{V}$, thus encoding the Galerkin approach pursued at the variational level. Finally, exploiting the chain rule and the Jacobian $\mathbb{J}_h(\cdot; \bg{\mu})$ of $\mathbf{G}_h(\cdot; \bg{\mu})$, the Jacobian $\mathbb{J}_{\texttt{rb}}(\cdot; \bg{\mu})$ of $\mathbf{G}_{\texttt{rb}}(\cdot; \bg{\mu})$ is given by
		\begin{equation}
			\label{eq:rb-nonlinear-system-jacobian}
			\mathbb{J}_{\texttt{rb}}(\mathbf{w}; \, \bg{\mu}) = \mathbb{V}^T \mathbb{J}_h(\mathbb{V} ~ \mathbf{w}; \, \bg{\mu}) \, \mathbb{V} ~ \in ~ \mathbb{R}^{L \times L} \hspace*{0.3cm} \forall \mathbf{w} \in \mathbb{R}^L \, , \, \forall \bg{\mu} \in \mathcal{P} \, .
		\end{equation}
		Hence, starting from an initial guess $\mathbf{u}_{\texttt{rb}}^0 \in \mathbb{R}^L$, each iteration $k$, $k \geq 0$, of Newton's method applied to the reduced nonlinear system \eqref{eq:rb-nonlinear-system} entails the resolution of the linear system
		\begin{equation}
			\label{eq:rb-nonlinear-system-newton}
			\mathbb{J}_{\texttt{rb}}(\mathbf{u}_{\texttt{rb}}^k(\bg{\mu}); \, \bg{\mu}) ~ \delta \mathbf{u}_{\texttt{rb}}^k(\bg{\mu}) = - \mathbf{G}_{\texttt{rb}}(\mathbf{u}_{\texttt{rb}}^k(\bg{\mu}); \, \bg{\mu}) \, ,
		\end{equation}
		with $\mathbf{u}_{\texttt{rb}}^{k+1}(\bg{\mu}) = \mathbf{u}_{\texttt{rb}}^k(\bg{\mu}) + \delta \mathbf{u}_{\texttt{rb}}^k(\bg{\mu})$. Therefore, applying the POD-Galerkin RB method enables a dramatic reduction of the size of the linear systems to solve whenever the dimension $L$ of the reduced space $V_{\texttt{rb}}$ is much lower than the dimension ${N_h}$ of the underlying finite element space $V_h$. 
		
		%In the upcoming subsection, we detail the construction of a reduced basis via the POD method, highlighting its optimality properties and potential disadvantages.
		
	
	%
	% Subsection 3.1 : Proper orthogonal decomposition
	%
		
	\subsection{Proper orthogonal decomposition}
	\label{section:Proper Orthogonal Decomposition} 
		
		%In a general sense, \emph{Proper Orthogonal Decomposition} (POD) is a powerful method of data analysis aimed at reducing the cardinality of a given high-dimensional dataset (or system). First, an orthonormal basis for the original data space is generated, consisting of basis vectors called \emph{modes} or \emph{principal components}. Ideally, the first modes embody much of the \emph{energy} of the system, and so they express the \emph{essential information} of data \cite{QMN15}. Therefore, a meaningful low-dimensional representation of data is obtained by truncating the orthonormal basis to retain only a few POD modes, then projecting the system onto the truncated basis \cite{Vol08}. This approach perfectly fits our needs, as we shall see hereunder. However, it is clear that the interest in the POD method extends far beyond the field of reduced-order modeling techniques, finding a fertile ground in, e.g., random variables, image processing, and data compression \cite{Lia02}.
		
		Consider a collection of $N$ snapshots $\big\lbrace u_h \big( \bg{\mu}^{(1)} \big), \, \ldots \, , u_h \big( \bg{\mu}^{(N)} \big) \big\rbrace$, corresponding to the finite and discrete parameter set $\Xi_N = \big\lbrace \bg{\mu}^{(1)}, \, \ldots \, , \bg{\mu}^{(N)} \big\rbrace \subset \mathcal{P}$, and let $\mathcal{M}_{\Xi_N}$ be the associated subspace, i.e., \[ \mathcal{M}_{\Xi_N} = \text{span} \big\lbrace u_h \big( \bg{\mu}^{(1)} \big), \, \ldots \, , u_h \big( \bg{\mu}^{(N)} \big) \big\rbrace \, . \] We can assume that $\mathcal{M}_{\Xi_N}$ provides a good approximation of the \emph{discrete solution manifold} $\mathcal{M}_h$, \[ \mathcal{M}_h = \big\lbrace u_h(\bg{\mu}) ~ : ~ \bg{\mu} \in \mathcal{P} \big\rbrace \, , \] as long as the number of snapshots is sufficiently large (but typically much smaller than the dimension ${N_h}$ of the FE space). Then, we aim at finding a parameter-independent \emph{reduced basis} for $\mathcal{M}_{\Xi_N}$, i.e., a collection of FE functions $\big\lbrace \psi_1, \, \ldots \, , \psi_L \big\rbrace \subset \mathcal{M}_{\Xi_N}$, with $L \ll {N_h}$ and $L$ \emph{independent} of $N_h$, so that the associated linear space 
		\begin{equation*}
			V_{\texttt{rb}} = \text{span} \big\lbrace \psi_1, \, \ldots \, , \psi_L \big\rbrace
		\end{equation*}
		constitutes a low-rank approximation of $\mathcal{M}_{\Xi_N}$, optimal in some later defined sense. To this end, consider the \emph{snapshot matrix} $\mathbb{S} \in \mathbb{R}^{{N_h} \times N}$ gathering the nodal values of the snapshots in a column-wise sense, i.e.,
		\begin{equation*}
			\mathbb{S} = \big[ \mathbf{u}_h \big( \bg{\mu}^{(1)} \big) \, \big| \, \ldots \, \big| \, \mathbf{u}_h \big( \bg{\mu}^{(N)} \big) \big] \, .
		\end{equation*}
		Denoting by $R$ the rank of $\mathbb{S}$, with $R \leq \min \big\lbrace{ {N_h}, \, N \big\rbrace}$, the singular value decomposition (SVD) of $\mathbb{S}$ ensures the existence of two orthogonal matrices $\mathbb{W} = \big[ \mathbf{w}_1 \, \big| \, \ldots \, \big| \, \mathbf{w}_{N_h} \big] \in \mathbb{R}^{{N_h} \times {N_h}}$ and $\mathbb{Z} = \big[ \mathbf{z}_1 \, \big| \, \ldots \, \big| \, \mathbf{z}_N \big] \in \mathbb{R}^{N \times N}$, and a diagonal matrix $\mathbb{D} = \text{diag}(\sigma_1, \, \ldots \, , \sigma_R) \in \mathbb{R}^{R \times R}$, with $\sigma_1 \geq \sigma_2 \geq \ldots \geq \sigma_r > 0$, such that
		\begin{equation}
			\label{eq:svd}
			\mathbb{S} = \mathbb{W} 
			\begin{bmatrix}
			\hspace*{-0.15cm}
			\begin{array}{cc}
				\mathbb{D} & 0 \\
				0 & 0
			\end{array} 
			\hspace*{-0.15cm}
			\end{bmatrix}
			\mathbb{Z}^T = \mathbb{W} ~ \Sigma ~ \mathbb{Z}^T \, ,
		\end{equation}
		where the zeros denote null matrices of appropriate dimensions. The real values $\big\lbrace \sigma_i \big\rbrace_{1 \leq i \leq R}$ are called \emph{singular values} of $\mathbb{S}$, the columns $\big\lbrace \mathbf{w}_m \big\rbrace_{1 \leq m \leq N_h}$ of $\mathbb{W}$ are called \emph{left singular vectors} of $\mathbb{S}$, and the columns $\big\lbrace \mathbf{z}_n \big\rbrace_{1 \leq n \leq N}$ of $\mathbb{Z}$ are called \emph{right singular vectors} of $\mathbb{S}$, and they are related by the following relations:
		\begin{equation}
			\label{eq:svd-relationships}
			\begin{aligned}
				& \mathbb{S} \mathbb{S}^T \mathbf{w}_m = 
				\begin{cases}
					& \hspace*{-0.3cm} \sigma_m^2 ~ \mathbf{w}_m \hspace*{0.5cm} \text{for $1 \leq m \leq R$} \, , \\
					& \hspace*{-0.3cm} \mathrlap{\mathbf{0}}\hphantom{\sigma_m^2 ~ \mathbf{w}_m} \hspace*{0.5cm} \text{for $R+1 \leq m \leq {N_h}$} \, ,
				\end{cases}
				\qquad
				&& \mathbb{S}^T \mathbb{S} ~ \mathbf{z}_n =
				\begin{cases}
					& \hspace*{-0.3cm} \sigma_n^2 ~ \mathbf{z}_n \hspace*{0.74cm} \text{for $1 \leq n \leq R$} \, , \\
					& \hspace*{-0.3cm} \mathrlap{\mathbf{0}}\hphantom{\sigma_n^2 ~ \mathbf{z}_n} \hspace*{0.74cm} \text{for $R+1 \leq n \leq N$} \, ,
				\end{cases} \\
				& \mathbb{S} ~ \mathbf{z}_i = \sigma_i ~ \mathbf{w}_i \hspace*{0.5cm} \text{for $1 \leq i \leq R$} \, , 
				&& \mathbb{S}^T \mathbf{w}_i = \sigma_i ~ \mathbf{z}_i \hspace*{0.5cm} \text{for $1 \leq i \leq R$} \, .
			\end{aligned}
		\end{equation}
		%In particular, \eqref{eq:left-singular-vectors} and \eqref{eq:right-singular-vectors} state that the first $R$ columns of $\mathbb{W}$ and $\mathbb{Z}$ are eigenvectors of $\mathbb{S} \mathbb{S}^T$ and $\mathbb{S}^T \mathbb{S}$, respectively, with eigenvalues $\lambda_i = \sigma_i^2$, $i = 1, \, \ldots \, , R$, while the remaining columns (if any, i.e., if $R < {N_h}$ or $R < N$, respectively) belongs to the kernel of $\mathbb{S} \mathbb{S}^T$ and $\mathbb{S}^T \mathbb{S}$, respectively. 
		
		\noindent Due to the sparsity pattern of $\Sigma$ in \eqref{eq:svd}, the SVD of $\mathbb{S}$ can be cast in the compact form:
		\begin{equation*}
			\label{eq:svd-compact}
			\mathbb{S} = \mathbb{W}_R^{} ~ \mathbb{D} ~ \mathbb{Z}_R^T \, ,
		\end{equation*}
		with $\mathbb{W}_R \in \mathbb{R}^{{N_h} \times R}$ and $\mathbb{Z}_R \in \mathbb{R}^{N \times R}$ retaining only the first $R$ columns of $W$ and $Z$, respectively, i.e.,
		\begin{equation*}
			\label{eq:svd-compact-matrices}
			\mathbb{W}_R = \big[ \mathbf{w}_1 \, \big| \, \ldots \, \big| \, \mathbf{w}_R \big] \hspace*{0.3cm} \text{and} \hspace*{0.3cm} \mathbb{Z}_R = \big[ \mathbf{z}_1 \, \big| \, \ldots \, \big| \, \mathbf{z}_R \big] \, .
		\end{equation*}
		%Exploiting the orthonormality of the columns of $\mathbb{W}_R$, the generic column $\mathbf{s}_n = \mathbf{u}_h \big( \bg{\mu}^{(n)} \big)$ of $\mathbb{S}$, $n = 1, \, \ldots \, N$, can be expressed as \cite{Vol08}
		%\begin{equation*}
		%		\mathbf{s}_n = \sum_{r = 1}^R (\mathbf{s}_n, \, \mathbf{w}_r)_{\mathbb{R}^{N_h}} \mathbf{w}_r \, ,
		%\end{equation*}
		%where $(\cdot,\cdot)_{\mathbb{R}^{N_h}}$ denotes the Euclidean scalar product in $\mathbb{R}^{N_h}$. Therefore, the columns $\big\lbrace \mathbf{w}_1, \, \ldots \, , \mathbf{w}_R \big\rbrace$ of $\mathbb{W}_R$ constitute an orthonormal basis for the column space $\text{Col}(\mathbb{S})$ of $\mathbb{S}$. 
		
		\noindent Assume now we seek to approximate the columns of $\mathbb{S}$ by means of $L$ orthonormal vectors $\big\lbrace \wt{\mathbf{w}}_1, \, \ldots \, , \wt{\mathbf{w}}_L \big\rbrace$, with $L < R$. It is an easy matter to show that for each $\mathbf{s}_n$, $n = 1, \, \ldots \, , N$, the element of $\text{span} \big\lbrace \wt{\mathbf{w}}_1, \, \ldots \, , \wt{\mathbf{w}}_L \big\rbrace$ closest to $\mathbf{s}_n$ in the Euclidean norm $\norm{\cdot}_{\mathbb{R}^{N_h}}$ is given by
		\begin{equation*}
			\sum_{l = 1}^L \big( \mathbf{s}_n, \, \wt{\mathbf{w}}_l \big)_{\mathbb{R}^{N_h}} \wt{\mathbf{w}}_l \, .
		\end{equation*} 
		Hence, we could measure the error committed by approximating the columns of $\mathbb{S}$ via the vectors $\big\lbrace \wt{\mathbf{w}}_l \big\rbrace_{1 \leq l \leq L}$ through the quantity
		\begin{equation}
			\label{eq:basis-error}
			\varepsilon(\wt{\mathbf{w}}_1, \, \ldots \, , \wt{\mathbf{w}}_L) = \sum_{n = 1}^N \norm{\mathbf{s}_n - \sum_{l = 1}^L \big( \mathbf{s}_n, \, \wt{\mathbf{w}}_l \big)_{\mathbb{R}^{N_h}} \wt{\mathbf{w}}_l}_{\mathbb{R}^{N_h}}^2 \, .
		\end{equation}
		The Schmidt-Eckart-Young theorem \cite{EY36, Sch07} states that the \emph{POD basis} of rank $L$ $\big\lbrace \mathbf{w}_1, \, \ldots \, , \mathbf{w}_L \big\rbrace$, consisting of the first $L$ left singular values of $\mathbb{S}$, minimizes \eqref{eq:basis-error} among all the orthonormal bases of $\mathbb{R}^L$. Therefore, in the POD-Galerkin RB method, we set $\bg{\psi}_l = \mathbf{w}_l$, for all $l = 1, \, \ldots \, , L$, so that
		\begin{equation*}
			\mathbb{V} = \big[ \mathbf{w}_1 \, \big| \, \ldots \, \big| \, \mathbf{w}_L \big] \, . %\, \footnote{Please observe that generally we do not decorate $\mathbb{V}$ with any subscript nor superscript reporting the dimension of the reduced basis it represents, unless not clear from the context.} \, .
		\end{equation*}
		%Hence, in the reduced basis problem \eqref{eq:pde-rb} the test and trial functions are picked from the subspace $V_{\texttt{rb}}$ of $V_h$ spanned by the functions $\big\lbrace \psi_l \big\rbrace_{1 \leq l \leq L}$, given by
		%\begin{equation}
		%	\label{eq:pod-fe-basis-functions}
		%	\psi_l(\bg{x}) = \sum_{m = 1}^{N_h} \psi_l^{(m)} ~ \phi_m(\bg{x}) = \sum_{m = 1}^{N_h} (\mathbf{w}_l)_m ~ \phi_m(\bg{x}) \hspace*{0.3cm} \text{for $1 \leq l \leq L$} \, .
		%\end{equation}
	
		\noindent From a computational viewpoint, the first $L$ left singular vectors $\big\lbrace \mathbf{w}_l \big\rbrace_{1 \leq l \leq L}$ of $\mathbb{S}$ can be efficiently computed through the so-called \emph{method of snapshots}. We should distinguish two cases:
		\begin{enumerate}[label=(\alph*)]
			\item if ${N_h} \leq N$: directly solve the eigenvalue problems $\mathbb{S} \mathbb{S}^T \mathbf{w}_l = \lambda_l ~ \mathbf{w}_l$, for $1 \leq l \leq L$;
			\item if ${N_h} > N$: compute the \emph{correlation} matrix $\mathbb{M} = \mathbb{S}^T \mathbb{S}$ and solve the eigenvalue problems $\mathbb{M} ~ \mathbf{z}_l = \lambda_l ~ \mathbf{z}_l$, for $1 \leq l \leq L$. Then, by \eqref{eq:svd-relationships} set $\mathbf{w}_l = (\lambda_l)^{-\nicefrac{1}{2}} ~ \mathbb{S} ~ \mathbf{z}_l$, for $1 \leq l \leq L$.
		\end{enumerate}
		
		\algrenewcommand\textproc{}
		
		\begin{algorithm}[t]	
			\begin{algorithmic}[1]
				\Function{$\mathbb{V} = $ \textsc{PODG\_offline}}{$\mathcal{P}$, $\Omega_h$, $N$}
					\State generate the parameter set $\Xi_N = \big\lbrace \bg{\mu}^{(1)}, \, \ldots \, , \bg{\mu}^{(N)} \big\rbrace$
					\State compute the high-fidelity solutions $\big\lbrace \mathbf{u}_h \big( \bg{\mu}^{(1)} \big), \, \ldots \, , \mathbf{u}_h \big( \bg{\mu}^{(N)} \big) \big\rbrace$ via FE-Newton's method
					\State generate the POD basis functions $\big\lbrace \mathbf{w}_1, \, \ldots \, , \mathbf{w}_L \big\rbrace$ via method of snapshots
					\State assemble $\mathbb{V} = \big[ \mathbf{w}_1 \, \big| \, \ldots \, \big| \, \mathbf{w}_L \big]$
				\EndFunction
				
				\vspace*{0.3cm}
				
				\setcounter{ALG@line}{0}
				
				\Function{$\mathbf{u}_L (\bg{\mu}) = $ \textsc{PODG\_online}}{$\bg{\mu}$, $\mathbb{V}$}
					\State assemble and solve the reduced system \eqref{eq:rb-nonlinear-system} via Newton's method, yielding $\mathbf{u}_{\texttt{rb}} (\bg{\mu})$
					\State $\mathbf{u}_L (\bg{\mu}) = \mathbb{V} \, \mathbf{u}_{\texttt{rb}} (\bg{\mu})$
				\EndFunction
			\end{algorithmic}
			
			\caption{The offline and online stages for the POD-Galerkin (POD-G) RB method.}
			\label{alg:pod-galerkin}
		\end{algorithm}
		
		\algrenewcommand\textproc{\textsc}
		
	
	%
	% Subsection 3.2 : Implementation of POD-G method
	%
	
	\subsection{Implementation: details and issues}
	\label{section:Implementation: details and issues}
	
		The numerical procedure presented so far can be efficiently carried out within an offline-online framework \cite{Pru02}. The parameter-independent \emph{offline} step consists of the generation of the snapshots through a high-fidelity, expensive discretization scheme and the subsequent construction of the reduced basis via POD. To determine an appropriate dimension for the basis, which ensures a desired degree of accuracy, one can resort to empirical criteria, like, e.g., the \emph{relative information content} \cite{QMN15}. Then, given a new parameter value $\bg{\mu} \in \mathcal{P}$, the nonlinear reduced system \eqref{eq:rb-nonlinear-system} is solved \emph{online} using, e.g., Newton's method, which entails the assembly and resolution of linear systems of the form \eqref{eq:rb-nonlinear-system-newton}. The main steps of the resulting POD-Galerkin (POD-G) RB method are summarized in Algorithm \ref{alg:pod-galerkin}.
				
		However, to enjoy a significant reduction in the computational burden with respect to traditional (full-order) discretization techniques, the complexity of any online query should be \emph{independent} of the original size of the problem. At this regard, notice that the operative definitions \eqref{eq:rb-nonlinear-system} and \eqref{eq:rb-nonlinear-system-jacobian} of the reduced residual vector $\mathbf{G}_{\texttt{rb}}(\cdot; \bg{\mu})$ and its Jacobian $\mathbb{J}_{\texttt{rb}}(\cdot; \bg{\mu})$ involve the evaluation of the (high-fidelity) residual vector $\mathbf{G}_h(\cdot; \bg{\mu})$ and its Jacobian $\mathbb{J}_h(\cdot; \bg{\mu})$, whose cost clearly depends on ${N_h}$. Moreover, due to the nonlinearity of the underlying PDE and the non-affinity in the parameter dependence (partially induced by the transformation map $\bg{\Phi}(\cdot; \bg{\mu}_g)$), the assembly of the reduced linear systems \eqref{eq:rb-nonlinear-system-newton} has to be embodied directly in the online stage, thus seriously compromising the efficiency of the overall procedure \cite{Bar04}. Without escaping the algebraic framework, this can be successfully overcome upon resorting to suitable techniques such as the discrete empirical interpolation method (DEIM) \cite{Cha10} or its matrix variant (MDEIM) \cite{NMA15}, aiming at recovering an affine dependency on the parameter $\bg{\mu}$. On the other hand, we should point out that the implementation of such techniques is problem-dependent and of an \emph{intrusive} nature, as it requires to access and modify the assembly routines of the corresponding computational code \cite{Cas15}. Moreover, any interpolation procedure unavoidably introduces a further level of approximation. As a matter of fact, typically one needs to generate a larger number of snapshots in the offline stage and then retain a larger number of POD modes to guarantee the same accuracy provided by the standard POD-Galerkin method \cite{Bar04}.		
		
		
	%
	% Section 4 : Artificial neural networks
	%
	
	\section{Artifical neural networks}
	\label{section:Artificial neural networks}
	
		Inspired by the biological information processing system (see, e.g., \cite{Hay05, Kri07}), an \emph{artificial neural network} (ANN), usually simply referred to as \emph{neural network}, is a computational model capable to learn from observational data, i.e., by example, thus providing an alternative to the algorithmic programming paradigm \cite{Nie15}. As its original counterpart, it consists of a collection of processing units, called (artificial) \emph{neurons}, and directed \emph{ weighted synaptic} connections among the neurons. Data travel among neurons through the connections, following the direction imposed by the synapses. Hence, an artificial neural network is an \emph{oriented graph} to all intents and purposes, with the neurons as \emph{nodes} and the synapses as oriented \emph{edges}, whose weights are adjusted by means of a \emph{training} process to configure the network for a specific application \cite{SD13}. 
		
		Formally, a neural network could then be defined as follows \cite{Kri07}.
		
		\begin{definition}[Neural network]
			\label{def:neural-network}
			\emph{
			A \emph{neural network} is a sorted triple $\left( \mathcal{N} \, , \mathcal{V} \, , w \right)$, where $\mathcal{N}$ is the set of \emph{neurons}, with cardinality $|\mathcal{N}|$, $\mathcal{V} = \big\lbrace (i \, , j), \, 1 \leq i \, , j  \leq |\mathcal{N}| \big\rbrace$ is the set of \emph{connections}, with $(i,j)$ denoting the oriented connection linking the sending neuron $i$ with the target neuron $j$, and $w : \mathcal{V} \rightarrow \mathbb{R}$ is the \emph{weight function}, defining the weight $w_{i,j} = w((i,j))$ of the connection $(i,j)$. A weight may be either positive or negative, making the underlying connection either excitatory or inhibitory, respectively. By convention, $w_{i,j} = 0$ means that neurons $i$ and $j$ are not directly connected.
			}
		\end{definition}
		
		\noindent In the following, we dive deeper into the structure and training of a neural network, starting by detailing the working principles of an artificial neuron.  
		
				
	%
	% Subsection 4.1 : Neuronal model
	%
											
	\subsection{Neuronal model}
	\label{section:Neuronal model}
	
		An artificial neuron represents a simplified model of a biological neuron \cite{Kri07}. To introduce the components of the model, let us consider the neuron $j$ represented in Fig. \ref{fig:neural-model}. Suppose that it is connected with $m$ sending neurons $\big\lbrace s_1, \, \ldots \, , s_m \big\rbrace$, and $n$ receiving (target) neurons $\big\lbrace r_1, \, \ldots \, , r_n \big\rbrace$. Denoting by $y_{\Omega}(t) \in \mathbb{R}$ the scalar output fired by a generic neuron $\Omega$ at time $t$, neuron $j$ gets the weighted inputs $w_{s_k,j} \, y_{s_k}(t)$, $k = 1, \, \ldots \, , m$, at time $t$, and sends out the output $y_j(t + \Delta t)$ to the target neurons $\big\lbrace r_1, \, \ldots \, , r_n \big\rbrace$ at time $t + \Delta t$. In particular, neuron $r_i$, $i = 1, \, \ldots \, , n$, receives as input $w_{j,r_i} \, y_j(t + \Delta t)$. Please note that in the context of ANNs, the time is discretized by introducing the timestep $\Delta t$. This is clearly not plausible from a biological viewpoint; however, it substantially simplifies the implementation. In the following, we will avoid to specify the dependence on time unless strictly necessary, thus to lighten the notation.
		
		An artificial neuron $j$ is completely characterized by three functions: the propagation function, the activation function and the output function. These will be defined and detailed below in the same order they get involved in the data flow. 
		
		\vspace*{0.3cm}
		
		\noindent \textbf{Propagation function}. The propagation function $f_{prop}$ converts the vectorial input $\mathbf{p} = [y_{s_1}, \, \ldots \, , y_{s_m}]^T \in \mathbb{R}^m$ into a scalar $u_{j}$ often called \emph{net input}, i.e.,
		\begin{equation*}
			\label{eq:propagation-function}
			u_{j} = f_{prop}(w_{s_1,j}, \, \ldots \, , w_{s_m,j}, \, y_{s_1}, \, \ldots \, , y_{s_m}) \, .
		\end{equation*}
		A common choice for $f_{prop}$ (used also in this work) is the weighted sum, adding up the scalar inputs multiplied by their respective weights:
		\begin{equation}
			\label{eq:weighted-sum}
			f_{prop}(w_{s_1,j}, \, \ldots \, , w_{s_m,j}, \, y_{s_1}, \, \ldots \, , y_{s_m}) = \sum_{k = 1}^m w_{s_k,j} ~ y_{s_k} \, .
		\end{equation} 
		%The function \eqref{eq:weighted-sum} provides a simple, yet effective way of modeling the accumulation of different input electric signals within a biological neuron \cite{Hay05}.
		
		\vspace*{0.3cm}
		
		\noindent \textbf{Activation or transfer function}. At each timestep, the \emph{activation state} $a_j$, often referred to as \emph{activation}, quantifies to which extent neuron $j$ is currently \emph{active} or \emph{excited}. It results from the activation function $f_{act}$, which combines the net input $u_j$ with a threshold $\theta_j \in \mathbb{R}$ \cite{Kri07}: 
		\begin{equation*}
			\label{eq:activation-function}
			a_j = f_{act}(u_j; \, \theta_j) = f_{act} \big( \sum_{k = 1}^m w_{s_k,j} ~ y_{s_k} \, ; \, \theta_j \big) \, .
		\end{equation*}
		Note that the threshold $\theta_j$ is a parameter of the network and as such one may choose to adapt it through a training process, exactly as it can be done for the synaptic weights. To ease the runtime access of $\theta_j$, it is common practice to introduce a \emph{bias neuron} in the network. A bias neuron is a continuously firing neuron, with constant output $y_{b} = 1$, which is directly connected with neuron $j$, assigning the \emph{bias weight} $w_{b,j} = - \theta_j$ to the connection. As can be deduced by the representation on the right in Fig. \ref{fig:neural-model}, $\theta_j$ is now treated as a synaptic weight, while the neuron threshold is set to zero. Hence, the net input becomes
		\begin{equation*}
			\label{eq:net-input}
			u_j = \sum_{k = 1}^m w_{s_k,j} ~ y_{s_k} - \, \theta_j \, ,
		\end{equation*}
		i.e., the threshold is included in the propagation function rather than in the activation function, which can now be expressed in the form
		\begin{equation*}
			a_j = f_{act} \big( \sum_{k = 1}^m w_{s_k,j} ~ y_{s_k} - \, \theta_j \big) \, .
		\end{equation*}		
		Conversely to the propagation function, there exist various choices for the activation function. Among all, sigmoid activation functions have been widely used for the realization of artificial neural networks due to their graceful combination of linear and nonlinear behaviour \cite{Hay05}. Sigmoid functions are s-shaped, monotically increasing, and assume values in a bounded interval; a well-known instance is given by the hyperbolic tangent,
		\begin{equation*}
			\label{eq:hyperbolic-tangent}
			f_{act}(v) = \dfrac{e^{v} - e^{-v}}{e^v + e^{-v}} \, .
		\end{equation*}
		
		%\vspace*{0.3cm}
		
		\noindent \textbf{Output function}. Finally, the output function $f_{out}$ calculates the scalar \emph{output} $y_j \in \mathbb{R}$ based on the activation state $a_j$ of the neuron:
		\begin{equation*}
			\label{eq:output-function}
			y_j = f_{out}(a_j) \, .
		\end{equation*} 
		Typically, $f_{out}$ is the identity function, so that activation and output of a neuron coincides, i.e., $y_j = f_{out}(a_j) = a_j$. %Note that while the input $\mathbf{p} = [y_{s_1}, \, \ldots \, , y_{s_m}]^T \in \mathbb{R}^m$ of the neuron is generally vectorial, i.e., $m > 1$, the output is scalar. 
		The output $y_j$ could then be sent either to other neurons or constitute a component of the overall output vector of the network, as for the neurons in the output layer of a feedforward neural network, illustrated in the following subsection.
		
		\vspace*{0.3cm}
		
		The neural model presented so far refers to the so called \emph{computing} neuron, i.e., a neuron which process input information to provide a response. However, in a neural network one may also identify \emph{source} neurons, supplying the network with the respective components of the activation pattern (input vector), without performing any computation \cite{Hay05}.
		
		%\vspace*{0.5cm}
		
		\iffalse
		\begin{figure}[H]
			\center
			\includegraphics[scale = 0.6]{neural_model_ter.eps}
			
			\caption{Visualization of the generic $j$-th neuron of an artificial neural network. The neuron accumulates the weighted inputs $\big\lbrace w_{s_1,j} ~ y_{s_1}, \, \ldots \, , w_{s_m,j} ~ y_{s_m} \big\rbrace$ coming from the sending neurons $s_1, \, \ldots \, , s_m$, and fires $y_j$, sent to the target neurons $\big\lbrace r_1, \, \ldots \, , r_n \big\rbrace$ through the synapsis $\big\lbrace w_{j,r_1}, \, \ldots \, , w_{j,r_n} \big\rbrace$. The neuron threshold $\theta_j$ is reported within its body.} 
			\label{fig:neural-model}
		\end{figure}
		
		\begin{figure}[H]
			\center
			\includegraphics[scale = 0.6]{neural_model_bias.eps}
			
			\caption{Visualization of the generic $j$-th neuron of an artificial neural network. The neuron accumulates the weighted inputs $\big\lbrace w_{s_1,j} ~ y_{s_1}, \, \ldots \, , w_{s_m,j} ~ y_{s_m}, \, -\theta_j \big\rbrace$ coming from the sending neurons $s_1, \, \ldots \, , s_m, \, b$, respectively, with $b$ the bias neuron. The neuron output $y_j$ is then conveyed towards the target neurons $\big\lbrace r_1, \, \ldots \, , r_n \big\rbrace$ through the synapsis $\big\lbrace w_{j,r_1}, \, \ldots \, , w_{j,r_n} \big\rbrace$. Observe that, conversely to the model offered in Fig. \ref{fig:neural-model}, the neuron threshold is now set to $0$.} 
			\label{fig:neural-model-bias}
		\end{figure}
		\fi
		
		\begin{figure}[t]
			\center
			\includegraphics[scale = 0.55]{neural_model_ter.eps}
			
			\caption{Visualization of the generic $j$-th neuron of an artificial neural network. The neuron accumulates the weighted inputs $\big\lbrace w_{s_1,j} ~ y_{s_1}, \, \ldots \, , w_{s_m,j} ~ y_{s_m} \big\rbrace$ respectively coming from the sending neurons $s_1, \, \ldots \, , s_m$. Then, it fires $y_j$, sent to the target neurons $\big\lbrace r_1, \, \ldots \, , r_n \big\rbrace$ through the synapsis $\big\lbrace w_{j,r_1}, \, \ldots \, , w_{j,r_n} \big\rbrace$. The neuron threshold is reported in brackets within its body.} 
			\label{fig:neural-model}
		\end{figure}
		
		
	%
	% Subsection 4.2 : Network topology
	%
	
	\subsection{Network topology: the feedforward neural network}
	\label{section:Network topology}
	
		The way neurons are interconnected within a network defines the \emph{topology} of the network itself, i.e., its design. In the literature, many network architectures have been proposed, sometimes tailored to a specific application. Among all, \emph{feedforward neural networks}, also called \emph{perceptrons} \cite{Ros58}, have been typically preferred in function regression tasks.
		
		In a feedforward neural network, neurons are arranged into \emph{layers}, with one \emph{input layer} of $M_I$ source neurons, $K$ \emph{hidden layers}, each one consisting of $H_k$ computing neurons, $k = 1, \, \ldots \, , K$, and an \emph{output layer} of $M_O$ computing neurons. As a characteristic property, neurons in a layer can only be connected with neurons in the next layer towards the output layer, and not within the same layer. Then, an \emph{activation pattern} $\mathbf{p} \in \mathbb{R}^{M_I}$, supplied to the network through the source nodes in the first layer, provides the input signal for the neurons in the first hidden layer. For each hidden layer, its output signals give the input pattern for the following layer. In this way, information travels towards the last layer of the network, i.e., the output layer, whose outputs constitute the components of the overall output $\mathbf{q} \in \mathbb{R}^{M_O}$ of the network \footnote{Please note that while the output of a single neuron is denoted with the letter $y$, we use the letter $\mathbf{q}$ (bolded) to indicate the overall output of the network. Clearly, for the $j$-th output neuron the output $y_j$ coincides with the corresponding entry of $\mathbf{q}$, i.e., $q_j = y_j$ for any $j = 1, \, \ldots \, , M_O$.}. Hence, a feedforward network establishes a map between the \emph{input space} $\mathbb{R}^{M_I}$ and the \emph{output space} $\mathbb{R}^{M_O}$. This does make this network architecture particularly suitable for continuous function approximation.
		
		Feedforward networks can be classified according to the number of hidden layers or, equivalently, the number of layers of trainable weights. Single-layer perceptrons (SLPs) consist of the input and output layer, without any hidden layer. Because of their quite simple structure, the range of application of SLPs is rather limited. Indeed, only \emph{linearly separable} data can be properly represented using SLPs \cite{Kri07}. Conversely, multi-layer perceptrons (MLPs), with at least one hidden layer, are universal function approximators, as stated by Cybenko \cite{Cyb88, Cyb89}. In detail:
		\begin{enumerate}[label=(\roman*)]
			\item an MLP with \emph{one} layer of hidden neurons and differentiable activation functions can approximate \emph{any continuous} function; %\cite{Cyb89};
			\label{cybenko-first-rule}
			\item an MLP with \emph{two} layers of hidden neurons and differentiable activation functions can approximate \emph{any} function. %\cite{Cyb88}. 
			\label{cybenko-second-rule}
		\end{enumerate}
		Therefore, in many practical applications there is no reason to employ MLPs with more than two hidden layers. However, \ref{cybenko-first-rule} and \ref{cybenko-second-rule} do not give any practical advice neither on the number of hidden neurons nor the number of samples required to train the network: these should be found pursuing a \emph{trial-and-error} (and likely time-consuming) approach \cite{Hag14}.
		
		An instance of a three-layer (i.e., two hidden layer plus the output layer) feedforward network is offered in Fig. \ref{fig:neural-network}. In this case, we have $M_I = 3$ input neurons (denoted with the letter $i$), $H_1 = H_2 = 6$ hidden neurons (letter $h$ for both hidden layers), and $M_O = 4$ output neurons (letter $o$). In particular, it represents an instance of a \emph{completely linked} perceptron, since each neuron is directly connected with all neurons in the following layer.
				
		\begin{figure}[t]
			\center
			\includegraphics[scale = 0.55]{neural_network_bis.eps}
			
			\caption{A three-layer feedforward neural network, with three input neurons, two hidden layers each one consisting of six neurons, and four output neurons. Within each connection, information flows from left to right.}
			\label{fig:neural-network}
		\end{figure}
		
		
	%
	% Subsection 4.3 : Training a multi-layer feedforward neural network
	%
	
	\subsection{Training a multi-layer feedforward neural network}
	\label{section:Training a multi-layer feedforward neural network}
	
		As previously mentioned, the principal characteristic of a neural network is its capability of \emph{learning} from the surrounding environment, storing the acquired knowledge via its internal parameters, i.e., the synaptic and bias weights. Learning is accomplished through a training process, during which the network is exposed to a collection of examples, called \emph{training patterns}. According to some performance measure, the weights are then adjusted by means of a well-defined set of rules. Therefore, the learning procedure is an \emph{algorithm}, typically iterative, such that after a successfull training, the neural network provides reasonable responses for unknown problems of the same class of the training set. This property is known as \emph{generalization} \cite{Kri07}. 
		
		Training algorithms can be classified based on the nature of the training set, i.e., the set of training patterns. We can then distinguish three \emph{learning paradigms}: supervised learning, unsupervised learning and reinforcement learning \cite{Hag14}. The choice of the learning paradigm is task-dependent. Particularly, for function approximation, the \emph{supervised} learning paradigm is the best choice. Consider the nonlinear unknown function $\bg{f}$,
		\begin{equation*}
			\begin{aligned}
				\bg{f} ~ : ~ & \mathbb{R}^{M_I} \hspace*{-0.3cm} && \rightarrow ~~ \mathbb{R}^{M_O} \\
						     & \mathbf{x} && \mapsto ~~ \mathbf{y} = \bg{f}(\mathbf{x}) \, ,
			\end{aligned}
		\end{equation*}
		and a set of labeled examples $\big\lbrace \mathbf{p}_i, \mathbf{t}_i = \bg{f}(\mathbf{p}_i)\big\rbrace_{1 \leq i \leq N_{tr}}$ which form the training set. Note that at each \emph{input pattern} $\mathbf{p}_i \in \mathbb{R}^{M_I}$, $i = 1, \, \ldots \, , N_{tr}$, corresponds an associated desired output or \emph{teaching input} $\mathbf{t}_i \in \mathbb{R}^{M_O}$. The goal is to approximate $\bg{f}$ over a domain $D \subset \mathbb{R}^{M_I}$ up to a user-defined tolerance $\epsilon$, i.e.,
		\begin{equation*}
			||\bg{F}(\mathbf{x}) - \boldsymbol{f}(\mathbf{x})|| < \epsilon \hspace*{0.3cm} \text{$\forall \mathbf{x} \in D$} \, ,
		\end{equation*}
		where $\bg{F} : \mathbb{R}^{M_I} \rightarrow \mathbb{R}^{M_O}$ is the actual input-output map established by the neural network and $||\cdot||$ is some suitable norm on $\mathbb{R}^{M_O}$. To this end, the synaptic weights are iteratively adjusted according to a set of well-defined rules. Consider the synapsis between a sending neuron $i$ and a target neuron $j$. At the $t$-th iteration (also called \emph{epoch}) of the training procedure, the weight $w_{i,j}(t)$ of the connection $(i,j)$ is modified by the time-dependent quantity $\Delta w_{i,j}(t)$, whose form depends on the specific learning rule. Hence, at the subsequent iteration $t+1$ the synaptic weight is simply given by
		\begin{equation*}
			\label{eq:weight-update}
			w_{i,j}(t+1) = w_{i,j}(t) + \Delta w_{i,j}(t) \, .
		\end{equation*}
		The whole training process is driven by an \emph{error} or \emph{performance} function $E$, which measures the discrepancy between the neural network knowledge of the surrounding environment and the actual state of the environment itself. %In other terms, the larger the the performance function is, the farther the neural network representation of the world is from the actual reality. 
		Therefore, every learning rule aims to \emph{minimize} the performance $E$. For this purpose, $E$ should be a scalar function of the free parameters, i.e., the weights, of the network, namely 
		\begin{equation*}
			\label{eq:performance-function}
			E = E(\mathbf{w}) \in \mathbb{R} \, , 
		\end{equation*}
		with $\mathbf{w} \in \mathbb{R}^{|\mathcal{V}|}$ be the vector collecting the weights $\big\lbrace w_{i,j} = w( \, (i,j) \, ) \big\rbrace_{(i,j) \in \mathcal{V}}$. Thus, the point over the error surface reached at the end of a successful training process provides the \emph{optimal} configuration $\mathbf{w}_{opt}$ for the network. A common choice for the performance function is the (accumulated) mean squared error (MSE)
		\begin{equation}
			\label{eq:accumulated-mse}
			E(\mathbf{w}) = \sum_{\mathbf{p} \in P} E_{\mathbf{p}}(\mathbf{w}) = \sum_{\mathbf{p} \in P} \dfrac{1}{M_O} \sum_{j = 1}^{M_O} \left( t_{\mathbf{p},j} - q_{\mathbf{p},j} \right)^2 \, ,
		\end{equation}
		where for each input pattern $\mathbf{p}$ belonging to the training set $P$, $\mathbf{t}_{\mathbf{p}}$ and $\mathbf{q}_{\mathbf{p}}$ denote the corresponding teaching input and actual output, respectively. Observe that \eqref{eq:accumulated-mse} accounts for the error committed on each input pattern in the training set, leading to a so-called \emph{offline} learning procedure.
		
		In our numerical tests, at each epoch $t$ the (vectorial) weight update $\Delta \mathbf{w}(t)$ is computed via the Levenberg-Marquardt algorithm, derived in the following subsection.
				
		The Levenberg-Marquardt algorithm \cite{Mar63} stands as an approximation to Newton's method \cite{Hag94}. According to the latter, at each iteration the \emph{search direction} $\Delta \mathbf{w}$ is sought by solving the following linear system:
		\begin{equation}
			\label{eq:newton}
			\nabla^2 E(\mathbf{w}) ~ \Delta \mathbf{w} = - \nabla E(\mathbf{w}) \, ,
		\end{equation}
		where $\nabla E(\mathbf{w})$ and $\nabla^2 E(\mathbf{w})$ respectively denotes the gradient vector and the Hessian matrix of $E$ with respect to its argument $\mathbf{w}$. Assume that the loss function is represented as the accumulated MSE and let $\mathbf{e}_{\mathbf{p}}$ be the error vector corresponding to the input pattern $\mathbf{p}$, namely, $\mathbf{e}_{\mathbf{p}} = \mathbf{t}_{\mathbf{p}} - \mathbf{q}_{\mathbf{p}}$. Introducing the Jacobian $\mathbb{J}_{\mathbf{p}}$ of the specific error vector $\mathbf{e}_{\mathbf{p}}$ with respect to $\mathbf{w}$, i.e., 
		\begin{equation*}
			\label{eq:jacobian}
			\mathbb{J}_{\mathbf{p}}(\mathbf{w}) = 
			\begin{bmatrix}
				\vspace*{0.2cm}
				\hspace*{-0.3cm} & \dfrac{\partial e_{\mathbf{p},1}}{\partial w_1} & \ldots & \dfrac{\partial e_{\mathbf{p},1}}{\partial w_{|\mathcal{V}|}} \\
				\vspace*{0.2cm}
				\hspace*{-0.3cm} & \vdots & \ddots & \vdots \\
				\hspace*{-0.3cm} & \dfrac{\partial e_{\mathbf{p},M_O}}{\partial w_1} & \ldots & \dfrac{\partial e_{\mathbf{p},M_O}}{\partial w_{|\mathcal{V}|}}
			\end{bmatrix}
			\in \mathbb{R}^{M_O \times |\mathcal{V}|}
		\end{equation*}
		simple computations yield
		\begin{equation}
			\label{eq:gradient}
			\nabla E(\mathbf{w}) = \sum_{\mathbf{p} \in P} \dfrac{2}{M_O} \mathbb{J}_{\mathbf{p}}(\mathbf{w})^T \mathbf{e}_{\mathbf{p}} \in \mathbb{R}^{|\mathcal{V}|}
		\end{equation}
		and
		\begin{equation}
			\label{eq:hessian}
			\nabla^2 E(\mathbf{w}) = \sum_{\mathbf{p} \in P} \dfrac{2}{M_O} \left[ \mathbb{J}_{\mathbf{p}}(\mathbf{w})^T \mathbb{J}_{\mathbf{p}}(\mathbf{w}) + \mathbb{S}(\mathbf{w}) \right] \in \mathbb{R}^{|\mathcal{V}| \times |\mathcal{V}|} \, ,
		\end{equation}
		with
		\begin{equation*}
			\mathbb{S}(\mathbf{w}) = \sum_{\mathbf{p} \in P} \dfrac{2}{M_O} \sum_{j = 1}^{M_O} e_{\mathbf{p},j} \nabla^2 e_{\mathbf{p},j} \, .
		\end{equation*}
		Assuming $\norm{\mathbb{S}(\mathbf{w})} \approx 0$ in some suitable matrix norm $\norm{\cdot}$, inserting \eqref{eq:gradient} and \eqref{eq:hessian} into \eqref{eq:newton} we get the linear system
		\begin{equation}
			\label{eq:newton-quadratic-function}
			\left[ \sum_{\mathbf{p} \in P} \mathbb{J}_{\mathbf{p}}(\mathbf{w})^T \mathbb{J}_{\mathbf{p}}(\mathbf{w}) \right] \Delta \mathbf{w} = - \sum_{\mathbf{p} \in P} \mathbb{J}_{\mathbf{p}}(\mathbf{w})^T \mathbf{e}_{\mathbf{p}}(\mathbf{w}) \, .
		\end{equation} 
		The Levenberg-Marquardt modification to Newton's method reads \cite{Hag94, Mar63}:
		\begin{equation}
			\label{eq:levenberg-marquardt}
			\left[ \sum_{\mathbf{p} \in P} \mathbb{J}_{\mathbf{p}}(\mathbf{w})^T \mathbb{J}_{\mathbf{p}}(\mathbf{w}) + \mu \mathbb{I} \right] \Delta \mathbf{w} = - \sum_{\mathbf{p} \in P} \mathbb{J}_{\mathbf{p}}(\mathbf{w})^T \mathbf{e}_{\mathbf{p}}(\mathbf{w}) \, ,
		\end{equation}
		with $\mu \geq 0$ and $\mathbb{I}$ the identity matrix of size $|\mathcal{V}| \times |\mathcal{V}|$. Note that if $\mu = 0$ we recover Newton's method \eqref{eq:newton-quadratic-function}, while for $\mu \gg 1$ the search direction $\Delta \mathbf{w}$ approaches the antigradient of $E$. Hence, the Levenberg-Marquardt algorithm can be seen as an interpolation between Newton's method and the steepest descent method, aiming to retain the advantages of both techniques \cite{Hag94}.
		
		The Levenberg-Marquardt training algorithm proceeds as follows. At each epoch $t$, we solve the (potentially large) linear system \eqref{eq:levenberg-marquardt}. Whenever the step $\Delta \mathbf{w}(t)$ leads to a reduction in the performance function, i.e., $E(\mathbf{w}(t) + \Delta \mathbf{w}(t)) < E(\mathbf{w}(t))$, the parameter $\mu$ is reduced by a factor $\beta > 1$. Conversely, if $E(\mathbf{w}(t) + \Delta \mathbf{w}(t)) > E(\mathbf{w}(t))$ the parameter is multiplied by the same factor $\beta$. This reflects the idea that far from the actual minimum we should prefer the gradient method to Newton's method, since the latter may diverge. Yet, once in a neighborhood of the minimum, we switch to Newton's method so to exploit its faster convergence \cite{Mar63}.
		
		The key step in the algorithm is the computation of the Jacobian $\mathbb{J}_{\mathbf{p}}(\mathbf{w})$ for each training vector $\mathbf{p}$. Suppose that the $k$-th element $w_k$ of $\mathbf{w}$ represents the weight $w_{i,j}$ of the connection $(i,j)$, for some $i$ and $j$, with $1 \leq i \, , j \leq |\mathcal{N}|$. Then, for any $h = 1, \, \ldots \, , M_O$ and $k = 1, \, \ldots \, , |\mathcal{V}|$, exploiting the chain rule we get (omitting the dependence on $\mathbf{w}$):
		\begin{equation*}
			\label{eq:jacobian-entry-equation}
			\begin{aligned}
				\big( \mathbb{J}_{\mathbf{p}} \big)_{h,k} = \dfrac{\partial e_{\mathbf{p},h}}{\partial w_{i,j}} = \dfrac{\partial e_{\mathbf{p},h}}{\partial u_{\mathbf{p},j}} ~ \dfrac{\partial u_{\mathbf{p},j}}{\partial w_{i,j}} 
				= \dfrac{\partial e_{\mathbf{p},h}}{\partial y_{\mathbf{p},j}} ~ \dfrac{\partial y_{\mathbf{p},j}}{\partial u_{\mathbf{p},j}} ~ \dfrac{\partial u_{\mathbf{p},j}}{\partial w_{i,j}} 
				= \dfrac{\partial e_{\mathbf{p},h}}{\partial y_{\mathbf{p},j}} ~ f'_{act}(u_{\mathbf{p},j}) ~ y_{\mathbf{p},i}
				= \delta_{\mathbf{p},h,j} ~ y_{\mathbf{p},i} \, ,
			\end{aligned}
		\end{equation*} 
		with
		\begin{equation}
			\label{eq:levenberg-marquardt-delta}
			\delta_{\mathbf{p},h,j} := - \dfrac{\partial e_{\mathbf{p},h}}{\partial u_{\mathbf{p},j}} = - \dfrac{\partial e_{\mathbf{p},h}}{\partial y_{\mathbf{p},j}} ~ f'_{act}(u_{\mathbf{p},j}) \, .
		\end{equation}
		For the computation of the derivative 
		\begin{equation*}
			\dfrac{\partial e_{\mathbf{p},h}}{\partial y_{\mathbf{p},j}}
		\end{equation*}
		occurring in \eqref{eq:levenberg-marquardt-delta}, we assume that, within the set of neurons $\mathcal{N}$, items are ordered such that the output neurons come first, i.e.,
		\begin{equation*}
			\text{$j$ output neuron \hspace*{0.2cm} $\Leftrightarrow$ \hspace*{0.2cm} $1 \leq j \leq M_O$} \, .
		\end{equation*} 
		We can then distinguish three cases:
		\begin{enumerate}[label=(\roman*)]
			\item $j$ output neuron, $j = h$: since $e_{\mathbf{p},h} = e_{\mathbf{p},j} = t_{\mathbf{p},j} - y_{\mathbf{p},j}$, then 
			\begin{equation*}
				\label{eq:jacobian-first-case}
				\dfrac{\partial e_{\mathbf{p},h}}{\partial y_{\mathbf{p},j}} = -1 \, ;
			\end{equation*}
			\item $j$ output neuron, $j \neq h$: the output of an output neuron can not influence the signal fired by another output neuron, hence 
			\begin{equation*}
				\label{eq:jacobian-second-case}
				\dfrac{\partial e_{\mathbf{p},h}}{\partial y_{\mathbf{p},j}} = 0 \, ;
			\end{equation*}
			\item $j$ inner neuron: letting $R$ be the set of successors of $j$, the chain rule yields
			\begin{equation*}
				\label{eq:jacobian-third-case}
				\dfrac{\partial e_{\mathbf{p},h}}{\partial y_{\mathbf{p},j}} = \sum_{r \in R} \dfrac{\partial e_{\mathbf{p},h}}{\partial u_{\mathbf{p},r}} ~ \dfrac{\partial u_{\mathbf{p},r}}{\partial y_{\mathbf{p},j}} = - \sum_{r \in R} \delta_{\mathbf{p},h,r} ~ w_{j,r} \, .
			\end{equation*}
		\end{enumerate}
		Ultimately, at any iteration of the learning algorithm, the entries of the Jacobian matrix $\mathbb{J}_{\mathbf{p}}$ are given by
		\begin{equation*}
			\big( \mathbb{J}_{\mathbf{p}} \big)_{h,k} = \dfrac{\partial e_{\mathbf{p},h}}{\partial w_k} = \delta_{\mathbf{p},h,j} ~ y_{\mathbf{p},i} \, , \hspace*{0.3cm} 1 \leq h \leq M_O \, , 1 \leq k \leq |\mathcal{V}| \, , \text{$w_k = w_{i,j}$ for some $i$ and $j$} \, ,
		\end{equation*}
		with
		\begin{subnumcases}{\delta_{\mathbf{p},h,j} =}
			\label{eq:levenberg-marquardt-inner-neuron}
		   	f'_{act}(u_{\mathbf{p},j}) \sum_{r \in R} \delta_{\mathbf{p},h,r} ~ w_{j,r} \, , & \text{if $j$ inner neuron} \, , \\
		   	\label{eq:levenberg-marquardt-output-neuron}
			f'_{act}(u_{\mathbf{p},j}) ~ \delta_{jh}^K \, , & \text{if $j$ output neuron} \, ,
		\end{subnumcases}
		where $\delta_{jh}^K$ is the Kronecker delta. Observe that \eqref{eq:levenberg-marquardt-inner-neuron} defines $\delta_{\mathbf{p},h,j}$ for a hidden node $j$ by relying on the neurons in the following layer, whereas \eqref{eq:levenberg-marquardt-output-neuron} only involves the net input $u_{\mathbf{p},j}$ of the neuron itself. Therefore, the coupled equations \eqref{eq:levenberg-marquardt-inner-neuron}-\eqref{eq:levenberg-marquardt-output-neuron} implicitly set the order in which the entries of $\mathbb{J}_{\mathbf{p}}(\mathbf{w})$ have to be determined: starting from the output layer, compute the terms related to connections ending in that layer, then move backwards to the preceeding layer. In this way, the error is \emph{backpropagated} from the output down to the input, leaving traces in each layer of the network \cite{Kri07, WH60}.
		
		Let us finally remark that a trial and error approach is required to find satisfactory values for $\mu$ and $\beta$; as proposed in \cite{Mar63}, a good starting point may be $\mu = 0.01$, with $\beta = 10$. Moreover, the dimension of the system \eqref{eq:levenberg-marquardt} increases nonlinearly with the number of neurons in the network, making the Levenberg-Marquardt algorithm poorly efficient for large networks \cite{Hag94}. However, it turns out to be efficient and accurate for networks with few hundreads of connections. 
		
		
	%
	% Section 5 : POD-NN method
	%
	
	\section{A non-intrusive reduced basis method using artificial neural networks}
	\label{section:A non-intrusive RB method using neural networks}
							
		The scenario portrayed so far motivates the research for an alternative approach to tackle any online query within the reduced basis framework, hopefully skipping the assembly and resolution of the reduced system. To this end, let us remark that there exists a one-to-one correspondence between the reduced space $V_{\texttt{rb}}$ and the column space $\text{Col}(\mathbb{V})$ of $\mathbb{V}$. Indeed, letting $\big\lbrace \phi_1, \, \ldots \, , \phi_{N_h} \big\rbrace$ be a basis for $V_h$ and $\big\lbrace \psi_1, \, \ldots \, , \psi_L \big\rbrace$ be the reduced basis, from Eq. \eqref{eq:rb-fe-coefficients} follows:
		\begin{equation*}
			V_{\texttt{rb}} \ni v_{L} = \sum_{j = 1}^L v_{\texttt{rb}}^{(j)} ~ \psi_j = \sum_{j = 1}^L v_{\texttt{rb}}^{(j)} \sum_{i = 1}^{N_h} \mathbb{V}_{i,j} ~ \phi_i = \sum_{i = 1}^{N_h} \big( \mathbb{V} ~ \mathbf{v}_{\texttt{rb}} \big)_i \phi_i \hspace*{0.3cm} \leftrightarrow \hspace*{0.3cm} \mathbb{V} ~ \mathbf{v}_{\texttt{rb}} \in \text{Col}(\mathbb{V}) \, .
		\end{equation*} 
		In particular, this implies that the projection of any $v_h \in V_h$ onto $V_{\texttt{rb}}$ in the discrete scalar product $(\cdot,\cdot)_h$, 
		\begin{equation}
			\label{eq:discrete-scalar-product}
			(\chi_h, \, \xi_h)_h = \sum_{i = 1}^{N_h} \chi_h^{(i)} ~ \xi_h^{(i)} = (\bg{\chi}_h, \, \bg{\xi}_h)_{\mathbb{R}^{N_h}} \hspace*{0.3cm} \text{$\forall \chi_h, \, \xi_h \in V_h$} \, , 
		\end{equation}
		algebraically corresponds to the projection $\mathbf{v}_h^{\mathbb{V}}$ of $\mathbf{v}_h$ onto $\text{Col}(\mathbb{V})$ in the Euclidean scalar product, given by
		\begin{equation*}
			\mathbf{v}_h^{\mathbb{V}} = \mathbb{P} ~ \mathbf{v}_h \hspace*{0.5cm} \text{with} \hspace*{0.5cm} \mathbb{P} = \mathbb{V} \mathbb{V}^T \in \mathbb{R}^{{N_h} \times {N_h}} \, .
		\end{equation*}
		Note that $\mathbf{v}_h^{\mathbb{V}}$ is the element of $\text{Col}(\mathbb{V})$ closest to $\mathbf{v}_h$ in the Euclidean norm, i.e.,
		\begin{equation*}
			\norm{\mathbf{v}_h - \mathbf{v}_h^{\mathbb{V}}}_{\mathbb{R}^{N_h}} = \inf_{\mathbf{w}_h \in \text{Col}(\mathbb{V})} \norm{\mathbf{v}_h - \mathbf{w}_h}_{\mathbb{R}^{N_h}} \, .
		\end{equation*}
		Therefore, the element of $V_{\texttt{rb}}$ closest to the high-fidelity solution $u_h$ in the discrete norm $\norm{\cdot}_h = \sqrt{(\cdot,\cdot)_h}$ can be expressed as
		\begin{equation*}
			\label{eq:high-fidelity-projected}
			u^{\mathbb{V}}_h(\bg{x}; \, \bg{\mu}) = \sum_{j = 1}^{N_h} \big( \mathbb{V} \mathbb{V}^T \mathbf{u}_h(\bg{\mu}) \big)_j \phi_j(\bg{x}) = \sum_{i = 1}^L \big( \mathbb{V}^T \mathbf{u}_h(\bg{\mu}) \big)_i ~ \psi_i(\bg{x}) \, .
		\end{equation*} 
		Motivated by this last equality, once a reduced basis has been constructed (e.g., via POD of the snapshot matrix), we aim at approximating the function
		\begin{equation}
			\label{eq:map-to-approximate}
			\begin{aligned}
				\bg{\pi} ~ : ~ \mathcal{P} \subset \mathbb{R}^P ~ & \rightarrow ~ \mathbb{R}^L \\
				\bg{\mu} ~~ & \mapsto ~ \mathbb{V}^T \mathbf{u}_h(\bg{\mu}) \, ,
			\end{aligned}
		\end{equation}
		which maps each input vector parameter $\bg{\mu} \in \mathcal{P}$ to the coefficients $\mathbb{V}^T \mathbf{u}_h(\bg{\mu})$ for the expansion of $u^{\mathbb{V}}_h$ in the reduced basis $\big\lbrace \psi_i \big\rbrace_{1 \leq i \leq L}$. Then, given a new parameter instance $\bg{\mu}$, the associated RB solution is simply given by the evaluation at $\bg{\mu}$ of the so-built approximation $\hat{\bg{\pi}}$ of $\bg{\pi}$, i.e.
		\begin{equation*}
			\mathbf{u}_{\texttt{rb}}(\bg{\mu}) = \hat{\bg{\pi}}(\bg{\mu}) \, ,
		\end{equation*}
		and, consequently,
		\begin{equation*}
			\mathbf{u}_L(\bg{\mu}) = \mathbb{V} ~ \hat{\bg{\pi}}(\bg{\mu}) \, .
		\end{equation*}
		Note that, provided that the construction of $\hat{\bg{\pi}}$ is entirely carried out within the offline stage, this approach leads to a \emph{non-intrusive} RB method, enabling a complete decoupling between the online step and the underlying full-order model. Moreover, the accuracy of the resulting reduced solution uniquely relies on the quality of the reduced basis and the effectiveness of the approximation $\hat{\bg{\pi}}$ of the map $\bg{\pi}$. %; we shall appreciate the consequence of these facts in the next section.
		
		In the literature, different approaches for the \emph{interpolation} of \eqref{eq:map-to-approximate} have been developed, e.g., exploiting some geometrical considerations concerning the solution manifold $\mathcal{M}$ \cite{Ams10}, or employing radial basis functions \cite{Chen17}. In this work we resort to artificial neural networks, in particular multi-layer perceptrons, for the \emph{nonlinear regression} of the map $\bg{\pi}$, leading to the POD-NN RB method. As described in Subsection \ref{section:Training a multi-layer feedforward neural network}, any neural network is tailored to the particular application at hand by means of a preliminary \emph{training} phase. Here, we are concerned with a function regression task, thus we straightforwardly adopt a \emph{supervised learning} paradigm, training the perceptron via exposition to a collection of (known) input-output pairs
		\begin{equation*}
			P_{tr} = \big\lbrace \big( ~ \bg{\mu}^{(i)}, \, \mathbb{V}^T \mathbf{u}_h \big( \bg{\mu}^{(i)} \big) ~ \big) \big\rbrace_{1 \leq i \leq N_{tr}} \, .
		\end{equation*} 
		According to the notation and nomenclature previously introduced, for any $i = 1, \, \ldots \, , N_{tr}$, $\mathbf{p}_i = \bg{\mu}^{(i)} \in \mathbb{R}^P$ represents the \emph{input pattern} and $\mathbf{t}_i =  \mathbb{V}^T \mathbf{u}_h \big( \bg{\mu}^{(i)} \big) \in \mathbb{R}^L$ the associated \emph{teaching input}; together, they constitute a \emph{training pattern}. In this respect, note that the teaching inputs $\mathbb{V}^T \mathbf{u}_h \big( \bg{\mu}^{(i)} \big)$, $i = 1, \, \ldots \, , N_{tr}$, are generated through the FE solver. On the one hand, this ensures the reliability of the teaching patterns, given the assumed high-fidelity of the FE scheme %(conversely to the reduced solver, as already appreciated before). 
		On the other hand, this also suggests to incorporate the learning phase of the perceptron within the offline step of the POD-NN RB method, as described in Algorithm \ref{alg:pod-nn}. In doing so, we exploit the natural decoupling between the training and the evaluation of neural networks, thus fulfilling the necessary requirement to enable great online efficiency. %; we refer the reader to the following chapter for a numerical validation of this assertion. 
				
		However, it should be pointed out that the design of an effective learning procedure may require a larger amount of snapshots than the generation of the reduced space does. Moreover, we mentioned in Subsection \ref{section:Network topology} the time-consuming yet unavoidable \emph{trial}-\emph{and}-\emph{error} approach which one should pursue in the search for an optimal network topology. To this end, we propose an automatic procedure, resumed in the following. %Algorithm \ref{}.
		
		\algrenewcommand\textproc{}
		
		\begin{algorithm}[b]	
			\begin{algorithmic}[1]
				\Function{$\big[ \mathbb{V}, \mathcal{N}_{\texttt{rb}}, \mathcal{V}_{\texttt{rb}}, \mathbf{w}_{\texttt{rb}} \big] = $ \textsc{PODNN\_offline}}{$\mathcal{P}$, $\Omega_h$, $N$}
					\State generate the parameter set $\Xi_N = \big\lbrace \bg{\mu}^{(1)}, \, \ldots \, , \bg{\mu}^{(N)} \big\rbrace$
					\State compute the high-fidelity solutions $\big\lbrace \mathbf{u}_h \big( \bg{\mu}^{(1)} \big), \, \ldots \, , \mathbf{u}_h \big( \bg{\mu}^{(N)} \big) \big\rbrace$ via FE-Newton's method
					\State generate the POD basis functions $\big\lbrace \mathbf{w}_1, \, \ldots \, , \mathbf{w}_L \big\rbrace$ via method of snapshots
					\State assemble $\mathbb{V} = \big[ \mathbf{w}_1 \, \big| \, \ldots \, \big| \, \mathbf{w}_L \big]$
					\State find an optimal network configuration $\big( \mathcal{N}_{\texttt{rb}}, \mathcal{V}_{\texttt{rb}}, \mathbf{w}_{\texttt{rb}} \big)$ relying upon LHS and Levenberg-Marquardt algorithm
				\EndFunction
				
				\vspace*{0.3cm}
				
				\setcounter{ALG@line}{0}
				
				\Function{$\mathbf{u}_L^{\texttt{NN}} (\bg{\mu}) = $ \textsc{PODNN\_online}}{$\bg{\mu}$, $\mathbb{V}$, $\mathcal{N}_{\texttt{rb}}$, $\mathcal{V}_{\texttt{rb}}$, $\mathbf{w}_{\texttt{rb}}$}
					\State evaluate the output $\mathbf{u}_{\texttt{rb}}^{\texttt{NN}}(\bg{\mu})$ of the network $\big( \mathcal{N}_{\texttt{rb}}, \, \mathcal{V}_{\texttt{rb}}, \, \mathbf{w}_{\texttt{rb}} \big)$ for the input vector $\bg{\mu}$
					\State $\mathbf{u}_L^{\texttt{NN}} (\bg{\mu}) = \mathbb{V} \, \mathbf{u}_{\texttt{rb}}^{\texttt{NN}}(\bg{\mu})$
				\EndFunction
			\end{algorithmic}
			
			\caption{The offline and online stages for the POD-NN RB method.}
			\label{alg:pod-nn}
		\end{algorithm}
		
		While Cybenko's theorems (see Subsection \ref{section:Network topology}) allow one to consider perceptrons with no more than two hidden layers, no similar a priori and general results are available for the number $H$ of neurons per layer (to ease the work, we uniquely consider networks with the same number of neurons in both the first and the second hidden layer). Hence, given an initial amount $N_{tr}$ of training samples (say $N_{tr} = 100$), we train the network for increasing values of $H$, stopping when overfitting of training data occurs (due to an excessive number of neurons with respect to the number of training patterns). In case the best configuration, i.e., the one yielding the smallest error on a \emph{test} data set, does not meet a desired level of accuracy, we generate a new set of snapshots, which will enlarge the current training set, and we then proceed to re-train the network in different configurations. At this point, it is worth pointing out two remarks. 
		\begin{enumerate}[label=(\roman*)]
			\vspace*{-0.1cm}
			\item Once the training set has been enriched, we can limit ourselves to network configurations including a number of neurons \emph{no smaller} than the current optimal network does. Indeed, the error (on the test data set) yielded by a neural network is decreasing in the number of patterns it is exposed to during the learning phase, and the larger the number of neurons, the faster the decay.
			%\vspace*{-0.7cm}
			\item In order to maximize the additional quantity of information available for the learning, we should ensure that the new training inputs, i.e., parameter values, do not overlap with the ones already present in the training parameter set. To this aim, we pursue an euristhic approach, employing, at each iteration, the latin hypercube sampling \cite{Imam08}, which has proved to provide a good compromise between randomness and even covarage of the parameter domain. %; an example is offered in Fig. \ref{fig:lhs} for $\mathcal{P} \subset \mathbb{R}^2$.
		\end{enumerate} 
		\vspace*{-0.1cm}
		The procedure is then iterated until a satisfactory degree of accuracy and generalization is attained, or the available resources (i.e., computing power, running time and memory space) are exhausted. Therefore, the speed-up enabled at the \emph{online} stage by the pursuit of a neural network-based approach comes at the cost of an extended \emph{offline} phase. 
		
		\iffalse
		\begin{figure}[t]
			\center
			\subfloat{\includegraphics[scale = 0.425, trim = {1cm 12cm 2.5cm 3.5cm}, clip]{lhs_sample}}
			\subfloat{\includegraphics[scale = 0.5, trim = {8cm 13.4cm 1cm 8cm}, clip]{lhs_sample_legend}}
			
			\caption{Three point sets randomly generate through the latin hypercube sampling. Observe the good coverage provided by the union of the sets, with only a few overlapping points.}
			\label{fig:lhs}
		\end{figure}
		\fi
		
		In our numerical tests, we resort to the Levenberg-Marquardt algorithm to properly adjust the weights of the perceptron during the learning phase, relying on the mean squared error (MSE) \eqref{eq:accumulated-mse} as performance function. To motivate this choice, let 
		\begin{equation*}
			\mathbf{u}_{\texttt{rb}}^{\texttt{NN}}(\bg{\mu}) \in \mathbb{R}^L
		\end{equation*}
		be the (actual) output provided by the network for a given input $\bg{\mu}$, and 
		\begin{equation*}
			\mathbf{u}_L^{\texttt{NN}}(\bg{\mu}) = \mathbb{V} ~ \mathbf{u}_{\texttt{rb}}^{\texttt{NN}}(\bg{\mu}) \in \text{Col}(\mathbb{V}) \subset \mathbb{R}^{N_h} \, . 
		\end{equation*}
		Then (omitting the dependence on the input vector to ease the notation):
		\begin{equation*}
			\label{eq:pod-nn-mse}
				MSE \big( \mathbf{u}_{\texttt{rb}}^{\texttt{NN}}, \, \mathbb{V}^T \mathbf{u}_h \big) \propto \norm{\mathbf{u}_{\texttt{rb}}^{\texttt{NN}} - \mathbb{V}^T \mathbf{u}_h}^2_{\mathbb{R}^L} = \norm{\mathbf{u}_L^{\texttt{NN}} - \mathbb{V} ~ \mathbb{V}^T \mathbf{u}_h}^2_{\mathbb{R}^{N_h}} = \norm{u_L^{\texttt{NN}} - u_h^{\mathbb{V}}}^2_h \, ,
		\end{equation*} 
		where
		\begin{equation*}
			\label{eq:pod-nn-solution}
			u_L^{\texttt{NN}}(\bg{x}; \, \bg{\mu}) = \sum_{i = 1}^L \big( \mathbf{u}_{\texttt{rb}}^{\texttt{NN}}(\bg{\mu}) \big)_i ~ \psi_i(\bg{x}) \in V_{\texttt{rb}} \, .
		\end{equation*}
		Therefore, for any training input $\bg{\mu}^{(i)}$, $i = 1, \, \ldots \, , N_{tr}$, by minimizing the MSE we actually minimize the distance (in the discrete norm $\norm{\cdot}_h$) between the approximation provided by the neural network and the projection of the FE solution onto the reduced space $V_{\texttt{rb}}$. The proper \emph{generalization} to other parameter instances not included in the training set is then ensured by the implementation of suitable techniques (e.g., early stopping \cite{Mat16}, generalized cross validation \cite{Koh95}) aiming at preventing the network to \emph{overfit} the training data.
		
		%In the following subsection, we aim at further investigating the accuracy which the proposed reduced basis strategy could provide. To this end, we develop a simplified \emph{a priori} error analysis. Yet, no rigorous proof is provided; rather, the goal is to give some insights on the potential of the method.
		
			
	%
	% Section 6 : Numerical results
	%
	
	\section{Numerical results}
	\label{section:Numerical results}
	
		In this section, we present the numerical results obtained via the FE, POD-G and POD-NN RB methods applied to parametrized full-Dirichlet boundary value problems (BVPs) for the one-dimensional and two-dimensional nonlinear Poisson equation and the two-dimensional incompressible steady Navier-Stokes equations. %We consider instances of the Poisson equation stated on either one- or two-dimensional domains. 
		In the one-dimensional case we deal uniquely with physical parameters, whereas in two spatial dimensions we consider purely geometrical parametrizations. %For the two-dimensional Navier-Stokes equations we also consider a purely geometrical parametrization. %Particularly, we will tackle the well-known lid-driven cavity problem for different values of the Reynold's number. The application of both the POD-G and the POD-NN techniques to differential problems featuring both physical and geometrical parameters, as well as natural or periodic boundary conditions, is left as future work.
		
		The two RB techniques considered in this work are compared both in terms of accuracy and performance enabled at the online stage. For this, let $\norm{\cdot}$ be the canonical (Euclidean) norm on $\mathbb{R}^{N_h}$. In the online phases of the POD-G and POD-NN methods, for a new parameter value $\bg{\mu} \in \mathcal{P}$ (not involved either in the generation of the POD basis nor in the identification of an optimal neural network), the following \emph{relative} errors with respect to the high-fidelity solution $\mathbf{u}_h(\bg{\mu})$ are analyzed:
		\begin{enumerate}[label=(\alph*)]
			\item the POD-G relative error,
			\begin{equation}
				\label{eq:podg-error}
				\varepsilon_{\texttt{PODG}}^{}(L, \bg{\mu}) = \dfrac{\norm{\mathbf{u}_h(\bg{\mu}) - \mathbf{u}_L(\bg{\mu})}}{\norm{\mathbf{u}_h(\bg{\mu})}} = \dfrac{\norm{\mathbf{u}_h(\bg{\mu}) - \mathbb{V} \, \mathbf{u}_{\texttt{rb}}(\bg{\mu})}}{\norm{\mathbf{u}_h(\bg{\mu})}} \, ;
			\end{equation} 
			\item the POD-NN relative error,
			\begin{equation}
				\label{eq:podnn-error}
				\varepsilon_{\texttt{PODNN}}^{}(L, \bg{\mu}) = \dfrac{\norm{\mathbf{u}_h(\bg{\mu}) - \mathbf{u}_L^{\texttt{NN}}(\bg{\mu})}}{\norm{\mathbf{u}_h(\bg{\mu})}} = \dfrac{\norm{\mathbf{u}_h(\bg{\mu}) - \mathbb{V} \, \mathbf{u}_{\texttt{rb}}^{\texttt{NN}}(\bg{\mu})}}{\norm{\mathbf{u}_h(\bg{\mu})}} \, ;
			\end{equation} 
			\item the relative projection error, i.e., the error committed by approximating the high-fidelity solution with its projection (in the discrete scalar product $(\cdot,\cdot)_h$, see \eqref{eq:discrete-scalar-product}) onto the reduced space $V_{\texttt{rb}}$,
			\begin{equation}
				\label{eq:projection-error}
				\varepsilon_{\mathbb{V}}^{}(L, \bg{\mu}) = \dfrac{\norm{\mathbf{u}_h(\bg{\mu}) - \mathbf{u}_h^{\mathbb{V}}(\bg{\mu})}}{\norm{\mathbf{u}_h(\bg{\mu})}} = \dfrac{\norm{\mathbf{u}_h(\bg{\mu}) - \mathbb{V} \mathbb{V}^T \mathbf{u}_h(\bg{\mu})}}{\norm{\mathbf{u}_h(\bg{\mu})}} \, ;
			\end{equation}
			recall that \eqref{eq:projection-error} provides a lower-bound for both \eqref{eq:podg-error} and \eqref{eq:podnn-error} (as noted in Section \ref{section:A non-intrusive RB method using neural networks}). 
		\end{enumerate}
		The above errors are then evaluated on a \emph{test} parameter set $\Xi_{te} \subset \mathcal{P}$ consisting of $N_{te}$ randomly picked samples. Given that a sufficiently large number of test values is chosen, statistics for $\varepsilon_{\texttt{PODG}}^{}(L, \cdot)$, $\varepsilon_{\texttt{PODNN}}^{}(L, \cdot)$ and $\varepsilon_{\mathbb{V}}^{}(L, \cdot)$ on $\Xi_{te}$ can be reasonably assumed independent of the particular choice made for $\Xi_{te}$, thus making any subsequent error analysis reliable. In particular, in our numerical studies we let $N_{te}$ range from $50$ up to $100$, and we consider the average of the errors over the test data set, respectively denoted by
		\begin{equation*}
			\bar{\varepsilon}_{\texttt{PODG}}^{} = \bar{\varepsilon}_{\texttt{PODG}}^{}(L) = \dfrac{\sum_{\bg{\mu} \in \Xi_{te}} \varepsilon_{\texttt{PODG}}^{}(L, \bg{\mu})}{N_{te}} \, , \qquad \bar{\varepsilon}_{\texttt{PODNN}}^{} = \bar{\varepsilon}_{\texttt{PODNN}}^{}(L) = \dfrac{\sum_{\bg{\mu} \in \Xi_{te}} \varepsilon_{\texttt{PODNN}}^{}(L, \bg{\mu})}{N_{te}} \, , \qquad \bar{\varepsilon}_{\mathbb{V}}^{} = \bar{\varepsilon}_{\mathbb{V}}^{}(L) = \dfrac{\sum_{\bg{\mu} \in \Xi_{te}} \varepsilon_{\mathbb{V}}^{}(L, \bg{\mu})}{N_{te}} \, .
		\end{equation*}
		In the training phase of neural networks, we rely upon the well-known cross-validation procedure \cite{Koh95}. While the training samples $\Xi_{tr} \subset \mathcal{P}$ are generated via successive latin hypercube samplings (as explained in Subsection \ref{section:A non-intrusive RB method using neural networks}), the \emph{validation} inputs $\Xi_{va} \subset \mathcal{P}$ are randomly picked through a Monte Carlo sampling, as it is done for $\Xi_{te}$. The ratio between the size of $\Xi_{va}$ and $\Xi_{tr}$ is set to $0.3$. The training is then performed using the \emph{multiple restarts} approach to prevent the results to depend on the way the weights are (randomly) initialized; see, e.g., \cite{Kri07, Mat16}. Typically, five restarts are performed for each network topology. To ease the research for an optimal network configuration and by exploiting Cybenko's results (see Subsection \ref{section:Network topology}), we limit ourselves to three-layers neural networks, with the same number of neurons for both hidden layers. For each topology, the hyperbolic tangent is used as activation function for the hidden neurons.  
		
		All the results presented in the following subsections have been obtained via a MATLAB\textsuperscript{\textregistered} implementation of the FE, POD-G and POD-NN methods, with all the simulations carried out on a Lenovo laptop equipped with a CPU Intel Core i7 @ 2.50 GHz.
				
		
	%
	% Subsection 6.1 : Nonlinear Poisson equation
	%
	
	\subsection{Nonlinear Poisson equation}
	\label{section:Nonlinear Poisson equation}
		
		Despite a rather simple form, the Poisson equation has proved itself to be effective to model steady phenomena occurring in, e.g., electromagnetism, heat transfer, and underground flows \cite{MM10}. We consider the following version of the parametrized Poisson equation for a state variable $\wt{u} = \wt{u}(\bg{\mu})$:		
		\begin{subequations}
			\label{eq:poisson-differential}
			\begin{empheq}[left=\empheqlbrace]{align}
				\label{eq:poisson-differential-first-equation}
				- \wt{\nabla} \cdot \left( \wt{k}(\wt{\bg{x}}, \, \wt{u}(\bg{\mu}); \bg{\mu}_{ph}) ~ \wt{\nabla} \wt{u}(\bg{\mu}) \right) & = \wt{s}(\wt{\bg{x}}; \, \bg{\mu}_{ph}) & \hspace*{-2cm} \text{in $\wt{\Omega}(\bg{\mu}_g)$} \, , \\
				\wt{u}(\bg{\mu}) & = \wt{h}(\wt{\bg{\sigma}}; \, \bg{\mu}_{ph}) & \hspace*{-2cm} \text{on $\wt{\Gamma}_D$} \, , \\
				\wt{k}(\wt{\bg{\sigma}}, \, \wt{u}(\bg{\mu}); \bg{\mu}_{ph}) ~ \wt{\nabla} \wt{u}(\bg{\mu}) \cdot \wt{\bg{n}} & = 0 & \hspace*{-2cm} \text{on $\wt{\Gamma}_N$} \, .
			\end{empheq}
		\end{subequations}
		Here, for any $\bg{\mu}_g \in \mathcal{P}_g$, $\wt{\bg{x}}$ and $\wt{\bg{\sigma}}$ denote a generic point in $\wt{\Omega}$ and on $\wt{\Gamma}$, respectively, $\wt{\nabla}$ is the nabla operator with respect to $\wt{\bg{x}}$, $\wt{\bg{n}} = \wt{\bg{n}}(\wt{\bg{\sigma}})$ denotes the outward normal to $\wt{\Gamma}$ in $\wt{\bg{\sigma}}$, $\wt{k} ~ : ~ \wt{\Omega} \times \mathbb{R} \times \mathcal{P}_{ph} \rightarrow (0,\infty)$ is the diffusion coefficient, $\wt{s} ~ : ~ \wt{\Omega} \times \mathcal{P}_{ph} \rightarrow \mathbb{R}$ is the source term, and $\wt{h} ~ : ~ \wt{\Gamma}_D \times \mathcal{P}_{ph} \rightarrow \mathbb{R}$ encodes the Dirichlet boundary conditions. We should point out that, to ease the subsequent discussion, we limit the attention to homogeneous Neumann boundary constraints.
		
		Let us fix $\bg{\mu} \in \mathcal{P}$ and set 
		\begin{equation*}
			\wt{V} = \wt{V}(\bg{\mu}_g) = H^1_{\wt{\Gamma}_D} \big( \wt{\Omega}(\bg{\mu}_g) \big) = \big\lbrace v \in H^1 \big( \wt{\Omega}(\bg{\mu}_g) \big) ~ : ~ v \big\rvert_{\wt{\Gamma}_D} = 0 \big\rbrace \, ,
		\end{equation*}
		Multiplying \eqref{eq:poisson-differential-first-equation} by a \emph{test} function $v \in \wt{V}$, integrating over $\wt{\Omega}$, and exploiting integration by parts on the left-hand side, yields:
		\begin{equation}
			\label{eq:poisson-weak-derivation}
			\int_{\wt{\Omega}(\bg{\mu}_g)} \wt{k}(\wt{u}(\bg{\mu}); \, \bg{\mu}_{ph}) ~ \wt{\nabla} \wt{u}(\bg{\mu}) \cdot \wt{\nabla} v ~ d\wt{\Omega}(\bg{\mu}_g) = \int_{\wt{\Omega}(\bg{\mu}_g)} \wt{s}(\bg{\mu}_{ph}) ~ v ~ d\wt{\Omega}(\bg{\mu}_g) \, ,
		\end{equation}
		where we have omitted the dependence on the space variable $\wt{\bg{x}}$ for ease of notation. For the integrals in Eq. \eqref{eq:poisson-weak-derivation} to be well-defined, we require, for any $\bg{\mu}_g \in \mathcal{P}_g$, 
		\begin{equation*}
			\text{$|\wt{k}(\wt{\bg{x}}, \, r; \, \bg{\mu}_g)| < \infty$ for almost any (a.a.) $\wt{\bg{x}} \in \wt{\Omega}(\bg{\mu}_g), \, r \in \mathbb{R}$}  \hspace*{0.3cm} \text{and} \hspace*{0.3cm} \wt{s}(\bg{\mu}_{ph}) \in L^2 \big( \wt{\Omega}(\bg{\mu}_g) \big) \, .
		\end{equation*}
		Let then $\wt{l} = \wt{l}(\bg{\mu}) \in H^1 \big( \wt{\Omega}(\bg{\mu}_g) \big)$ be a \emph{lifting} function such that $\wt{l}(\bg{\mu}) \big\rvert_{\wt{\Gamma}_D} = \wt{h}(\bg{\mu}_{ph})$, with $\wt{h}(\bg{\mu}_{ph}) \in H^{1/2} \big( \wt{\Gamma}_D \big)$. We assume that such a function can be constructed, e.g., by interpolation of the boundary condition. Hence, upon defining
		\begin{equation}
			\label{eq:poisson-weak-forms}
			\begin{aligned}
				\wt{a}(w, \, v; \, \bg{\mu}) := & \int_{\wt{\Omega}(\bg{\mu}_g)} \wt{k}(w + \wt{l}(\bg{\mu}); \, \bg{\mu}_{ph}) ~ \wt{\nabla} w \cdot \wt{\nabla} v ~ d\wt{\Omega}(\bg{\mu}_g) + \int_{\wt{\Omega}(\bg{\mu}_g)} \wt{k}(w + \wt{l}(\bg{\mu}); \, \bg{\mu}_{ph}) ~ \wt{\nabla} \wt{l}(\bg{\mu}) \cdot \wt{\nabla} v ~ d\wt{\Omega}(\bg{\mu}_g) && \forall w, \, v \in \wt{V} \, , \\
				\wt{f}(v; \, \bg{\mu}) := & \int_{\wt{\Omega}(\bg{\mu}_g)} \wt{s}(\bg{\mu}_{ph}) ~ v ~ d\wt{\Omega}(\bg{\mu}_g) && \forall v \in \wt{V} \, ,
			\end{aligned}
		\end{equation}
		the weak formulation of problem \eqref{eq:poisson-differential} reads: given $\bg{\mu} \in \mathcal{P}$, find $\wt{u}(\bg{\mu}) \in \wt{V}(\bg{\mu}_g)$ such that
		\begin{equation}
			\label{eq:poisson-weak}
			\wt{a}(\wt{u}(\bg{\mu}), \, v; \, \bg{\mu}) = \wt{f}(v; \, \bg{\mu}) \hspace*{0.3cm} \forall v \in \wt{V}(\bg{\mu}_g) \, , 
		\end{equation}
		Then, the weak solution of problem \eqref{eq:poisson-differential} is given by $\wt{u}(\bg{\mu}) + \wt{l}(\bg{\mu})$. Note that using a lifting function makes the formulation \eqref{eq:poisson-weak} \emph{symmetric}, i.e., both the solution and the test functions are picked up from the same functional space \cite{Qua10}.
		
		%Lastly, let us remark that the weak formulation \eqref{eq:poisson-weak} can be cast in the form \eqref{eq:pde-variational-form} upon setting, for any $v \in \wt{V}(\bg{\mu}_g)$:
		%	\begin{equation*}
		%		\langle \wt{G}(\wt{u}(\bg{\mu}); \bg{\mu}_{ph}); v \rangle_{\wt{V},\wt{V}'} = \wt{g}(\wt{u}(\bg{\mu}), \, v; \, \bg{\mu}) = \wt{a}(\wt{u}(\bg{\mu}), \, v; \, \bg{\mu}) - \wt{f}(v; \, \bg{\mu}) \, .
		%	\end{equation*} 
		
		Let us now re-state the variational problem \eqref{eq:poisson-weak} onto the reference domain $\Omega$. For this purpose, let $\Gamma_D$ and $\Gamma_N$ be the portions of the boundary $\Gamma = \partial \Omega$ on which we impose Dirichlet and Neumann boundary conditions, respectively. Moreover, we denote by $\mathbb{J}_{\bg{\Phi}}(\cdot; \bg{\mu}_g)$ the Jacobian of the parametrized map $\bg{\Phi}(\cdot; \bg{\mu}_g)$, with determinant $\lvert \mathbb{J}_{\bg{\Phi}}(\cdot; \bg{\mu}_g) \rvert$, and we set
		\begin{equation*}
			k(\bg{x}, \, \cdot; \, \bg{\mu}) = \wt{k}(\bg{\Phi}(\bg{x}; \, \bg{\mu}_g), \, \cdot; \, \bg{\mu}_{ph}) \, , \hspace*{0.2cm} s(\bg{x}; \, \bg{\mu}) = \wt{s}(\bg{\Phi}(\bg{x}; \, \bg{\mu}_g); \, \bg{\mu}_{ph}) \hspace*{0.2cm} \text{and} \hspace*{0.2cm} h(\bg{x}; \bg{\mu}) = \wt{h}(\bg{\Phi}(\bg{x}; \bg{\mu}_g); \, \bg{\mu}_{ph}) \, .
		\end{equation*}
		Letting \[ V = H^1_{\Gamma_D}(\Omega) \] and exploiting standard change of variables formulas, the variational formulation of the Poisson problem \eqref{eq:poisson-differential} over $\Omega$ reads: given $\bg{\mu} \in \mathcal{P}$, find $u(\bg{\mu}) \in V$ such that
		\begin{equation*}
			\label{eq:poisson-weak-reference}
			a(u(\bg{\mu}), \, v; \, \bg{\mu}) = f(v; \, \bg{\mu}) \hspace*{0.3cm} \forall v \in V \, ,
		\end{equation*}
		with
		\begin{subequations}
			\label{eq:poisson-weak-forms-reference}
			\begin{align}
				\label{eq:poisson-weak-forms-reference-first}
				&
				\begin{aligned}
				a(w, \, v; \, \bg{\mu}) = & \int_{\Omega} k(w + l(\bg{\mu}); \, \bg{\mu}) ~ \mathbb{J}^{-T}_{\bg{\Phi}}(\bg{\mu}_g) \nabla w \cdot \mathbb{J}^{-T}_{\bg{\Phi}}(\bg{\mu}_g) \nabla v ~ \lvert \mathbb{J}_{\bg{\Phi}}(\bg{\mu}_g) \rvert \, d \Omega \\
				& + \int_{\Omega} k(w + l(\bg{\mu}); \, \bg{\mu}) ~ \mathbb{J}^{-T}_{\bg{\Phi}}(\bg{\mu}_g) \nabla l(\bg{\mu}) \cdot \mathbb{J}^{-T}_{\bg{\Phi}}(\bg{\mu}_g) \nabla v ~ \lvert \mathbb{J}_{\bg{\Phi}}(\bg{\mu}_g) \rvert \, d \Omega \, ,
				\end{aligned} \\
				\label{eq:poisson-weak-forms-reference-second}
				& f(v; \, \bg{\mu}) = \int_{\Omega} s(\bg{\mu}) ~ v ~ \lvert \mathbb{J}_{\bg{\Phi}}(\bg{\mu}_g) \rvert \, d \Omega \, ,  
			\end{align}
		\end{subequations}
		for any $w$, $v \in V$ and $\bg{\mu} \in \mathcal{P}$. Note that, as done in \eqref{eq:poisson-weak-forms}, we resort to a lifting function $l(\bg{\mu}) \in H^1(\Omega)$ with $l(\bg{\mu}) \big\rvert_{\Gamma_D} = g(\bg{\mu})$, such that the weak solution to problem \eqref{eq:poisson-differential} re-stated over $\Omega$ is obtained as $u(\bg{\mu}) + l(\bg{\mu})$.
		
		%Let us also remark that, when on a parameter-independent configuration, one can avoid to distinguish between physical and geometrical parameters, since even the latter now affect the integrands in \eqref{eq:poisson-weak-forms-reference-first} and \eqref{eq:poisson-weak-forms-reference-second}, and not the domain of integration.
		
		
	%
	% Subsection 6.1.1 : One-dimensional test case
	%
		
	\subsubsection{One-dimensional test case}
	\label{section:One-dimensional test case}
	
		\begin{figure}[b!]
			\center
			\includegraphics[scale = 0.385, trim = {1cm 9cm 1cm 3.5cm}, clip]{poisson1d_3_error_vs_rank}
			\includegraphics[scale = 0.385, trim = {1.5cm 9cm 1cm 3.5cm}, clip]{poisson1d_3_fe_vs_podnn}
			
			\vspace*{-0.2cm}
			
			\caption{Convergence analysis for the POD-G and POD-NN methods applied to problem \eqref{eq:poisson1d} (\emph{left}) and comparison between the FE and the POD-NN solutions for three parameter values (\emph{right}). These latter results have been obtained via a neural network with $H_1 = H_2 = 35$ units per hidden layer and employing $L = 10$ POD modes.}
			\label{fig:poisson1d-fig2}
		\end{figure}
			
		Consider the following BVP for the one-dimensional Poisson equation, featuring an exponential nonlinearity (in the diffusion coefficient) and involving three parameters:
		\begin{equation}
			\label{eq:poisson1d}
			\begin{cases}
				& - \big( \exp(u(\bg{\mu})) ~ u(\bg{\mu})' \big)' = s(x; \, \bg{\mu}) \hspace*{0.3cm} \text{in $\Omega = \left( -\dfrac{\pi}{2}, \, \dfrac{\pi}{2} \right)$} \, , \\[0.3cm]
				& u(\bg{\mu}) \big|_{x = \pm \nicefrac{\pi}{2}} = \mu_2 \left( 2 \pm \sin\left( \dfrac{\mu_1 \, \pi}{2} \right) \right) \exp \left( \pm \dfrac{\mu_3 \, \pi}{2} \right) \, .
			\end{cases}
		\end{equation}
		Here, $\bg{\mu} = \big(\mu_1, \, \mu_2, \, \mu_3 \big) \in \mathcal{P} = \big[ 1, \, 3 \big] \times \big[ 1, \, 3 \big] \times \big[ -0.5, \, 0.5 \big]$ and $s(\cdot; \bg{\mu})$ is defined such that $u_{ex}(x; \, \bg{\mu}) = \mu_2 \big( 2 + \sin(\mu_1 \, x) \big) \, \exp \big( \mu_3 \, x \big)$ is the exact solution to the problem for any $\bg{\mu} \in \mathcal{P}$. 
		
		%Figure \ref{fig:poisson1d-fig1} represents the first $10$ POD modes for the problem, yielded by singular value decomposition of $N = 100$ (linear) FE snapshots computed over a uniform grid of $100$ nodes. The inclusion of such modes within the standard POD-Galerkin RB framework enables an average relative error of about $10^{-5}$ on a test set $\Xi_{te}$ of $N_{te} = 100$ randomly chosen samples. This is whitnessed by Fig. \ref{fig:poisson1d-fig2}, which shows the exponential convergence of the solution with respect to the number of basis functions $L$ retained in the model (\emph{left}). The results for the POD-NN method employing three-layers neural networks with a varying number of neurons are also provided, together with the comparison with the FE solutions for some parameter values (\emph{right}). Note that when only a few modes are considered (e.g., $L \leq 5)$, the POD-NN scheme may lead to a significant reduction of the error with respect to the POD-G technique. Moreover, as we expand the training set, we obtain more accurate predictions for a larger number of POD coefficients, provided that we allow the size of the network to increase.
		
		%The left plot of Fig. \ref{fig:poisson1d-fig2} shows the exponential convergence of the POD-G and POD-NN solutions with respect to the number of basis functions $L$ retained in the model. For the POD-NN method, results refer to different amounts of training patterns and hidden neurons. First, note that including $L = 10$ POD modes within the reduced basis framework, both numerical procedures enable an average relative error of about $10^{-5}$ on a test set $\Xi_{te}$ of $N_{te} = 100$ randomly chosen samples. However, when only a few modes are considered (e.g., $L \leq 5)$, the POD-NN scheme may lead to a significant reduction of the error with respect to the POD-G technique. Moreover, as we expand the training set, we obtain more accurate predictions for a larger number of POD coefficients, provided that we allow the size of the network to properly increase. The reliability of the proposed RB method is also confirmed by the right plot of Fig. \ref{fig:poisson1d-fig2}, providing a comparison between the FE and POD-NN solutions for some parameter values.
		
		The left plot of Fig. \ref{fig:poisson1d-fig2} shows the convergence of the POD-G and POD-NN solutions with respect to the number of basis functions $L$ retained in the model, generated via POD of $N = 100$ FE snapshots computed over a uniform mesh of $100$ nodes. Observe how the performance of the POD-NN method highly depends on the number of training patterns and hidden neurons used. In fact, for $N_{tr} = 100$ and $H_1 = H_2 = 15$ (upward-pointing triangles), the method can well approximate only the first five generalized coordinates, actually providing more satisfactory results than POD-G does. Yet, the error stops decreasing for $L > 5$. However, as we expand the training set, we obtain more accurate predictions for a larger number of POD coefficients, provided that we allow the size of the network to properly increase. Particularly, employing $N_{tr} = 400$ training samples and $H_1 = H_2 = 35$ internal neurons (right-pointing triangles), the POD-NN error features an exponential decay for $L \leq 10$, resembling the projection error (squares) as desired. The reliability of the proposed RB method is also confirmed by the right plot of Fig. \ref{fig:poisson1d-fig2}, offering a comparison between the FE and POD-NN solutions for some parameter values.
				
		Diving deeper into the analysis of the POD-NN method, Fig. \ref{fig:poisson1d-fig3} offers the results concerning the search for an optimal three-layers network configuration using the methodology described in Section \ref{section:A non-intrusive RB method using neural networks}. The steps followed by the routine are represented by solid tracts; however, for the sake of completeness, we also report the test error (i.e., the minimum over multiple restarts) for any considered number of hidden neurons per layer ($H_1$ and $H_2$) and any dimension of the training set ($N_{tr}$). We remark that the basic assumption which the proposed routine relies on is fulfilled: as the number of available samples increases, the amount of neurons which allows to attain the minimum error increases as well, while the minimum error itself decreases.
		
		\begin{figure}[t!]
			\center
			\includegraphics[scale = 0.4, trim = {1cm 9cm 1.5cm 3.5cm}, clip]{poisson1d_3_nn_convergence}
			
			\caption{Error analysis for the POD-NN RB method applied to problem \eqref{eq:poisson1d} for several numbers of training samples. The solid sections represent the steps followed by the automatic routine desscribed in Section \ref{section:A non-intrusive RB method using neural networks}.}
			\label{fig:poisson1d-fig3}
		\end{figure}
		
		
	%
	% Subsection 6.1.2 : Two-dimensional test case
	%
	
	\subsubsection{Two-dimensional test case}
	\label{section:Two-dimensional test case}
		
		Let $\bg{\mu} = [\mu_1, \, \mu_2, \, \mu_3] \in \mathcal{P} = [-0.5, \, 0.5] \times [-0.5, \, 0.5] \times [1, \, 5]$ and $\wt{\Omega}(\bg{\mu})$ be the stenosis geometry reported on the left in Fig. \ref{fig:poisson2d-fig1}, parametrized in the depths $\mu_1$ and $\mu_2$ of the bottom and top restrictions (or inflations), respectively, and the length $\mu_3$ of the vessel. The Poisson problem we deal with in this section reads:
		\begin{equation}
			\label{eq:poisson2d}
			\begin{cases}
				& - \wt{\nabla} \cdot \big( \exp(\wt{u}(\bg{\mu})) ~ \wt{\nabla} \wt{u}(\bg{\mu}) \big) = \wt{s}(\wt{x}, \, \wt{y}) \hspace*{20pt} \text{in $\wt{\Omega}(\bg{\mu})$} \, , \\
				& \wt{u}(\bg{\mu}) = \wt{\sigma}_x \sin(\pi \, \wt{\sigma}_x) \cos(\pi \, \wt{\sigma}_y) \hspace*{33.5pt} \text{on $\partial \wt{\Omega}(\bg{\mu})$} \, .
			\end{cases}
		\end{equation}
		with the source term $\wt{s} = \wt{s}(\wt{x}, \, \wt{y})$ properly chosen so that the exact solution to the problem is given by \\ $\wt{u}_{ex}(\wt{x}, \, \wt{y}) = \wt{y} \sin(\pi \, \wt{x}) \cos(\pi \, \wt{y})$ for all $\bg{\mu} \in \mathcal{P}$. Albeit the state equation shows up an exponential nonlinearity and the computational domain presents curvilinear boundaries, both the RB methodologies studied in this work provide accurate solutions, close to the optimal one, i.e., $u_h^{\mathbb{V}}(\bg{\mu})$. %the projection of the truth solution $u_h(\bg{\mu})$ onto the reduced space $V_{\texttt{rb}}$. 
		This can be drawn from Fig. \ref{fig:poisson2d-fig2}, offering the finite element solution (\emph{top left}) to \eqref{eq:poisson2d} when $\bg{\mu} = (0.349, \, -0.413, \, 4.257)$, and the pointwise errors committed by either the projection onto $V_{\texttt{rb}}$ (\emph{top right}), or the POD-G method (\emph{bottom left}), or the POD-NN method (\emph{bottom right}). The computational mesh employed consists of $2792$ nodes, resulting in $N_h = 2632$ degrees of freedom (as many as the inner nodes) for the FE scheme. The reduced space $V_{\texttt{rb}}$ is generated by $L = 30$ basis functions, given by POD of $N = 100$ snapshots. Specifically, the results concerning the POD-NN method have been obtained by employing a neural network equipped with $H_1 = H_2 = 35$ neurons per hidden layer and trained with $N_{tr} = 200$ learning samples. In this way, the POD-NN procedure leads to an error field which shows similar patterns to those featured by the projection error field. This is not surprising, due to the way neural networks are trained within the POD-NN framework (see Section \ref{section:A non-intrusive RB method using neural networks}). 
		
		%\vspace*{-0.2cm}
						
		\begin{figure}[t!]
			\center
			\begin{minipage}{0.4\textwidth}
				\center
				\includegraphics[scale = 0.65, trim = {0 0 0 0.5cm}, clip]{domain_stenosis} 
			\end{minipage}
			\begin{minipage}{0.4\textwidth}
				\center
				\vspace*{-0.2cm}
				\includegraphics[scale = 0.65, trim = {0 0 0 0}, clip]{bc_square_poster}
			\end{minipage}
			
			\caption{The physical (\emph{left}) and reference (\emph{right}) domains for the Poisson problem \eqref{eq:poisson2d}.}
			\label{fig:poisson2d-fig1}	
		
			\center
			\subfloat{\includegraphics[scale = 0.275, trim = {1cm 3cm 4cm 2.5cm}, clip]{poisson2d_3_fe_solution}}
			\subfloat{\includegraphics[scale = 0.275, trim = {1cm 3cm 4cm 2.5cm}, clip]{poisson2d_3_error_projection}} \\[-0.2cm]
			\subfloat{\includegraphics[scale = 0.275, trim = {1cm 3cm 4cm 2.5cm}, clip]{poisson2d_3_error_podg}}
			\subfloat{\includegraphics[scale = 0.275, trim = {1cm 3cm 4cm 2.5cm}, clip]{poisson2d_3_error_podnn}}
			
			%\vspace*{-0.2cm}
			
			\caption{FE solution (\emph{top left}) to the Poisson problem \eqref{eq:poisson2d} with $\bg{\mu} = (0.349, \, -0.413, \, 4.257)$, and pointwise errors yielded by either its projection onto $V_{\texttt{rb}}$ (\emph{top right}), or the POD-G method (\emph{bottom left}), or the POD-NN method (\emph{bottom right}). The results have been obtained by employing $L = 30$ POD modes.}
			\label{fig:poisson2d-fig2}
		\end{figure}	
		
		\iffalse
		\begin{figure}[t]
			\center
			\includegraphics[scale = 0.4, trim = {1.5cm 9.6cm 1.5cm 3.8cm}, clip]{poisson2d_3_error_vs_rank}
			\hspace*{1cm}
			\includegraphics[scale = 0.385, trim = {1.5cm 8.95cm 1.5cm 3.8cm}, clip]{poisson2d_3_time}
			
			\caption{Error analysis (\emph{left}) and online CPU time (\emph{right}) for the POD-G and the POD-NN methods applied to problem \eqref{eq:poisson2d}. $N_{te} = 50$ randomly picked parameter values are considered. The reduced basis have been generated via POD, relying on $N = 100$ snapshots. The second plot refers to RB models including $L = 30$ modal functions; within the POD-NN framework, a neural network emboding $35$ neurons per inner layer has been used.}
			\label{fig:poisson2d-fig3}
		\end{figure}
		\fi
		
		%\vspace*{-0.2cm}
		
		A quantitative evidence of the efficacy of the RB approximations is given in the plot on the left in Fig. \ref{fig:poisson2d-fig3}, which reports a convergence analysis for both the POD-G and the POD-NN methods with respect to the number $L$ of retained modal basis functions. As usual, the error has to be intended as an average over a parameter data set $\Xi_{te}$, here consisting of $N_{te} = 50$ samples. However, the second plot of Fig. \ref{fig:poisson2d-fig3} reveals how the former scheme produces a response for any online query approximately $10^3$ times slowlier than the proposed POD-NN procedure does, preventing any computational gain with respect to the finite element scheme. %This assertion can also be extended to the memory demand. In fact, in addition to the assembly of the full-order residual vector $\mathbf{G}_h(\cdot;\bg{\mu}) \in \mathbb{R}^{N_h}$ and the associated Jacobian $\mathbb{J}_h(\cdot;\bg{\mu}) \in \mathbb{R}^{{N_h} \times {N_h}}$, the online phase of the POD-G RB method requires the storage of the POD basis $\mathbb{V} \in \mathbb{R}^{{N_h} \times L}$. Here, ${N_h} = 2632$ and $L = 30$, with the (sparse) Jacobian featuring $18388$ non-zero entries. Hence, recalling that a float occupies $4$ bytes in memory, the memory footprint approximately amounts to $400$ Kb. Instead, the POD-NN method requires only the underpinning neural network to tackle a query. Therefore, assuming $35$ computing units within both inner layers, we need a bit more than $9$ Kb of RAM, coincident with the space necessary to save the synaptical and bias weights characterizing the network.
		
		Let us further analyze the first plot of Fig. \ref{fig:poisson2d-fig3}. Within the POD-NN framework, when the proper orthogonal decomposition of a set of $N = 100$ snapshots is coupled with a three-layers perceptron with $35$ neurons per hidden layer, trained to approximate the map \eqref{eq:map-to-approximate} relying on $N_{tr} = 200$ learning patterns, for each $L \leq 30$ the relative error is smaller than the error committed by the POD-Galerkin procedure. Yet, halfing the number of learning patterns employed, the POD-NN error curve stops decreasing at $L = 20$, then incurring in a plateu. In this regard, Fig. \ref{fig:poisson2d-fig4} suggests that as the size of the available training set decreases, one should coherently reduce the amount of computing units to be embodied in the network, thus to limit the risk of overfitting. Particularly, when only $N_{tr} = 100$ training samples are used, the optimal network configuration comprises $20$ neurons per internal layer (\emph{left}). However, doubling the dimension of the training set, the same configuration does not allow to fully exploit the augmented amount of information. In that case, a network with, e.g., $35$ neurons per hidden layer is preferable (\emph{right}). 
						
		\begin{figure}[t]
			\center
			\includegraphics[scale = 0.395, trim = {1.5cm 9.6cm 1.5cm 3.8cm}, clip]{poisson2d_3_error_vs_rank}
			\hspace*{0.8cm}
			\includegraphics[scale = 0.38, trim = {1.5cm 8.95cm 1.5cm 3.8cm}, clip]{poisson2d_3_time}
			
			\caption{Error analysis (\emph{left}) and online CPU time (\emph{right}) for the POD-G and the POD-NN methods applied to problem \eqref{eq:poisson2d}. $N_{te} = 50$ randomly picked parameter values are considered. The reduced basis have been generated via POD, relying on $N = 100$ snapshots. The second plot refers to RB models including $L = 30$ modal functions; within the POD-NN framework, a neural network emboding $35$ neurons per inner layer has been used.}
			\label{fig:poisson2d-fig3}
			
			\vspace*{0.3cm}
			
			\center
			\includegraphics[scale = 0.38, trim = {1.5cm 9.6cm 1.5cm 3.8cm}, clip]{poisson2d_3_nn_convergence}
			\hspace*{1cm}
			\includegraphics[scale = 0.385, trim = {1.5cm 9.8cm 1.5cm 3.8cm}, clip]{poisson2d_3_nn_comparison}
						
			\caption{Convergence analysis with respect to the number of hidden neurons (\emph{left}) and modal functions (\emph{right}) used within the POD-NN framework applied to problem \eqref{eq:poisson2d}. The results provided in the first plot have been obtained using $L = 30$ modes; the solid tracts refer to the steps performed by the automatic routine carried out to find an optimal network configuration.}
			\label{fig:poisson2d-fig4}
		\end{figure}	
		
		The distinguishing and novel feature of the POD-NN method is represented by the employment of multi-layer perceptrons to recover the coefficients of the reduced model. To motivate this choice, let us pursue a more traditional approach in the interpolation step by resorting to cubic splines \cite{Deb78}. To this end, let $\Xi_{\delta} \subset \mathcal{P}$ be a tensor-product grid on the parameter domain, based on, e.g., Chebyshev nodes. %\footnote{For a given natural number $n$, the Chebyshev nodes in the interval $(a, \, b)$ are \[ x_k = \dfrac{1}{2}(a + b) + \dfrac{1}{2}(b - a) \, \cos \left( \dfrac{2k - 1}{2n} \pi \right) \hspace*{0.3cm} \text{for $k = 1, \, \ldots \, , n$} \, . \]}. 
		In the offline phase, for each $\bg{\mu}_{\delta} \in \Xi_{\delta}$, we compute the truth solution $\mathbf{u}_h(\bg{\mu}_{\delta}) \in \mathbb{R}^{N_h}$ and we extract the expansion coefficients $\mathbb{V}^T \mathbf{u}_h(\bg{\mu}_{\delta}) \in \mathbb{R}^L$. At the online stage, given a new parameter value $\bg{\mu} \in \mathcal{P}$, the $i$-th expansion coefficient, $i = 1, \, \ldots \, , L$, is sought by cubic spline interpolation of the samples \[ \big\lbrace \big( \bg{\mu}_{\delta}, \, \big( \mathbb{V}^T \mathbf{u}_h(\bg{\mu}_{\delta}) \big)_i \big) \big\rbrace_{\bg{\mu}_{\delta} \in \Xi_{\delta}} \, . \] Hence, denoting by $\mathbf{u}_{\texttt{rb}}^{\texttt{CS}}(\bg{\mu}) \in \mathbb{R}^L$ the so-constructed approximation of $\mathbb{V}^T \mathbf{u}_h(\bg{\mu})$, the reduced-order approximation of $\mathbf{u}_h(\bg{\mu})$ is given by $\mathbf{u}_L^{\texttt{CS}}(\bg{\mu}) = \mathbb{V} \, \mathbf{u}_{\texttt{rb}}^{\texttt{CS}}(\bg{\mu})$. Similarly to the POD-G and the POD-NN method, the accuracy of the resulting POD-CS procedure can be assessed by evaluating and averaging the relative error $\varepsilon_{\texttt{PODCS}}^{}(L, \, \bg{\mu})$, defined as
		\begin{equation*}
			\label{eq:podcs-error}
			\varepsilon_{\texttt{PODCS}}^{}(L, \, \bg{\mu}) = \dfrac{\norm{\mathbf{u}_h(\bg{\mu}) - \mathbf{u}_L^{\texttt{CS}}(\bg{\mu})}}{\norm{\mathbf{u}_h(\bg{\mu})}} = \dfrac{\norm{\mathbf{u}_h(\bg{\mu}) - \mathbb{V} \, \mathbf{u}_{\texttt{rb}}^{\texttt{CS}}(\bg{\mu})}}{\norm{\mathbf{u}_h(\bg{\mu})}} \, ,
		\end{equation*}
		on a test parameter set $\Xi_{te} \subset \mathcal{P}$, with $\Xi_{te} \cap \Xi_{\delta} = \emptyset$.
				
		For the Poisson problem \eqref{eq:poisson2d}, we report in the left plot of Fig. \ref{fig:poisson2d-fig5} the relative errors (averaged on $\Xi_{te}$) yielded by both the POD-CS and the POD-NN methods. For the former, we provide the results obtained on tensor-product grids consisting of $N_{\delta}$ interpolation points, with $N_{\delta} \in \lbrace 5^3, \, 7^3, \, 10^3 \rbrace$. For the latter, $H_1 = H_2 = 35$ neurons per hidden layer and $N_{tr} = 200$ training patterns have been used. Although the online performance of the two procedures are basically the same, as shown by the second plot in Fig. \ref{fig:poisson2d-fig5}, we observe that the level of predictive accuracy enabled by the POD-NN method can be attained or at least approached by cubic spline interpolation only when this relies on $N_{\delta} = 1000$ samples. In fact, as mentioned in the Introduction, a standard interpolation technique may require a large number of samples, thus snapshots, to be able to enforce the constraints characterizing the nonlinear manifolds which the reduced bases typically belong to \cite{Ams10}. Hence, although this approach provides a valuable alternative for paremetrized problems with a few parameters, it may be unfeasible in real-life applications involving hundreads of parameters. Conversely, the secret hope behind the choice of a neural network-based regression is that the amount of required snapshots properly scales with the dimension of the parameter space. This would justify the overheads due to the training phase.
		
		\begin{figure}[t!]
			\center
			\includegraphics[scale = 0.39, trim = {1.5cm 9.34cm 1.5cm 3.8cm}, clip]{poisson2d_3_cs_error}
			\hspace*{1cm}
			\includegraphics[scale = 0.38, trim = {1.5cm 8.9cm 1.5cm 3.8cm}, clip]{poisson2d_3_cs_time}
			
			\caption{Average relative errors (\emph{left}) and online run times (\emph{right}) on $\Xi_{te}$ for the POD-CS and the POD-NN methods applied to problem \eqref{eq:poisson2d}. For the latter, the results refer to an MLP equipped with $H_1 = H_2 = 35$ neurons per hidden layer.}
			\label{fig:poisson2d-fig5}
		\end{figure}
		
		
	%
	% Subsection 6.2 : Steady uncompressible Navier-Stokes equations
	%
		
	\subsection{Steady uncompressible Navier-Stokes equations}
	\label{section:Steady uncompressible Navier-Stokes equations}
	
		The system of the Navier-Stokes equations model the conservation of mass and momentum for an incompressible Newtonian viscous fluid confined in a region $\wt{\Omega}(\bg{\mu}_g) \subset \mathbb{R}^d$, $d = 2, \, 3$ \cite{Ran99}. Letting $\wt{\bg{v}} = \wt{\bg{v}}(\wt{\bg{x}}; \, \bg{\mu})$ and $\wt{p} = \wt{p}(\wt{\bg{x}}; \, \bg{\mu})$ be respectively the velocity and pressure of the fluid, the parametrized steady version of the Navier-Stokes equations considered in this work reads
		\begin{subequations}
			\label{eq:ns-differential}
			\begin{empheq}[left=\empheqlbrace]{align}
				\label{eq:mass-conservation}
				\wt{\nabla} \cdot \wt{\bg{v}}(\bg{\mu}) & = 0 & \hspace*{-2cm} \text{in $\wt{\Omega}(\bg{\mu}_g)$} \, , \\
				\label{eq:momentum-conservation}
				- \nu(\bg{\mu}) ~ \wt{\Delta} \wt{\bg{v}}(\bg{\mu}) + (\wt{\bg{v}}(\bg{\mu}) \cdot \wt{\nabla}) \wt{\bg{v}}(\bg{\mu}) + \dfrac{1}{\rho(\bg{\mu})} \wt{\nabla} \wt{p}(\bg{\mu}) & = \bg{0} & \hspace*{-2cm} \text{in $\wt{\Omega}(\bg{\mu}_g)$} \, , \\
				\wt{\bg{v}}(\bg{\mu}) & = \wt{\bg{h}}(\bg{\mu}_{ph}) & \hspace*{-2cm} \text{on $\wt{\Gamma}_D(\bg{\mu}_g)$} \, , \\
				\wt{p}(\bg{\mu}) \wt{\bg{n}} - \nu(\bg{\mu}) \wt{\nabla} \wt{\bg{v}}(\bg{\mu}) \cdot \wt{\bg{n}} & = \bg{0} & \hspace*{-2cm} \text{on $\wt{\Gamma}_N(\bg{\mu}_g)$} \, .
			\end{empheq}
		\end{subequations}
		Here, $\wt{\bg{h}}(\bg{\mu}_{ph})$ denotes the velocity field prescribed on $\wt{\Gamma}_D(\bg{\mu}_g)$, whereas homogeneous Neumann conditions are applied on $\wt{\Gamma}_N(\bg{\mu}_g)$. Furthermore, $\rho(\bg{\mu})$ and $\nu(\bg{\mu})$ represent the uniform density and kinematic viscosity of the fluid, respectively. Note that, despite these quantities encode physical properties, we let them depend on the geometrical parameters as well. Indeed, fluid dynamics can be characterized (and controlled) by means of a wealth of dimensionless quantities, e.g., the Reynolds number, which combine physical properties of the fluid with geometrical features of the domain. Therefore, a numerical study of the sensitivity of the system \eqref{eq:ns-differential} with respect to $\bg{\mu}_g$ may be carried out by adapting either $\rho(\bg{\mu})$ or $\nu(\bg{\mu})$ as $\bg{\mu}_g$ varies, so to preserve a dimensionless quantity of interest; we refer the reader to Subsection \ref{section:The lid-driven cavity problem} for a practical example. 
		%For our purpose, it is worth noticing that the nonlinearity of problem \eqref{eq:ns-differential} lies in the convective term
		%\begin{equation*}
		%	\big( \wt{\bg{v}}(\bg{\mu}) \cdot \wt{\nabla} \big) \wt{\bg{v}}(\bg{\mu}) \, ,
		%\end{equation*}
		%which gives rise to a \emph{quadratic} nonlinearity. %Conversely, the other terms appearing in the momentum equation \eqref{eq:momentum-conservation} and in Eq. \eqref{eq:mass-conservation}, which enforces mass conservation \cite{QMN15}, are linear in the solution $(\wt{\bg{v}}(\bg{\mu}), \, \wt{p}(\bg{\mu}))$.
		
		To write the differential system \eqref{eq:ns-differential} in weak form over a $\bg{\mu}_g$-independent configuration $\Omega$, let us introduce the velocity and pressure spaces \[ \wt{X}(\bg{\mu}_g) = \big[ H^1_{\wt{\Gamma}_D} \big( \wt{\Omega}(\bg{\mu}_g) \big) \big]^d ~~ \text{and} ~~ \wt{Q}(\bg{\mu}_g) = L^2 \big( \wt{\Omega}(\bg{\mu}_g) \big) \, , \] respectively, with \[ X = \big[ H^1_{\Gamma_D} \big( \Omega \big) \big]^d ~~ \text{and} ~~ Q = L^2 \big( \Omega \big) \] be their respective counterparts over $\Omega$. By multiplying \eqref{eq:ns-differential} for test functions $\big( \wt{\bg{\chi}}, \, \wt{\xi} \big) \in \wt{V}(\bg{\mu}_g) = \wt{X}(\bg{\mu}_g) \times \wt{Q}(\bg{\mu}_g)$, integrating by parts and then tracing everything back onto $\Omega$ by means of the parametrized map $\bg{\Phi}(\cdot; \, \bg{\mu}_g)$, we end up with the following parametrized weak variational problem: given $\bg{\mu} \in \mathcal{P}$, find $u(\bg{\mu}) = (\bg{v}(\bg{\mu}), \, p(\bg{\mu})) \in V = X \times Q$ so that
		\begin{equation*}
			\label{eq:ns-weak-reference}
			\begin{aligned}
				a(\bg{v}(\bg{\mu}), \, \bg{\chi}; \, \bg{\mu}) + c(\bg{v}(\bg{\mu}), \, \bg{v}(\bg{\mu}), \, \bg{\chi}; \, \bg{\mu}) + d(\bg{v}(\bg{\mu}), \, \bg{\chi}; \, \bg{\mu}) + b(p(\bg{\mu}), \, \nabla \cdot \bg{\chi}; \, \bg{\mu}) & = f_1(\bg{\chi}; \, \bg{\mu}) \, , \\
				b(\nabla \cdot \bg{v}(\bg{\mu}), \, \xi; \, \bg{\mu}) & = f_2(\xi; \, \bg{\mu}) \, ,
			\end{aligned}
		\end{equation*}
		for all $(\bg{\chi}, \, \xi) \in V$, with, for any $\bg{\mu} \in \mathcal{P}$,	
		\begin{equation*}
			\label{eq:ns-weak-forms}
			\begin{aligned}
				& c(\bg{\psi}, \, \bg{\chi}, \, \bg{\eta}; \, \bg{\mu}) = \int_{\Omega} \left( \bg{\psi} \cdot \mathbb{J}^{-T}_{\bg{\Phi}}(\bg{\mu}_g) \nabla \right) \bg{\chi} \cdot \bg{\eta} ~ \lvert \mathbb{J}_{\bg{\Phi}}(\bg{\mu}_g) \rvert \, d \Omega \, , \qquad && d(\bg{\psi}, \, \bg{\chi}; \, \bg{\mu}) = c(\bg{l}(\bg{\mu}), \, \bg{\psi}, \, \bg{\chi}; \, \bg{\mu}) + c(\bg{\psi}, \, \bg{l}(\bg{\mu}), \, \bg{\chi}; \, \bg{\mu}) \, , \\[0.1cm]
				& a(\bg{\psi}, \, \bg{\chi}; \, \bg{\mu}) = \nu(\bg{\mu}) \int_{\Omega} \mathbb{J}^{-T}_{\bg{\Phi}}(\bg{\mu}_g) \nabla \bg{\psi} ~ : ~ \mathbb{J}^{-T}_{\bg{\Phi}}(\bg{\mu}_g) \nabla \bg{\chi} ~ \lvert \mathbb{J}_{\bg{\Phi}}(\bg{\mu}_g) \rvert \, d \Omega \, , && f_1(\bg{\psi}; \, \bg{\mu}) = - a(\bg{l}(\bg{\mu}), \, \bg{\psi}; \, \bg{\mu}) - c(\bg{l}(\bg{\mu}), \, \bg{l}(\bg{\mu}), \, \bg{\psi}; \, \bg{\mu}) \, , \\[0.2cm]
				& b(\bg{\psi}, \, \xi; \, \bg{\mu}) = - \dfrac{1}{\rho(\bg{\mu})} \int_{\Omega} \left( \mathbb{J}^{-T}_{\bg{\Phi}}(\bg{\mu}_g) \nabla \cdot \bg{\psi} \right) ~ \xi ~ \lvert \mathbb{J}_{\bg{\Phi}}(\bg{\mu}_g) \rvert \, d \Omega \, , && f_2(\xi; \, \bg{\mu}) = - b(\bg{l}(\bg{\mu}), \, \xi; \, \bg{\mu}) \, ,
			\end{aligned}
		\end{equation*}
		for all $\bg{\psi}$, $\bg{\chi}$, $\bg{\eta} \in X$ and $\xi \in Q$. In the definitions of $d(\cdot, \, \cdot; \, \bg{\mu})$, $f_1(\cdot; \, \bg{\mu})$ and $f_2(\cdot; \, \bg{\mu})$, $\bg{l}(\bg{\mu}) \in \big[ H^1(\Omega) \big]^d$ denotes the lifting vector field, with $\bg{l}(\bg{\mu}) \big\rvert_{\Gamma_D} = \bg{h}(\bg{\mu})$, $\bg{h}(\bg{x}; \, \bg{\mu}) = \wt{\bg{h}}(\bg{\Phi}(\bg{x}; \, \bg{\mu}_g); \, \bg{\mu}_{ph})$ being the velocity field prescribed on $\Gamma_D$. Hence, the weak solution to \eqref{eq:ns-differential} defined over the fixed domain $\Omega$ is given by $\big( \bg{v}(\bg{\mu}) + \bg{l}(\bg{\mu}), p(\bg{\mu}) \big)$.
	
	
	%
	% Subsection 6.2.1 : The lid-driven cavity problem
	%
	
	\subsubsection{The lid-driven cavity problem}
	\label{section:The lid-driven cavity problem}
		
		In this section, we are concerned with the numerical simulation of a viscous flow within a parallelogram-shaped cavity, illustrated in Fig. \ref{fig:dc-domain} (\emph{left}). The geometry of the domain is affected by three parameters: $\mu_1 \in [1, \, 2]$ and $\mu_2 \in [1, \, 2]$ define the length of the horizontal and slanting (possibly vertical) edges, respectively, whereas $\mu_3 \in \big[ \nicefrac{\pi}{6}, \, \nicefrac{5 \pi}{6} \big]$ denotes the angle between the oblique sides and the positive $\wt{x}$-semiaxis. The Dirichlet boundary conditions enforced on the velocity field, graphically represented in Fig. \ref{fig:dc-domain} (\emph{center}), are such that the flow is driven by a unit horizontal velocity at the top boundary \cite{Per02}. For this reason, this benchmark is typically referred to as the \emph{lid-driven cavity} problem. In addition, to retain the uniqueness of the solution, we fix the pressure at the lower-left corner \cite{Dho14}. Therefore, to properly resolve the steep velocity gradient near the upper boundary and the pressure singularities at the top corners, the computational mesh $\Omega_h$ (Fig. \ref{fig:dc-mesh}, \emph{right}) is refined in the upper part of the domain. %, leading to ${N_h}_{\bg{v}} = 4868$ degrees of freedom for $\bg{v}$ and ${N_h}_p = 691$ nodal values for $p$. 
		
		For the Navier-Stokes equations, a suitable choice of the FE spaces is crucial to fulfill the well-known \emph{inf-sup} stability condition \cite{Ran99}. At this regard, a common and effective choice consists in using quadratic finite elements for the components of the velocity field and linear finite elements for the pressure, leading to the so-called $\mathbb{P}^2 - \mathbb{P}^1$ (or Taylor-Hood) FE discretization \cite{Per02}. Figure \ref{fig:dc-solutions-different-domains} shows the velocity streamlines (\emph{top}) and the pressure distribution (\emph{bottom}) obtained through the resulting FE method for, from the left to the right, $\bg{\mu} = \big( 1, \, \nicefrac{2}{\sqrt{3}}, \, \nicefrac{2 \pi}{3} \big)$, $\bg{\mu} = \big( 1, \, 1, \, \nicefrac{\pi}{2} \big)$ or $\bg{\mu} = \big( 1, \, \nicefrac{2}{\sqrt{3}}, \, \nicefrac{\pi}{6} \big)$. Observe how the moving lid induces a large velocity gradient close to the top boundary, while it only slightly affects the fluid in the lower part of the cavity. In turn, this gives rising to two vortices, whose extent highly depends on the shape of the domain, or, equivalently, $\bg{\mu}$. Conversely, the pressure field does not undergo remarkable alterations across the parameter space. For all the configurations, a low-pressure region takes place at the upper-left corner of the domain and in the center of the major vortex (although it may be not clearly visible). Instead, the upper-right corner represents a stagnation point for the $\wt{x}$-velocity, and so therein the pressure assumes larger values than in the rest of the cavity \cite{Dho14}.
		
		\begin{figure}[t!]
			\center
			\begin{minipage}{0.32\textwidth}
				\center
				\includegraphics[scale = 0.6]{dc_domain_poster}
			\end{minipage}
			\begin{minipage}{0.32\textwidth}
				\center
				\includegraphics[scale = 0.6]{dc_domain_bc_poster}
			\end{minipage}
			\begin{minipage}{0.32\textwidth}
				\center
				\includegraphics[scale = 0.275, trim = {2cm 9cm 1.5cm 3.5cm}, clip]{dc_mesh}
			\end{minipage}
			
			\caption{Computational domain (\emph{left}), enforced velocity at the boundaries (\emph{center}) and computational mesh $\Omega_h$ (\emph{right}) for the lid-driven cavity problem for the uncompressible Navier-Stokes equations.}
			\label{fig:dc-domain}
			
			\center 
			\subfloat[$\bg{\mu} = \big( 1, \, \nicefrac{2}{\sqrt{3}}, \, \nicefrac{2 \pi}{3} \big)$]{\includegraphics[scale = 0.25, trim = {6.5cm 0 5.5cm 0}, clip]{dc_400_vel_fem_solution_1}}
			\subfloat[$\bg{\mu} = \big( 1, \, 1, \, \nicefrac{\pi}{2} \big)$]{\includegraphics[scale = 0.25, trim = {7.5cm 0 7.5cm 0}, clip]{dc_400_vel_fem_solution_2}}
			\subfloat[$\bg{\mu} = \big( 1, \, \nicefrac{2}{\sqrt{3}}, \, \nicefrac{\pi}{3} \big)$]{\includegraphics[scale = 0.25, trim = {4.8cm 0 5.5cm 0}, clip]{dc_400_vel_fem_solution_3}} \\
			\subfloat[$\bg{\mu} = \big( 1, \, \nicefrac{2}{\sqrt{3}}, \, \nicefrac{2 \pi}{3} \big)$]{\includegraphics[scale = 0.25, trim = {6.5cm 0 5.5cm 0}, clip]{dc_400_p_fem_solution_1}}
			\subfloat[$\bg{\mu} = \big( 1, \, 1, \, \nicefrac{\pi}{2} \big)$]{\includegraphics[scale = 0.25, trim = {7.5cm 0 7.5cm 0}, clip]{dc_400_p_fem_solution_2}}
			\subfloat[$\bg{\mu} = \big( 1, \, \nicefrac{2}{\sqrt{3}}, \, \nicefrac{\pi}{3} \big)$]{\includegraphics[scale = 0.25, trim = {5cm 0 5.5cm 0}, clip]{dc_400_p_fem_solution_3}}
					
			\caption{Velocity streamlines (\emph{top}) and pressure distribution (\emph{bottom}) for the lid-driven cavity problem, computed through the FE method. Three different parameter values are considered; for all configurations, the Reynold's number is $400$.}
			\label{fig:dc-solutions-different-domains}
		\end{figure}
		
		All the results presented in Fig. \ref{fig:dc-solutions-different-domains} refer to a Reynold's number of $400$. The Reynold's number is an adimensional group, defined as the ratio between the inertia and the viscous (or friction) force. For the specific problem at hand, it reads:
		\begin{equation*}
			\vspace*{-0.1cm}
			Re = \dfrac{\max \big\lbrace \mu_1, \, \mu_2 \big\rbrace}{\nu(\bg{\mu})} \, ,
		\end{equation*}
		with $\nu = \nu(\bg{\mu})$ being the dynamic viscosity of the fluid. In our analyses, $\nu(\bg{\mu})$ is adapted as the geometry varies so that the Reynold's number is either $200$ or $400$. %\vspace*{0.2cm} \footnote{We would like to point out that these values for the Reynold's number have been chosen sufficiently low, so that the flow can be reasonably treated as steady.}.
																			
		Regarding the reduced-order modeling of the parametrized problem of interest, we slightly distance ourselves from the general workflow depicted in Sections \ref{section:Projection-based reduced basis method} and \ref{section:A non-intrusive RB method using neural networks}. Indeed, for the Navier-Stokes equations is convenient to construct two separate reduced spaces \cite{Bal14, Chen17, QMN15}: $X_{\texttt{rb}}$, with dimension $L_{\bg{v}}$, for the velocity field and $Q_{\texttt{rb}}$, with dimension $L_p$, for the pressure distribution, respectively represented by the matrices 
		\begin{equation*}
			\mathbb{V}_{\bg{v}} = \left[ \bg{\psi}_1^{\bg{v}} \, \big| \, \ldots \, \big| \, \bg{\psi}_{L_{\bg{v}}}^{\bg{v}} \right] \in \mathbb{R}^{{N_h}_{\bg{v}} \times L_{\bg{v}}} \hspace*{0.5cm} \text{and} \hspace*{0.5cm} \mathbb{V}_p = \left[ \bg{\psi}_1^p \, \big| \, \ldots \, \big| \, \bg{\psi}_{L_p}^p \right] \in \mathbb{R}^{{N_h}_p \times L_p} \, ,
		\end{equation*} 
		with ${N_h}_{\bg{v}}$ (respectively, ${N_h}_p$) the size of the FE velocity (resp., pressure) space. 
		
		Actually, although the underpinning finite element solver has been designed to fulfill the inf-sup condition, thus ensuring the stability of the scheme, when dealing with incompressible flows this property may not be automatically inherited by the POD-Galerkin reduced-order solver, which as a result may show severe instabilities \cite{Bur06}. Indeed, one has to carefully choose the reduced velocity space $X_{\texttt{rb}}$ so to meet an equivalent of the inf-sup condition at the reduced level. To this end, in our simulations we resort to a \emph{supremizer enrichment} of $X_{\texttt{rb}}$, as proposed in \cite{Bal14} (to which we refer the readers which are interested in the rational and details of this approach). At each pressure snapshot $\mathbf{p}_h \big( \bg{\mu}^{(n)} \big) \in \mathbb{R}^{{N_h}_p}$, $n = 1, \, \ldots \, , N$, we associate the \emph{supremizer solution} $\mathbf{s}_h \big( \bg{\mu}^{(n)} \big) \in \mathbb{R}^{{N_h}_{\bg{v}}}$, defined as the solution to a linear system of the form
		\begin{equation*}
			\mathbb{A} \, \mathbf{s}_h \big( \bg{\mu}^{(n)} \big) = \mathbb{B} \big( \bg{\mu}^{(n)} \big) \, \mathbf{p}_h \big( \bg{\mu}^{(n)} \big) \, ,
		\end{equation*}
		with $\mathbb{A} \in \mathbb{R}^{{N_h}_{\bg{v}} \times {N_h}_{\bg{v}}}$ and $\mathbb{B} \big( \bg{\mu}^{(n)} \big) \in \mathbb{R}^{{N_h}_{\bg{v}} \times {N_h}_p}$. Letting $\mathbb{V}_{\bg{s}} = \left[ \bg{\psi}_1^{\bg{s}} \, \big| \, \ldots \, \big| \, \bg{\psi}_{L_{\bg{s}}}^{\bg{s}} \right] \in \mathbb{R}^{{N_h}_{\bg{v}} \times L_{\bg{s}}}$ be the POD basis of rank $L_{\bg{s}}$ extracted from the ensemble of snapshots $\lbrace \mathbf{s} \big( \bg{\mu}^{(1)} \big) , \, \ldots \, , \mathbf{s} \big( \bg{\mu}^{(N)} \big) \rbrace$, the POD-G approximation of the velocity field is then sought in the form
		\begin{equation*}
			\bar{\mathbf{v}}_L(\bg{\mu}) = \overline{\mathbb{V}}_{\bg{v}} \, \bar{\mathbf{v}}_{\texttt{rb}}(\bg{\mu}) = \sum_{i = 1}^{L_{\bg{v}}} v_{\texttt{rb}}^{(i)} \bg{\psi}_i^{\bg{v}} + \sum_{i = 1}^{L_{\bg{s}}} s_{\texttt{rb}}^{(i)} \bg{\psi}_i^{\bg{s}} \, ,
		\end{equation*} 
		where
		\begin{equation*}
			\overline{\mathbb{V}}_{\bg{v}} = \left[ \mathbb{V}_{\bg{v}} ~ \mathbb{V}_{\bg{s}} \right] \in \mathbb{R}^{{N_h}_{\bg{v}} \times (L_{\bg{v}} + L_{\bg{s}})} \hspace*{0.5cm} \text{and} \hspace*{0.5cm} \bar{\mathbf{v}}_{\texttt{rb}}(\bg{\mu}) =
			\begin{bmatrix}
			\hspace*{-0.15cm}
			\begin{array}{c}
				\mathbf{v}_{\texttt{rb}}(\bg{\mu}) \\
				\mathbf{s}_{\texttt{rb}}(\bg{\mu})
			\end{array} 
			\hspace*{-0.15cm}
			\end{bmatrix} 
			\in \mathbb{R}^{L_{\bg{v}} + L_{\bg{s}}} \, .
		\end{equation*}
		In \cite{Bal14}, several numerical evidences suggest that a proper value for $L_{\bg{s}}$ should lay in between $L_p / 2$ and $L_p$. In our simulations, we set $L_{\bg{s}} = L_p = L_{\bg{v}}$, leading to a stable reduced (nonlinear) system in $L = 3 \, L_{\bg{v}}$ unknowns.
		
		Concerning the POD-NN procedure, for any $\bg{\mu} \in \mathcal{P}$, reduced-order discretizations $\mathbf{v}_{L_{\bg{v}}}^{\texttt{NN}}(\bg{\mu})$ and $\mathbf{p}_{L_p}^{\texttt{NN}}(\bg{\mu})$ of $\bg{v}(\bg{\mu})$ and $p(\bg{\mu})$, respectively, are sought in the form
		\begin{equation*}
			\mathbf{v}_{L_{\bg{v}}}^{\texttt{NN}}(\bg{\mu}) = \mathbb{V}_{\bg{v}} \, \mathbf{v}_{\texttt{rb}}^{\texttt{NN}}(\bg{\mu}) \hspace*{0.5cm} \text{and} \hspace*{0.5cm} \mathbf{p}_{L_p}^{\texttt{NN}}(\bg{\mu}) = \mathbb{V}_{p} \, \mathbf{p}_{\texttt{rb}}^{\texttt{NN}}(\bg{\mu}) \, .
		\end{equation*}
		Here, $\mathbf{v}_{\texttt{rb}}^{\texttt{NN}}(\bg{\mu})$ and $\mathbf{p}_{\texttt{rb}}^{\texttt{NN}}(\bg{\mu})$ denote the output vectors provided by two \emph{distinct} multi-layer perceptrons, respectively trained with the ensemble of input-output patterns
		\begin{equation*}
			\big\lbrace \bg{\mu}^{(i)}, \, \mathbb{V}_{\bg{v}}^T \, \mathbf{v}_h \big( \bg{\mu}^{(i)} \big) \big\rbrace_{1 \leq i \leq N_{tr}} \hspace*{0.5cm} \text{and} \hspace*{0.5cm} \big\lbrace \bg{\mu}^{(i)}, \, \mathbb{V}_{p}^T \, \mathbf{p}_h \big( \bg{\mu}^{(i)} \big) \big\rbrace_{1 \leq i \leq N_{tr}} \, .
		\end{equation*}
		At this regard, let us point out two important remarks.
		\begin{enumerate}[label=(\roman*)]
			\item The supremizer solutions are not involved (either directly or indirectly) in the POD-NN framework. Indeed, they ensure the overall stability of the POD-Galerkin procedure, but they do not power-up the accuracy of the method, and so they can be disregarded whenever stability is not an issue.
			\item The routine outlined in Section \ref{section:A non-intrusive RB method using neural networks}, aiming at finding an optimal network configuration, is applied \emph{separately} to the perceptrons designated to predict the velocity field and to the perceptrons required to approximate the pressure distribution. As a result, once the offline phase of the POD-NN method is completed, we may end up with two completely different networks, equipped with $H_1^{\bg{v}} = H_2^{\bg{v}}$ and $H_1^p = H_2^p$ computing units per hidden layer, respectively.  
		\end{enumerate}
		Figure \ref{fig:dc-error-vs-rank} reports the error committed by both RB procedures when approximating the velocity field (\emph{left}) and the pressure distribution (\emph{right}) by means of $L_{\bg{v}}$ velocity modes, $L_p$ pressure modes and, exclusively for the POD-Galerkin method, $L_{\bg{s}}$ supremizer modes, with $5 \leq L_{\bg{v}} = L_{\bg{s}} = L_p \leq 35$. The plots on the top refer to a Reynold's number of $200$, the ones on the bottom to a Reynold's number of $400$. Please note that for clarity of illustration, the symbols denoting the projection, POD-G and POD-NN errors have been decorated with a superscript (either $\bg{v}$ or $p$) recalling the state variable they refer to. 
		
		\iffalse
		\begin{figure}[t!]
			\center
			\vspace*{-0.5cm}
			\subfloat{\includegraphics[scale = 0.22, trim = {7.5cm 0 7.5cm 0.4cm}, clip]{dc_200_vel_basis_1}}
			\subfloat{\includegraphics[scale = 0.22, trim = {7.5cm 0 7.5cm 0.4cm}, clip]{dc_200_vel_basis_2}}
			\subfloat{\includegraphics[scale = 0.22, trim = {7.5cm 0 7.5cm 0.4cm}, clip]{dc_200_vel_basis_3}}
			\subfloat{\includegraphics[scale = 0.22, trim = {7.5cm 0 7.5cm 0.4cm}, clip]{dc_200_vel_basis_4}} \\
			\subfloat{\includegraphics[scale = 0.22, trim = {7.5cm 0 7.5cm 0.4cm}, clip]{dc_200_p_basis_1}}
			\subfloat{\includegraphics[scale = 0.22, trim = {7.5cm 0 7.5cm 0.4cm}, clip]{dc_200_p_basis_2}}
			\subfloat{\includegraphics[scale = 0.22, trim = {7.5cm 0 7.5cm 0.4cm}, clip]{dc_200_p_basis_3}}
			\subfloat{\includegraphics[scale = 0.22, trim = {7.5cm 0 7.5cm 0.4cm}, clip]{dc_200_p_basis_4}}
			
			\caption{First four POD modes for the velocity (\emph{top}) and the pressure (\emph{bottom}) for the parametrized lid-driven cavity problem with $Re = 200$.}
			\label{fig:dc-modal-functions}
		\end{figure}
		\fi
		
		\begin{figure}[t!]
			\center
			\includegraphics[scale = 0.4, trim = {2cm 9cm 1.5cm 3.5cm}, clip]{dc_200_vel_error_vs_rank}
			\hspace*{1cm}
			\includegraphics[scale = 0.4, trim = {2cm 9cm 1.5cm 3.5cm}, clip]{dc_200_p_error_vs_rank} \\
			\includegraphics[scale = 0.4, trim = {2cm 9cm 1.5cm 3.5cm}, clip]{dc_400_vel_error_vs_rank}
			\hspace*{1cm}
			\includegraphics[scale = 0.4, trim = {2cm 9cm 1.5cm 3.5cm}, clip]{dc_400_p_error_vs_rank}
			
			\caption{Velocity (\emph{left}) and pressure (\emph{right}) error analysis for the POD-G and POD-NN methods applied to the lid-driven cavity problem with $Re = 200$ (\emph{top}) and $Re = 400$ (\emph{bottom}).}
			\label{fig:dc-error-vs-rank}
		\end{figure}
									
		While the error yielded on the velocity field shows an almost perfect exponential decay with the number of POD modes included in the RB model, the POD-G method faces difficulties in providing a correct recovery of the pressure, already for $Re = 200$. Indeed, the corresponding error curve may not be monotone, and generally lays at least one order of magnitude above the projection error. On the contrary, the POD-NN method attains a satisfactory predictive accuracy. Particularly, the advantages of resorting to a neural network-based nonlinear regression coupled with a POD procedure are evident when the approximation is built upon a few basis functions, say $L_p < 20$. However, even for $L_p = 35$ (i.e., the dimension of the largest basis tested in our experiments), the POD-NN error is smaller than the POD-G one. Moreover, the proposed routine recommends to use fewer neurons to predict the POD coefficients for the pressure than for the velocity, thus resulting in a lighter perceptron. 
		
		This is confirmed also by Fig. \ref{fig:dc-nn-convergence}, which provides a sensitivity analysis of the predictive accuracy featured by the POD-NN method with respect the amount of neurons and training samples used. Observe that for the pressure (\emph{right}), employing more than $30$ neurons within each hidden layer is counter-productive, both for $Re = 200$ (\emph{top}) and $Re = 400$ (\emph{bottom}). This assertion agrees with the peculiar role played by the pressure in the parametrized lid-driven cavity problem, with similar patterns featured across the entire parameter domain. 
		
		On the contrary, we have seen that the velocity field presents more complex dynamics, highly varying with the domain configuration. As a result, an optimal approximation of the velocity is obtained for the maximum values of $H_i^{\bg{v}}$, $i = 1, \, 2$, and $N_{tr}$ tested, that is, $H_1^{\bg{v}} = H_2^{\bg{v}} = 35$ and $N_{tr} = 300$ for $Re = 200$, $H_1^{\bg{v}} = H_2^{\bg{v}} = 40$ and $N_{tr} = 300$ for $Re = 400$. Moreover, we may not be able to exactly attain the same precision enabled by the standard POD-Galerkin procedure, although the results are quite similar. However, this (slight) loss of accuracy is offset by a (great) reduction in the online run time: the POD-NN method takes around $2/100$ of seconds per query, against the average $40$ seconds required by the POD-G method; see Fig. \ref{fig:dc-time}. As for the test cases for the Poisson problem, this comes at the cost of a longer offline phase. In fact, the research and training of sufficiently accurate perceptrons lasted about $4.5$ hours here.
										
		\begin{figure}[t]
			\center
			\includegraphics[scale = 0.37, trim = {1cm 9cm 1.5cm 3.5cm}, clip]{dc_200_vel_nn_convergence}
			\hspace*{1cm}
			\includegraphics[scale = 0.37, trim = {1cm 9cm 1.5cm 3.5cm}, clip]{dc_200_p_nn_convergence} \\
			\includegraphics[scale = 0.37, trim = {1cm 9cm 1.5cm 3.5cm}, clip]{dc_400_vel_nn_convergence}
			\hspace*{1cm}
			\includegraphics[scale = 0.37, trim = {1cm 9cm 1.5cm 3.5cm}, clip]{dc_400_p_nn_convergence}
			
			\vspace*{-0.1cm}
			
			\caption{Convergence analysis with respect to the number of hidden neurons and training samples used within the POD-NN procedure to approximate the velocity field (\emph{left}) and the pressure distribution (\emph{right}) by means of $35$ modal functions. The Reynold's number is either $200$ (\emph{top}) or $400$ (\emph{bottom}).}
			\label{fig:dc-nn-convergence}
		\end{figure}
		
		\begin{figure}[H]
			\center
			\includegraphics[scale = 0.37, trim = {1cm 9cm 1.5cm 3.5cm}, clip]{dc_200_time}
			\hspace*{0.6cm}
			\includegraphics[scale = 0.37, trim = {1cm 9cm 1.5cm 3.5cm}, clip]{dc_400_time}
			
			\vspace*{-0.1cm}
			
			\caption{Online run times for the POD-G and the POD-NN method applied to the lid-driven cavity problem with $Re = 200$ (\emph{left}) and $Re = 400$ (\emph{right}). $N_{te} = 75$ test configurations are considered. For the POD-NN method, the reported times include the (sequential) evaluation of both neural networks for the velocity and pressure field.}
			\label{fig:dc-time}
			
			\center
			\hspace*{-0.7cm}
			\stackunder[1pt]{\includegraphics[scale = 0.22, trim = {7cm 0 7.5cm 0.4cm}, clip]{dc_400_fem_vx_1}}{\footnotesize{$\bg{\mu} = (1.12, \, 1.70, \, 1.08)$, FEM}}
			\hspace*{0.8cm}
			\stackunder[1pt]{\includegraphics[scale = 0.22, trim = {8.25cm 0 7.5cm 0.4cm}, clip]{dc_400_fem_vx_2}}{\footnotesize{$\bg{\mu} = (1.90, \, 1.50, \, 1.60)$, FEM}}
			\hspace*{0.5cm}
			\stackunder[1pt]{\includegraphics[scale = 0.22, trim = {4.5cm 0 3cm 0.4cm}, clip]{dc_400_fem_vx_4}}{\footnotesize{$\bg{\mu} = (1.78, \, 1.99, \, 2.29)$, FEM}} \\[0.4cm]
			
			\hspace*{-0.7cm}
			\stackunder[1pt]{\includegraphics[scale = 0.22, trim = {7cm 0 7.5cm 0.4cm}, clip]{dc_400_podnn_vx_1}}{\footnotesize{$\bg{\mu} = (1.12, \, 1.70, \, 1.08)$, POD-NN}}
			\hspace*{0.8cm}
			\stackunder[1pt]{\includegraphics[scale = 0.22, trim = {8.25cm 0 7.5cm 0.4cm}, clip]{dc_400_podnn_vx_2}}{\footnotesize{$\bg{\mu} = (1.90, \, 1.50, \, 1.60)$, POD-NN}}
			\hspace*{0.5cm}
			\stackunder[1pt]{\includegraphics[scale = 0.22, trim = {4.5cm 0 3cm 0.4cm}, clip]{dc_400_podnn_vx_4}}{\footnotesize{$\bg{\mu} = (1.78, \, 1.99, \, 2.29)$, POD-NN}}
			
			\caption{$\wt{x}$-velocity contour at three parameter values, as computed through the FE (\emph{top row}) and POD-NN (\emph{bottom row}) method. For each configuration, the Reynold's number is $400$.}
			\label{fig:dc-x-velocity}
		\end{figure}
				
		%Another numerical evidence of the predictive accuracy of the proposed POD-NN RB method is provided in Fig. \ref{fig:dc-pressure}, \ref{fig:dc-x-velocity} and \ref{fig:dc-y-velocity}, showing the contour plots for the pressure, $\wt{x}$-velocity and $\wt{y}$-velocity, respectively, computed through the FE (\emph{top row}) and the POD-NN (\emph{bottom row}) scheme. Three different configurations, corresponding to as many input vectors, are considered; the Reynold's number is fixed to $400$. We can appreciate the good agreement between the solutions given by the full-order and the reduced-order methods. In particular, for the pressure distribution we also report the solutions provided by the standard POD-G technique. Although the patterns are qualitatively identical to the ones given by the high-fidelity method, with a stagnation point at the upper-left corner and a high-pressure region at the upper-right corner, the colorbars reveal a discrepancy in the maximum and minimum values attained, generally greater than for the POD-NN procedure. This motivates the larger error yielded by the POD-G method.
		
		Another numerical evidence of the predictive accuracy of the proposed POD-NN RB method is provided in Fig. \ref{fig:dc-x-velocity}, showing the contour plots for the $\wt{x}$-velocity computed through the FE (\emph{top row}) and the POD-NN (\emph{bottom row}) scheme. Three different configurations, corresponding to as many input vectors, are considered; the Reynold's number is fixed to $400$. We can appreciate the good agreement between the solutions given by the full-order and the reduced-order methods. 
														
		Lastly, Fig. \ref{fig:dc-streamlines} compares the streamlines obtained through the direct method (\emph{top}) and the proposed reduced basis approach (\emph{bottom}) over the three configurations previously considered. Streamlines provide an interesting test bed, as minor variations in velocity contours may lead to substantial differences in the streamlines \cite{Chen17}. However, we observe a good agreement between the two methods. In particular, in the second example, the POD-NN method is still able to detect the two micro recirculation zones at the lower corners of the domain. Whereas, the method partially fails in properly describing the velocity field at the bottom-left corner in the first configuration and at the bottom-right corner in the last configuration. However, these are dead zones, so we can safely disregard these little imprecisions.
				
		\begin{figure}[t]
			\center
			
			\hspace*{-0.7cm}
			\stackunder[1pt]{\includegraphics[scale = 0.22, trim = {7cm 0 7.5cm 0.4cm}, clip]{dc_400_fem_stream_1}}{\footnotesize{$\bg{\mu} = (1.12, \, 1.70, \, 1.08)$, FEM}}
			\hspace*{0.8cm}
			\stackunder[1pt]{\includegraphics[scale = 0.22, trim = {8.25cm 0 7.5cm 0.4cm}, clip]{dc_400_fem_stream_2}}{\footnotesize{$\bg{\mu} = (1.90, \, 1.50, \, 1.60)$, FEM}}
			\hspace*{0.5cm}
			\stackunder[1pt]{\includegraphics[scale = 0.22, trim = {4.5cm 0 3cm 0.4cm}, clip]{dc_400_fem_stream_4}}{\footnotesize{$\bg{\mu} = (1.78, \, 1.99, \, 2.29)$, FEM}} \\
			
			\hspace*{-0.7cm}
			\stackunder[1pt]{\includegraphics[scale = 0.22, trim = {7cm 0 7.5cm 0.4cm}, clip]{dc_400_podnn_stream_1}}{\footnotesize{$\bg{\mu} = (1.12, \, 1.70, \, 1.08)$, POD-NN}}
			\hspace*{0.8cm}
			\stackunder[1pt]{\includegraphics[scale = 0.22, trim = {8.25cm 0 7.5cm 0.4cm}, clip]{dc_400_podnn_stream_2}}{\footnotesize{$\bg{\mu} = (1.90, \, 1.50, \, 1.60)$, POD-NN}}
			\hspace*{0.5cm}
			\stackunder[1pt]{\includegraphics[scale = 0.22, trim = {4.5cm 0 3cm 0.4cm}, clip]{dc_400_podnn_stream_4}}{\footnotesize{$\bg{\mu} = (1.78, \, 1.99, \, 2.29)$, POD-NN}}
			
			\caption{Streamlines at three parameter values, as computed through the FE (\emph{top row}) and POD-NN (\emph{bottom row}) method. For each configuration, the Reynold's number is $400$.}
			\label{fig:dc-streamlines}
		\end{figure}
	
	
	%
	% Section 7 : Conclusion
	%
	
	\section{Conclusion}
	\label{section:Conclusion}	
	
		In this work, we have developed a non-intrusive RB method (referred to as POD-NN) for parametrized time-independent differential problems. The method extracts a reduced basis from a collection of snapshots through a POD procedure and employs multi-layer perceptrons to approximate the coefficients of the reduced model. By exploiting the fundamental results by Cybenko (see Subsection \ref{section:Network topology}), we can limit ourselves to perceptrons endowed with two hidden layers. The identification of the optimal number of inner neurons and the minimum amount of training samples which prevents overfitting is performed at the offline stage through an automatic routine, relying upon the latin hypercube sampling and the Levenberg-Marquardt training algorithm. On the one hand, this guarantees a complete decoupling between the offline and the online phase, with the latter which entails a computational cost independent of the dimension of the full-order model. On the other hand, this unavoidably extends the offline stage with respect to a standard projection-based RB procedure, making the POD-NN method practically convenient only when the underlying PDE has to be solved for many parameter values (many-query context). At this regard, the method would surely benefit from a GPU-powered implementation, enabling a high parallelization of the code.
		
		The POD-NN method has been successfully tested on the nonlinear Poisson equation in one and two spatial dimensions, and on two-dimensional cavity viscous flows, modeled through the steady uncompressible Navier-Stokes equations. In particular, the proposed RB strategy enabled the same predictive accuracy provided by the POD-Galerkin method while reducing by two or even three order of magnitudes the CPU time required to process an online query. %Generally, the optimal network configuration yielded by the developed automatic routine comprised around $35$ neurons per hidden layer and required from $200$ to $300$ training patterns not to incur in overfitting.
		
		All the test cases considered in our numerical studies involved three parameters, affecting either physical or geometrical factors of the differential problem. The extension of the POD-NN method to time-dependent problems depending on many parameters is left as future work. In this respect, the secret hope behind a neural network-based interpolation is that the number of training samples, thus snapshots, required to achieve a given level of accuracy properly scales with the dimension of the parameter space. In fact, as observed in Subsection \ref{section:Two-dimensional Poisson equation}, this represents the major weak point of traditional interpolation techniques.
		
		Lastly, let us point out that although in this work we used POD to recover a reduced space, this choice is not binding, and a greedy approach may also be pursued. 
		
		%All the test cases considered in our numerical studies involved three parameters, affecting either physical or geometrical factors of the differential problem. In the latter case, we made use of the boundary displacement-dependent transfinite map (see Subsection \ref{section:The boundary displacement-dependent transfinite map}) to formulate (and solve) the underlying PDE over a reference, parameter-independent domain. The extension of the POD-NN method to time-dependent problems depending on many parameters is left as future work. In this respect, the secret hope behind a neural network-based interpolation is that the number of training samples, thus snapshots, required to achieve a given level of accuracy properly scales with the dimension of the parameter space. In fact, as observed in Section \ref{section:poisson2d-3} already for a three-dimensional parameter domain, this represents the major weak point of traditional interpolation techniques.
		
		%Lastly, let us point out that although in this work we used POD to recover a reduced space, this choice is not binding, and a greedy approach may also be pursued. 
		
	
	%
	% Bibliography
	%
	
	\clearpage
	
	\section*{References}
		
	\begin{thebibliography}{50}
		
		\bibitem{Ams10}
		Amsallem., D. (2010). \emph{Interpolation on manifolds of CFD-based fluid and finite element-based structural reduced-order models for on-line aeroelastic predictions}. Doctoral dissertation, Department of Aeronautics and Astronautics, Stanford University.
		
		\bibitem{Bal14}
		Ballarin, F., Manzoni, A., Quarteroni, A., Rozza, G. (2014). \emph{Supremizer stabilization of POD-Galerkin approximation of parametrized Navier-Stokes equations}. MATHICSE Technical Report, \'Ecole Polytechnique F\'ed\'erale de Lausanne.
	
		\bibitem{Bar04}
		Barrault, M., Maday, Y., Nguyen, N. C., Patera, A. T. (2004). \emph{An 'empirical interpolation' method: Application to efficient reduced-basis discretization of partial differential equations}. Comptes Rendus Mathematique, 339(9):667-672.
		
		\bibitem{BNR00}
		Barthelmann, V., Novak, E., Ritter, K. (2000). \emph{High-dimensional polynomial interpolation on sparse grids}. Advances in Computational Mathematics, 12(4):273-288.
		
		\bibitem{Ben04}
		Bends\o{}e, M. P., Sigmund, O. (2004). \emph{Topology optimization: Theory, methods and applications}. Heidelberg, DE: Springer Science \& Business Media. 
		
		\bibitem{Bro93}
		Brown, P. F., Pietra, V. J. D., Pietra, S. A. D., Mercer, R. L. (1993). \emph{The mathematics of statistical machine translation: Parameter estimation}. Computational linguistics, 19(2):263-311.
		
		\bibitem{Buf12}
		Buffa, A., Maday, Y., Patera, A. T., Prud'Homme, C., Turinici, G. (2012). \emph{A priori convergence of the greedy algorithm for the parametrized reduced basis method}. ESAIM: Mathematical Modelling and Numerical Analysis, 46:595-603.
		
		\bibitem{Bur06}
		Burkardt, J., Gunzburger, M., Lee, H. C. (2006). \emph{POD and CVT-based reduced-order modeling of Navier-Stokes flows}. Computer Methods in Applied Mechanics and Egninnering, 196:337-355.
		
		\bibitem{Cas15}
		Casenave, F., Ern, A., Lelivre, T. (2015). \emph{A nonintrusive reduced basis method applied to aeroacoustic simulations}. Advances in Computational Mathematics, 41:961-986.
		
		\bibitem{Cha10}
		Chaturantabut, S., Sorensen, D. C. (2010). \emph{Nonlinear model reduction via discrete empirical interpolation}. SIAM Journal on Scientific Computing, 32(5):2737-2764.
		
		%\bibitem{CR97}
		%Caloz, G., Rappaz, J. (1997). \emph{Numerical analysis and bifurcation problems}. Handbook of numerical analysis, 5(2):487-637.
		
		\bibitem{Chen17}
		Chen, W., Hesthaven, J. S., Junqiang, B., Yang, Z., Tihao, Y. (2017). \emph{A greedy non-intrusive reduced order model for fluid dynamics}. Submitted to American Institute of Aeronautics and Astronautics.
	
		\bibitem{Cyb88}
		Cybenko, G. (1988). \emph{Continuous valued neural networks with two hidden layers are sufficient}. Technical Report, Department of Computer Science, Tufts University.
		
		\bibitem{Cyb89}
		Cybenko, G. (1989). \emph{Approximation by superpositions of a sigmoidal function}. Mathematics of Control, Signals, and Systems, 2(4):303314.
		
		\bibitem{Deb78}
		De Boor, C. (1978). \emph{A practical guide to splines}. New York, NY: Springer-Verlag.
		
		%\bibitem{Deb87}
		%De Boor, C., H\"{o}llig, K., Sabin, M. (1987). \emph{High accuracy geometric Hermite interpolation}. Computer Aided Geometric Design, 4(4):269-278.
		
		\bibitem{Dep08}
		Deparis, S. (2008). \emph{Reduced basis error bound computation of parameter-dependent Navier-Stokes equations by the natural norm approach}. SIAM Journal of Numerical Analysis, 46(4):2039-2067.
		
		\bibitem{Dho14}
		Dhondt, G. (2014). \emph{CalculiX CrunchiX user's manual}. Available at \url{http://web.mit.edu/calculix_v2.7/CalculiX/ccx_2.7/doc/ccx/node1.html}.
		
		\bibitem{EY36}
		Eckart, C., Young, G. (1936). \emph{The approximation of one matrix by another of lower rank}. Psychometrika, 1:211-218.
		
		\bibitem{Eft08}
		Eftang, J. L. (2008). \emph{Reduced basis methods for partial differential equations}. Master thesis, Department of Mathematical Sciences, Norwegian University of Science and Technology.
		
		%\bibitem{ESW04}
		%Elman, H. C., Silvester, D. J., Wathen, A. (2004). \emph{Finite elements and fast iterative solvers with applications in incompressible fluid dynamics}. New York, NY: Oxford University Press.
				
		%\bibitem{Fah88}
		%Fahlman, S. E. (1988). \emph{An empirical study of learning speed in back-propagation networks}. Technical Report CMU-CS-88-162, CMU.
		
		%\bibitem{GMW81}
		%Gill, P. E., Murray, W., Wright, M. H. (1981). \emph{Practical optimization}. Academic Press.
		
		\bibitem{Hag94}
		Hagan, M. T., Menhaj, M. B. (1994). \emph{Training feedforward networks with the Marquardt algorithm}. IEEE Transactions on Neural Networks, 5(6):989-993.
		
		\bibitem{Hag14}
		Hagan, M. T., Demuth, H. B., Beale, M. H., De Jes\'us, O. (2014). \emph{Neural Network Design, 2nd Edition}. Retrieved from \url{http://hagan.okstate.edu/NNDesign.pdf}.
		
		%\bibitem{Haa13}
		%Hassdonk, B. (2013). \emph{Model reduction for parametric and nonlinear problems via reduced basis and kernel methods}. CEES Computational Geoscience Seminar, Stanford University.
		
		\bibitem{Hay05}
		Haykin, S. (2004). \emph{Neural Networks: A comprehensive foundation}. Upper Saddle River, NJ: Prentice Hall.
		
		%\bibitem{Heb49}
		%Hebb, D. O. (1949). \emph{The organization of behaviour: A neuropsychological theory}. New York, NY: John Wiley \& Sons. 
		
		\bibitem{Lia02}
		Liang, Y. C., Lee, H. P., Lim, S. P., Lin, W. Z., Lee, K. H., Wu, C. G. (2002) \emph{Proper orthogonal decomposition and its applications - Part I: Theory}. Journal of Sound and Vibration, 252(3):527-544.
		
		%\bibitem{OBS}
		%Hassibi, B., Stork, D. G. (1993). \emph{Second order derivatives for network pruning: Optimal Brain Surgeon}. Advances in neural information processing systems, 164-171.
		
		\bibitem{HSR16}
		Hesthaven, J. S., Stamn, B., Rozza, G. (2016). \emph{Certified reduced basis methods for parametrized partial differential equations}. New York, NY: Springer.
		
		\bibitem{HSZ14}
		Hesthaven, J. S., Stamn, B., Zhang, S. (2014). \emph{Efficienty greedy algorithms for high-dimensional parameter spaces with applications to empirical interpolation and reduced basis methods}. ESAIM: Mathematical Modelling and Numerical Analysis, 48(1):259-283.
		
		%\bibitem{Hop82}
		%Hopfield, J. J. (1982). \emph{Neural networks and physical systems with emergent collective computational abilities}. Proceedings of the National Acadamedy Science, 79:2554-2558.
		
		\bibitem{Imam08}
		Imam, R. L. (2008). \emph{Latin hypercube sampling}. Encyclopedia of Quantitative Risk Analysis and Assessment.
		
		\bibitem{JIR14}
		Jaggli, C., Iapichino, L., Rozza, G. (2014). \emph{An improvement on geometrical parametrizations by transfinite maps}. Comptes Rendus de l'Acad\'emie des Sciences Paris, Series I, 352:263-268. 
				
		%\bibitem{KLM96}
		%Kaelbling, L. P., Littman, M. L., Moore, A. W. (1996). \emph{Reinforcement Learning: A Survey}. Journal of Artificial Intelligence Reserch, 4:237-285.
		
		\bibitem{Koh95}
		Kohavi, R. (1995). \emph{A study of cross-validation and bootstrap for accuracy estimation and model selection}. Proceedings of the $40^{th}$ International Joint Conference on Artificial Intelligence, 2(12):1137-1143.
		
		%\bibitem{Koh98}
		%Kohonen, T. (1998). \emph{The self-organizing map}. Neurocomputing, 21(1-3):1-6.
		
		\bibitem{Kri07}
		Kriesel, D. (2007). \emph{A Brief Introduction to Neural Networks}. Retrieved from \url{http://www.dkriesel.com/en/science/neural_networks}.
		
		\bibitem{LeM10}
		Le Ma\^{i}tre, O., Knio, O. M. (2010). \emph{Spectral methods for uncertainty quantification with applications to computational fluid dynamics}. Berlin, DE: Springer Science \& Business Media.
		
		\bibitem{LM67}
		Lee, E. B., Markus, L. (1967). \emph{Foundations of optimal control theory}. New York, NY: John Wiley \& Sons.
		
		\bibitem{Mad06}
		Maday, Y. (2006) \emph{Reduced basis method for the rapid and reliable solution of partial differential equations}. Proceedings of the International Congress of Mathematicians, Madrid, Spain, 1255-1269.
		
		\bibitem{Mar63}
		Marquardt, D. W. (1963). \emph{An algorithm for least-squares estimation of nonlinear parameters}. Journal of the Society for Industrial and Applied Mathematics, 11(2):431-441.
		
		\bibitem{Mat16}
		The MathWorks, Inc. (2016). \emph{Machine learning challenges: Choosing the best model and avoiding overfitting}. Retrieved from \url{https://it.mathworks.com/campaigns/products/offer/common-machine-learning-challenges.html}.
		
		\bibitem{MM10}
		Mitchell, W., McClain, M. A. (2010). \emph{A collection of 2D elliptic problems for testing adaptive algorithms}. NISTIR 7668.
		
		\bibitem{MN16}
		Manzoni, A., Negri, F. (2016). \emph{Automatic reduction of PDEs defined on domains with variable shape}. MATHICSE technical report, \'Ecole Polytechnique F\'ed\'erale de Lausanne.
		
		%\bibitem{Mol93}
		%M\o{}ller, M. D. (1993). \emph{A scaled conjugate gradient algorithm for fast supervised learning}. Neural Networks, 6:525-533.
						
		%\bibitem{MR86}
		%McClelland, J. L., Rumelhart, D. E. (1986). \emph{Parallel Distributed Processing: Explorations in the Microstructure of Cognition}. Cambridge, UK: MIT Press.
		
		\bibitem{Nie15}
		Nielsen, M. A. (2015). \emph{Neural Networks and Deep Learning}. Determination Press.
				
		\bibitem{NMA15}
		Negri, F., Manzoni, A., Amsallem, D. (2015). \emph{Efficient model reduction of parametrized systems by matrix discrete empirical interpolation}. Journal of Computational Physics, 303:431-454.
		
		\bibitem{Per02}
		Persson, P. O. (2002). \emph{Implementation of finite-element based Navier-Stokes solver}. Massachussets Institue of Technology.
		
		\bibitem{Pru02}
		Prud'homme, C., Rovas, D. V., Veroy, K., Machiels, L., Maday, Y., Patera, A. T., Turinici, G. (2002). \emph{Reliable real-time solution of parametrized partial differential equations: Reduced-basis output bound methods}. Journal of Fluids Engineering, 124(1):70-80.
		
		\bibitem{Qua10}
		Quarteroni, A. (2010). \emph{Numerical models for differential problems} (Vol. 2). New York, NY: Springer Science \& Business Media.
		
		\bibitem{QMN15}
		Quarteroni, A., Manzoni, A., Negri, F. (2015). \emph{Reduced basis methods for partial differential equations: An introduction} (Vol. 92). New York, NY: Springer, 2015.
		
		\bibitem{Ran99}
		Rannacher, R. (1999). \emph{Finite element methods for the incompressible Navier-Stokes equations}. Lecture notes, Institute of Applied Mathematics, University of Heidelberg.
		
		%\bibitem{RB93}
		%Riedmiller, M., Braun, H. (1993). \emph{A direct adaptive method for faster backpropagation learning: The rprop algorithm}. Neural Networks, IEEE International Conference on, 596-591.
		
		\bibitem{Ros58}
		Rosenblatt, F. (1958). \emph{The perceptron: A probabilistic model for information storage and organization in the brain}. Psychological Review, 65:386-408.
		
		%\bibitem{Rud64}
		%Rudin, W. (1964). \emph{Principles of mathematical analysis} (Vol. 3). New York, NY: McGraw-Hill.
		
		\bibitem{Sch07}
		Schmidt, E. (1907). \emph{Zur theorie der linearen und nichtlinearen integralgleichungen. I. Teil: Entwicklung willk\"urlicher funktionen nach systemen vorgeschriebener}. Mathematische Annalen, 63:433-476.
		
		\bibitem{SD13}
		Stergiou, C., Siganos, D. (2013). \emph{Neural Networks}. Retrieved from \url{https://www.doc.ic.ac.uk/~nd/surprise_96/journal/vol4/cs11/report.html#Introduction to neural networks}.
		
		\bibitem{Vol08}
		Volkwein, S. (2008). \emph{Model reduction using proper orthogonal decomposition}. Lecture notes, Department of Mathematics, University of Konstanz.
		
		\bibitem{WH60}
		Widrow, B., Hoff, M. E. (1960). \emph{Adaptive switching circuits}. Proceedings WESCON, 96-104.
		
	\end{thebibliography}
	
\end{document}
