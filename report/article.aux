\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\emailauthor{jan.hesthaven@epfl.ch}{J.S.~Hesthaven}
\emailauthor{stefano.ubbiali@epfl.ch}{S.~Ubbiali}
\citation{Eft08,HSR16,JIR14}
\citation{Bro93}
\citation{Ben04}
\citation{LM67}
\citation{LeM10}
\citation{Dep08}
\citation{QMN15}
\citation{Ams10}
\citation{Chen17}
\citation{Mad06}
\citation{Bal14,Chen17}
\citation{Lia02,Vol08}
\citation{HSZ14}
\citation{HSR16}
\citation{Bal14}
\providecommand\tcolorbox@label[2]{}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}}
\newlabel{section:Introduction}{{1}{1}{Introduction}{section.1}{}}
\citation{Buf12}
\citation{QMN15}
\citation{Bar04}
\citation{Cha10,NMA15}
\citation{Chen17}
\citation{Cas15}
\citation{Ams10,BNR00}
\citation{Hay05,Kri07}
\citation{Nie15}
\citation{SD13}
\citation{Kri07}
\@writefile{toc}{\contentsline {section}{\numberline {2}Artifical neural networks}{2}{section.2}}
\newlabel{section:Artificial neural networks}{{2}{2}{Artifical neural networks}{section.2}{}}
\newlabel{def:neural-network}{{2.1}{2}{Neural network}{definition.2.1}{}}
\citation{Kri07}
\citation{Hay05}
\citation{Kri07}
\citation{Hay05}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Neuronal model}{3}{subsection.2.1}}
\newlabel{section:Neuronal model}{{2.1}{3}{Neuronal model}{subsection.2.1}{}}
\newlabel{eq:propagation-function}{{2.1}{3}{Neuronal model}{subsection.2.1}{}}
\newlabel{eq:weighted-sum}{{2.1}{3}{Neuronal model}{equation.2.1}{}}
\newlabel{eq:activation-function}{{2.1}{3}{Neuronal model}{equation.2.1}{}}
\newlabel{eq:net-input}{{2.1}{3}{Neuronal model}{equation.2.1}{}}
\citation{Hay05}
\citation{Ros58}
\citation{Kri07}
\citation{Cyb88,Cyb89}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces Visualization of the generic $j$-th neuron of an artificial neural network, including (right) or not (left) a bias neuron. On the left, the neuron accumulates the weighted inputs ${\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\nulldelimiterspace \z@ \left \delimiter "42663A9 \vcenter to\@ne \big@size {}\right .$}\box \z@ }w_{s_1,j} \nobreakspace  {} y_{s_1}, \tmspace  +\thinmuskip {.1667em} \ldots  \tmspace  +\thinmuskip {.1667em} , w_{s_m,j} \nobreakspace  {} y_{s_m} {\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\nulldelimiterspace \z@ \left \delimiter "52673AA \vcenter to\@ne \big@size {}\right .$}\box \z@ }$ respectively coming from the sending neurons $s_1, \tmspace  +\thinmuskip {.1667em} \ldots  \tmspace  +\thinmuskip {.1667em} , s_m$; on the right, the neuron accumulates the weighted inputs ${\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\nulldelimiterspace \z@ \left \delimiter "42663A9 \vcenter to\@ne \big@size {}\right .$}\box \z@ }w_{s_1,j} \nobreakspace  {} y_{s_1}, \tmspace  +\thinmuskip {.1667em} \ldots  \tmspace  +\thinmuskip {.1667em} , w_{s_m,j} \nobreakspace  {} y_{s_m}, \tmspace  +\thinmuskip {.1667em} -\theta _j {\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\nulldelimiterspace \z@ \left \delimiter "52673AA \vcenter to\@ne \big@size {}\right .$}\box \z@ }$ respectively coming from the sending neurons $s_1, \tmspace  +\thinmuskip {.1667em} \ldots  \tmspace  +\thinmuskip {.1667em} , s_m, \tmspace  +\thinmuskip {.1667em} b$, with $b$ the bias neuron. In both situations, the neuron then fires $y_j$, sent to the target neurons ${\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\nulldelimiterspace \z@ \left \delimiter "42663A9 \vcenter to\@ne \big@size {}\right .$}\box \z@ }r_1, \tmspace  +\thinmuskip {.1667em} \ldots  \tmspace  +\thinmuskip {.1667em} , r_n {\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\nulldelimiterspace \z@ \left \delimiter "52673AA \vcenter to\@ne \big@size {}\right .$}\box \z@ }$ through the synapsis ${\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\nulldelimiterspace \z@ \left \delimiter "42663A9 \vcenter to\@ne \big@size {}\right .$}\box \z@ }w_{j,r_1}, \tmspace  +\thinmuskip {.1667em} \ldots  \tmspace  +\thinmuskip {.1667em} , w_{j,r_n} {\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\nulldelimiterspace \z@ \left \delimiter "52673AA \vcenter to\@ne \big@size {}\right .$}\box \z@ }$. The neuron threshold is reported in brackets within its body.\relax }}{4}{figure.caption.1}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:neural-model}{{2.1}{4}{Visualization of the generic $j$-th neuron of an artificial neural network, including (right) or not (left) a bias neuron. On the left, the neuron accumulates the weighted inputs $\big \lbrace w_{s_1,j} ~ y_{s_1}, \, \ldots \, , w_{s_m,j} ~ y_{s_m} \big \rbrace $ respectively coming from the sending neurons $s_1, \, \ldots \, , s_m$; on the right, the neuron accumulates the weighted inputs $\big \lbrace w_{s_1,j} ~ y_{s_1}, \, \ldots \, , w_{s_m,j} ~ y_{s_m}, \, -\theta _j \big \rbrace $ respectively coming from the sending neurons $s_1, \, \ldots \, , s_m, \, b$, with $b$ the bias neuron. In both situations, the neuron then fires $y_j$, sent to the target neurons $\big \lbrace r_1, \, \ldots \, , r_n \big \rbrace $ through the synapsis $\big \lbrace w_{j,r_1}, \, \ldots \, , w_{j,r_n} \big \rbrace $. The neuron threshold is reported in brackets within its body.\relax }{figure.caption.1}{}}
\newlabel{eq:hyperbolic-tangent}{{2.1}{4}{Neuronal model}{equation.2.1}{}}
\newlabel{eq:output-function}{{2.1}{4}{Neuronal model}{equation.2.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Network topology: the feedforward neural network}{4}{subsection.2.2}}
\newlabel{section:Network topology}{{2.2}{4}{Network topology: the feedforward neural network}{subsection.2.2}{}}
\citation{Cyb89}
\citation{Cyb88}
\citation{Hag14}
\citation{Kri07}
\citation{Hag14}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces A three-layer feedforward neural network, with $3$ input neurons, two hidden layers each one consisting of $6$ neurons, and $4$ output neurons. Within each connection, information flows from left to right.\relax }}{5}{figure.caption.2}}
\newlabel{fig:neural-network}{{2.2}{5}{A three-layer feedforward neural network, with $3$ input neurons, two hidden layers each one consisting of $6$ neurons, and $4$ output neurons. Within each connection, information flows from left to right.\relax }{figure.caption.2}{}}
\newlabel{cybenko-first-rule}{{{{(i)}}}{5}{Network topology: the feedforward neural network}{Item.1}{}}
\newlabel{cybenko-second-rule}{{{{(ii)}}}{5}{Network topology: the feedforward neural network}{Item.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Training a multi-layer feedforward neural network}{5}{subsection.2.3}}
\newlabel{section:Training a multi-layer feedforward neural network}{{2.3}{5}{Training a multi-layer feedforward neural network}{subsection.2.3}{}}
\citation{Mar63}
\citation{Hag94}
\newlabel{eq:weight-update}{{2.3}{6}{Training a multi-layer feedforward neural network}{subsection.2.3}{}}
\newlabel{eq:performance-function}{{2.3}{6}{Training a multi-layer feedforward neural network}{subsection.2.3}{}}
\newlabel{eq:accumulated-mse}{{2.2}{6}{Training a multi-layer feedforward neural network}{equation.2.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.1}The Levenberg-Marquardt algorithm}{6}{subsubsection.2.3.1}}
\newlabel{section:Levenberg-Marquardt algorithm}{{2.3.1}{6}{The Levenberg-Marquardt algorithm}{subsubsection.2.3.1}{}}
\newlabel{eq:newton}{{2.3}{6}{The Levenberg-Marquardt algorithm}{equation.2.3}{}}
\citation{Hag94,Mar63}
\citation{Hag94}
\citation{Mar63}
\newlabel{eq:jacobian}{{2.3.1}{7}{The Levenberg-Marquardt algorithm}{equation.2.3}{}}
\newlabel{eq:gradient}{{2.4}{7}{The Levenberg-Marquardt algorithm}{equation.2.4}{}}
\newlabel{eq:hessian}{{2.5}{7}{The Levenberg-Marquardt algorithm}{equation.2.5}{}}
\newlabel{eq:newton-quadratic-function}{{2.6}{7}{The Levenberg-Marquardt algorithm}{equation.2.6}{}}
\newlabel{eq:levenberg-marquardt}{{2.7}{7}{The Levenberg-Marquardt algorithm}{equation.2.7}{}}
\newlabel{eq:jacobian-entry-equation}{{2.3.1}{7}{The Levenberg-Marquardt algorithm}{equation.2.7}{}}
\newlabel{eq:levenberg-marquardt-delta}{{2.8}{7}{The Levenberg-Marquardt algorithm}{equation.2.8}{}}
\citation{Kri07,WH60}
\citation{Mar63}
\citation{Hag94}
\newlabel{eq:jacobian-first-case}{{{{(i)}}}{8}{The Levenberg-Marquardt algorithm}{Item.3}{}}
\newlabel{eq:jacobian-second-case}{{{{(ii)}}}{8}{The Levenberg-Marquardt algorithm}{Item.4}{}}
\newlabel{eq:jacobian-third-case}{{{{(iii)}}}{8}{The Levenberg-Marquardt algorithm}{Item.5}{}}
\newlabel{eq:levenberg-marquardt-inner-neuron}{{2.9a}{8}{The Levenberg-Marquardt algorithm}{equation.2.9}{}}
\newlabel{eq:levenberg-marquardt-output-neuron}{{2.9b}{8}{The Levenberg-Marquardt algorithm}{equation.2.9}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Parametrized partial differential equations}{8}{section.3}}
\newlabel{section:Parametrized partial differential equations}{{3}{8}{Parametrized partial differential equations}{section.3}{}}
\citation{Qua10}
\citation{MN16}
\citation{JIR14}
\newlabel{eq:pde-differential-form}{{3.1}{9}{Parametrized partial differential equations}{equation.3.1}{}}
\newlabel{eq:pde-variational-form}{{3}{9}{Parametrized partial differential equations}{equation.3.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}From physical to reference domain}{9}{subsection.3.1}}
\newlabel{section:From physical to reference domain}{{3.1}{9}{From physical to reference domain}{subsection.3.1}{}}
\newlabel{eq:parametrized-map}{{3.1}{9}{From physical to reference domain}{subsection.3.1}{}}
\newlabel{eq:pde-differential-reference}{{3.2}{9}{From physical to reference domain}{equation.3.2}{}}
\newlabel{eq:pde-weak-reference}{{3.3}{9}{From physical to reference domain}{equation.3.3}{}}
\newlabel{eq:parametrized-map-discrete}{{3.1}{9}{From physical to reference domain}{equation.3.3}{}}
\citation{MM10}
\citation{Qua10}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Problems of interest}{10}{subsection.3.2}}
\newlabel{section:Problems of interest}{{3.2}{10}{Problems of interest}{subsection.3.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.1}Nonlinear Poisson equation}{10}{subsubsection.3.2.1}}
\newlabel{section:Nonlinear Poisson equation}{{3.2.1}{10}{Nonlinear Poisson equation}{subsubsection.3.2.1}{}}
\newlabel{eq:poisson-differential}{{3.4}{10}{Nonlinear Poisson equation}{equation.3.4}{}}
\newlabel{eq:poisson-differential-first-equation}{{3.4a}{10}{Nonlinear Poisson equation}{equation.3.4alph1}{}}
\newlabel{eq:poisson-weak-derivation}{{3.5}{10}{Nonlinear Poisson equation}{equation.3.5}{}}
\newlabel{eq:poisson-weak-forms}{{3.6}{10}{Nonlinear Poisson equation}{equation.3.6}{}}
\newlabel{eq:poisson-weak}{{3.7}{10}{Nonlinear Poisson equation}{equation.3.7}{}}
\citation{Ran99}
\newlabel{eq:poisson-weak-reference}{{3.2.1}{11}{Nonlinear Poisson equation}{equation.3.7}{}}
\newlabel{eq:poisson-weak-forms-reference}{{3.8}{11}{Nonlinear Poisson equation}{equation.3.8}{}}
\newlabel{eq:poisson-weak-forms-reference-first}{{3.8a}{11}{Nonlinear Poisson equation}{equation.3.8alph1}{}}
\newlabel{eq:poisson-weak-forms-reference-second}{{3.8b}{11}{Nonlinear Poisson equation}{equation.3.8alph2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.2}Steady uncompressible Navier-Stokes equations}{11}{subsubsection.3.2.2}}
\newlabel{section:Steady uncompressible Navier-Stokes equations}{{3.2.2}{11}{Steady uncompressible Navier-Stokes equations}{subsubsection.3.2.2}{}}
\newlabel{eq:ns-differential}{{3.9}{11}{Steady uncompressible Navier-Stokes equations}{equation.3.9}{}}
\newlabel{eq:mass-conservation}{{3.9a}{11}{Steady uncompressible Navier-Stokes equations}{equation.3.9alph1}{}}
\newlabel{eq:momentum-conservation}{{3.9b}{11}{Steady uncompressible Navier-Stokes equations}{equation.3.9alph2}{}}
\newlabel{eq:ns-weak-reference}{{3.2.2}{12}{Steady uncompressible Navier-Stokes equations}{AMS.4}{}}
\newlabel{eq:ns-weak-forms}{{3.2.2}{12}{Steady uncompressible Navier-Stokes equations}{AMS.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Discrete full-order model}{12}{section.4}}
\newlabel{section:Discrete full-order model}{{4}{12}{Discrete full-order model}{section.4}{}}
\newlabel{eq:galerkin}{{4.1}{12}{Discrete full-order model}{equation.4.1}{}}
\newlabel{eq:newton-linearized-problem}{{4}{12}{Discrete full-order model}{equation.4.1}{}}
\newlabel{eq:galerkin-solution}{{4}{12}{Discrete full-order model}{equation.4.1}{}}
\citation{HSR16,QMN15}
\citation{HSR16}
\citation{Chen17}
\citation{Lia02,Vol08}
\newlabel{eq:galerkin-algebraic}{{4}{13}{Discrete full-order model}{equation.4.1}{}}
\newlabel{eq:galerkin-nonlinear-system}{{4.2}{13}{Discrete full-order model}{equation.4.2}{}}
\newlabel{eq:galerkin-nonlinear-system-equation}{{4.3}{13}{Discrete full-order model}{equation.4.3}{}}
\newlabel{eq:galerkin-linear-system}{{4.4}{13}{Discrete full-order model}{equation.4.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Projection-based reduced basis method}{13}{section.5}}
\newlabel{section:Projection-based reduced basis method}{{5}{13}{Projection-based reduced basis method}{section.5}{}}
\newlabel{eq:rb-solution}{{5}{14}{Projection-based reduced basis method}{section.5}{}}
\newlabel{eq:pde-rb}{{5.1}{14}{Projection-based reduced basis method}{equation.5.1}{}}
\newlabel{eq:pde-rb-newton}{{5}{14}{Projection-based reduced basis method}{equation.5.1}{}}
\newlabel{eq:rb-fe-coefficients}{{5.2}{14}{Projection-based reduced basis method}{equation.5.2}{}}
\newlabel{eq:rb-algebraic-formulation-1}{{5.3}{14}{Projection-based reduced basis method}{equation.5.3}{}}
\newlabel{eq:rb-nonlinear-system}{{5.4}{15}{Projection-based reduced basis method}{equation.5.4}{}}
\newlabel{eq:rb-nonlinear-system-jacobian}{{5.5}{15}{Projection-based reduced basis method}{equation.5.5}{}}
\newlabel{eq:rb-nonlinear-system-newton}{{5.6}{15}{Projection-based reduced basis method}{equation.5.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Proper Orthogonal Decomposition}{15}{subsection.5.1}}
\newlabel{section:Proper Orthogonal Decomposition}{{5.1}{15}{Proper Orthogonal Decomposition}{subsection.5.1}{}}
\newlabel{eq:svd}{{5.7}{15}{Proper Orthogonal Decomposition}{equation.5.7}{}}
\newlabel{eq:svd-relationships}{{5.8}{15}{Proper Orthogonal Decomposition}{equation.5.8}{}}
\citation{Vol08}
\citation{EY36,Sch07}
\citation{Pru02}
\citation{QMN15}
\newlabel{eq:svd-compact}{{5.1}{16}{Proper Orthogonal Decomposition}{equation.5.8}{}}
\newlabel{eq:svd-compact-matrices}{{5.1}{16}{Proper Orthogonal Decomposition}{equation.5.8}{}}
\newlabel{eq:basis-error}{{5.9}{16}{Proper Orthogonal Decomposition}{equation.5.9}{}}
\newlabel{eq:pod-fe-basis-functions}{{5.10}{16}{Proper Orthogonal Decomposition}{equation.5.10}{}}
\citation{Bar04}
\citation{Cha10}
\citation{NMA15}
\citation{Cas15}
\citation{Bar04}
\@writefile{loa}{\contentsline {algorithm}{\numberline {5.1}{\ignorespaces The offline and online stages for the POD-Galerkin (POD-G) RB method.\relax }}{17}{algorithm.5.1}}
\newlabel{alg:pod-galerkin}{{5.1}{17}{The offline and online stages for the POD-Galerkin (POD-G) RB method.\relax }{algorithm.5.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Implementation: details and issues}{17}{subsection.5.2}}
\newlabel{section:Implementation: details and issues}{{5.2}{17}{Implementation: details and issues}{subsection.5.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6}A non-intrusive reduced basis method using artificial neural networks}{17}{section.6}}
\newlabel{section:A non-intrusive RB method using neural networks}{{6}{17}{A non-intrusive reduced basis method using artificial neural networks}{section.6}{}}
\newlabel{eq:discrete-scalar-product}{{6.1}{17}{A non-intrusive reduced basis method using artificial neural networks}{equation.6.1}{}}
\citation{Ams10}
\citation{Chen17}
\newlabel{eq:high-fidelity-projected}{{6}{18}{A non-intrusive reduced basis method using artificial neural networks}{equation.6.1}{}}
\newlabel{eq:map-to-approximate}{{6.2}{18}{A non-intrusive reduced basis method using artificial neural networks}{equation.6.2}{}}
\citation{Imam08}
\citation{Mat16}
\citation{Koh95}
\@writefile{loa}{\contentsline {algorithm}{\numberline {6.1}{\ignorespaces The offline and online stages for the POD-NN RB method.\relax }}{19}{algorithm.6.1}}
\newlabel{alg:pod-nn}{{6.1}{19}{The offline and online stages for the POD-NN RB method.\relax }{algorithm.6.1}{}}
\newlabel{eq:pod-nn-mse}{{6}{19}{A non-intrusive reduced basis method using artificial neural networks}{Item.9}{}}
\newlabel{eq:pod-nn-solution}{{6}{19}{A non-intrusive reduced basis method using artificial neural networks}{Item.9}{}}
\citation{Koh95}
\citation{Kri07,Mat16}
\@writefile{toc}{\contentsline {section}{\numberline {7}Numerical results}{20}{section.7}}
\newlabel{section:Numerical results}{{7}{20}{Numerical results}{section.7}{}}
\newlabel{eq:podg-error}{{7.1}{20}{Numerical results}{equation.7.1}{}}
\newlabel{eq:podnn-error}{{7.2}{20}{Numerical results}{equation.7.2}{}}
\newlabel{eq:projection-error}{{7.3}{20}{Numerical results}{equation.7.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.1}One-dimensional Poisson equation}{21}{subsection.7.1}}
\newlabel{section:One-dimensional Poisson equation}{{7.1}{21}{One-dimensional Poisson equation}{subsection.7.1}{}}
\newlabel{eq:poisson1d}{{7.4}{21}{One-dimensional Poisson equation}{equation.7.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.1}{\ignorespaces Convergence analysis for the POD-G and POD-NN methods applied to problem \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces \ref  {eq:poisson1d}\unskip \@@italiccorr )}} (\emph  {left}) and comparison between the FE and the POD-NN solutions for three parameter values (\emph  {right}). These latter results have been obtained via a neural network with $H_1 = H_2 = 35$ units per hidden layer, and employing $L = 10$ POD modes.\relax }}{21}{figure.caption.5}}
\newlabel{fig:poisson1d-fig2}{{7.1}{21}{Convergence analysis for the POD-G and POD-NN methods applied to problem \eqref {eq:poisson1d} (\emph {left}) and comparison between the FE and the POD-NN solutions for three parameter values (\emph {right}). These latter results have been obtained via a neural network with $H_1 = H_2 = 35$ units per hidden layer, and employing $L = 10$ POD modes.\relax }{figure.caption.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.2}{\ignorespaces Error analysis for the POD-NN RB method applied to problem \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces \ref  {eq:poisson1d}\unskip \@@italiccorr )}} for several numbers of training samples. The solid sections represent the steps followed by the automatic routine desscribed in Section \ref  {section:A non-intrusive RB method using neural networks}.\relax }}{22}{figure.caption.6}}
\newlabel{fig:poisson1d-fig3}{{7.2}{22}{Error analysis for the POD-NN RB method applied to problem \eqref {eq:poisson1d} for several numbers of training samples. The solid sections represent the steps followed by the automatic routine desscribed in Section \ref {section:A non-intrusive RB method using neural networks}.\relax }{figure.caption.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.2}Two-dimensional Poisson equation}{22}{subsection.7.2}}
\newlabel{section:Two-dimensional Poisson equation}{{7.2}{22}{Two-dimensional Poisson equation}{subsection.7.2}{}}
\newlabel{eq:poisson2d}{{7.5}{22}{Two-dimensional Poisson equation}{equation.7.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.3}{\ignorespaces The physical (\emph  {left}) and reference (\emph  {right}) domains for the Poisson problem \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces \ref  {eq:poisson2d}\unskip \@@italiccorr )}}.\relax }}{22}{figure.caption.7}}
\newlabel{fig:poisson2d-fig1}{{7.3}{22}{The physical (\emph {left}) and reference (\emph {right}) domains for the Poisson problem \eqref {eq:poisson2d}.\relax }{figure.caption.7}{}}
\citation{Deb78}
\@writefile{lof}{\contentsline {figure}{\numberline {7.4}{\ignorespaces FE solution (\emph  {top left}) to the Poisson problem \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces \ref  {eq:poisson2d}\unskip \@@italiccorr )}} with $\bm  {\mu } = (0.349, \tmspace  +\thinmuskip {.1667em} -0.413, \tmspace  +\thinmuskip {.1667em} 4.257)$, and pointwise errors yielded by either its projection onto $V_{\texttt  {rb}}$ (\emph  {top right}), or the POD-G method (\emph  {bottom left}), or the POD-NN method (\emph  {bottom right}). The results have been obtained by employing $L = 30$ POD modes.\relax }}{23}{figure.caption.8}}
\newlabel{fig:poisson2d-fig2}{{7.4}{23}{FE solution (\emph {top left}) to the Poisson problem \eqref {eq:poisson2d} with $\bg {\mu } = (0.349, \, -0.413, \, 4.257)$, and pointwise errors yielded by either its projection onto $V_{\texttt {rb}}$ (\emph {top right}), or the POD-G method (\emph {bottom left}), or the POD-NN method (\emph {bottom right}). The results have been obtained by employing $L = 30$ POD modes.\relax }{figure.caption.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.5}{\ignorespaces Error analysis (\emph  {left}) and online CPU time (\emph  {right}) for the POD-G and the POD-NN methods applied to problem \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces \ref  {eq:poisson2d}\unskip \@@italiccorr )}}. $N_{te} = 50$ randomly picked parameter values are considered. The reduced basis have been generated via POD, relying on $N = 100$ snapshots. The second plot refers to RB models including $L = 30$ modal functions; within the POD-NN framework, a neural network emboding $35$ neurons per inner layer has been used.\relax }}{24}{figure.caption.9}}
\newlabel{fig:poisson2d-fig3}{{7.5}{24}{Error analysis (\emph {left}) and online CPU time (\emph {right}) for the POD-G and the POD-NN methods applied to problem \eqref {eq:poisson2d}. $N_{te} = 50$ randomly picked parameter values are considered. The reduced basis have been generated via POD, relying on $N = 100$ snapshots. The second plot refers to RB models including $L = 30$ modal functions; within the POD-NN framework, a neural network emboding $35$ neurons per inner layer has been used.\relax }{figure.caption.9}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.6}{\ignorespaces Convergence analysis with respect to the number of hidden neurons (\emph  {left}) and modal functions (\emph  {right}) used within the POD-NN framework applied to problem \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces \ref  {eq:poisson2d}\unskip \@@italiccorr )}}. The results provided in the first plot have been obtained using $L = 30$ modes; the solid tracts refer to the steps performed by the automatic routine carried out to find an optimal network configuration.\relax }}{24}{figure.caption.9}}
\newlabel{fig:poisson2d-fig4}{{7.6}{24}{Convergence analysis with respect to the number of hidden neurons (\emph {left}) and modal functions (\emph {right}) used within the POD-NN framework applied to problem \eqref {eq:poisson2d}. The results provided in the first plot have been obtained using $L = 30$ modes; the solid tracts refer to the steps performed by the automatic routine carried out to find an optimal network configuration.\relax }{figure.caption.9}{}}
\newlabel{eq:podcs-error}{{7.2}{24}{Two-dimensional Poisson equation}{figure.caption.9}{}}
\citation{Ams10}
\citation{Per02}
\citation{Dho14}
\citation{Ran99}
\citation{Per02}
\citation{Dho14}
\@writefile{lof}{\contentsline {figure}{\numberline {7.7}{\ignorespaces Average relative errors (\emph  {left}) and online run times (\emph  {right}) on $\Xi _{te}$ for the POD-CS and the POD-NN methods applied to problem \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces \ref  {eq:poisson2d}\unskip \@@italiccorr )}}. For the latter, the results refer to an MLP equipped with $H_1 = H_2 = 35$ neurons per hidden layer.\relax }}{25}{figure.caption.10}}
\newlabel{fig:poisson2d-fig5}{{7.7}{25}{Average relative errors (\emph {left}) and online run times (\emph {right}) on $\Xi _{te}$ for the POD-CS and the POD-NN methods applied to problem \eqref {eq:poisson2d}. For the latter, the results refer to an MLP equipped with $H_1 = H_2 = 35$ neurons per hidden layer.\relax }{figure.caption.10}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.3}The lid-driven cavity problem}{25}{subsection.7.3}}
\newlabel{section:The lid-driven cavity problem}{{7.3}{25}{The lid-driven cavity problem}{subsection.7.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.8}{\ignorespaces Computational domain (\emph  {left}), enforced velocity at the boundaries (\emph  {center}) and computational mesh $\Omega _h$ (\emph  {right}) for the lid-driven cavity problem for the uncompressible Navier-Stokes equations.\relax }}{26}{figure.caption.11}}
\newlabel{fig:dc-domain}{{7.8}{26}{Computational domain (\emph {left}), enforced velocity at the boundaries (\emph {center}) and computational mesh $\Omega _h$ (\emph {right}) for the lid-driven cavity problem for the uncompressible Navier-Stokes equations.\relax }{figure.caption.11}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.9}{\ignorespaces Velocity streamlines (\emph  {top}) and pressure distribution (\emph  {bottom}) for the lid-driven cavity problem, computed through the FE method. Three different parameter values are considered; for all configurations, the Reynold's number is $400$.\relax }}{26}{figure.caption.11}}
\newlabel{fig:dc-solutions-different-domains}{{7.9}{26}{Velocity streamlines (\emph {top}) and pressure distribution (\emph {bottom}) for the lid-driven cavity problem, computed through the FE method. Three different parameter values are considered; for all configurations, the Reynold's number is $400$.\relax }{figure.caption.11}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {$\bm {\mu } = {\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\nulldelimiterspace \z@ \left (\vcenter to\@ne \big@size {}\right .$}\box \z@ } 1, \tmspace +\thinmuskip {.1667em} \nicefrac {2}{\sqrt {3}}, \tmspace +\thinmuskip {.1667em} \nicefrac {2 \pi }{3} {\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\nulldelimiterspace \z@ \left )\vcenter to\@ne \big@size {}\right .$}\box \z@ }$}}}{26}{subfigure.9.1}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {$\bm {\mu } = {\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\nulldelimiterspace \z@ \left (\vcenter to\@ne \big@size {}\right .$}\box \z@ } 1, \tmspace +\thinmuskip {.1667em} 1, \tmspace +\thinmuskip {.1667em} \nicefrac {\pi }{2} {\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\nulldelimiterspace \z@ \left )\vcenter to\@ne \big@size {}\right .$}\box \z@ }$}}}{26}{subfigure.9.2}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {$\bm {\mu } = {\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\nulldelimiterspace \z@ \left (\vcenter to\@ne \big@size {}\right .$}\box \z@ } 1, \tmspace +\thinmuskip {.1667em} \nicefrac {2}{\sqrt {3}}, \tmspace +\thinmuskip {.1667em} \nicefrac {\pi }{3} {\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\nulldelimiterspace \z@ \left )\vcenter to\@ne \big@size {}\right .$}\box \z@ }$}}}{26}{subfigure.9.3}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(d)}{\ignorespaces {$\bm {\mu } = {\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\nulldelimiterspace \z@ \left (\vcenter to\@ne \big@size {}\right .$}\box \z@ } 1, \tmspace +\thinmuskip {.1667em} \nicefrac {2}{\sqrt {3}}, \tmspace +\thinmuskip {.1667em} \nicefrac {2 \pi }{3} {\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\nulldelimiterspace \z@ \left )\vcenter to\@ne \big@size {}\right .$}\box \z@ }$}}}{26}{subfigure.9.4}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(e)}{\ignorespaces {$\bm {\mu } = {\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\nulldelimiterspace \z@ \left (\vcenter to\@ne \big@size {}\right .$}\box \z@ } 1, \tmspace +\thinmuskip {.1667em} 1, \tmspace +\thinmuskip {.1667em} \nicefrac {\pi }{2} {\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\nulldelimiterspace \z@ \left )\vcenter to\@ne \big@size {}\right .$}\box \z@ }$}}}{26}{subfigure.9.5}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(f)}{\ignorespaces {$\bm {\mu } = {\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\nulldelimiterspace \z@ \left (\vcenter to\@ne \big@size {}\right .$}\box \z@ } 1, \tmspace +\thinmuskip {.1667em} \nicefrac {2}{\sqrt {3}}, \tmspace +\thinmuskip {.1667em} \nicefrac {\pi }{3} {\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\nulldelimiterspace \z@ \left )\vcenter to\@ne \big@size {}\right .$}\box \z@ }$}}}{26}{subfigure.9.6}}
\citation{Bal14,Chen17,QMN15}
\citation{Bur06}
\citation{Bal14}
\citation{Bal14}
\@writefile{lof}{\contentsline {figure}{\numberline {7.10}{\ignorespaces Velocity (\emph  {left}) and pressure (\emph  {right}) error analysis for the POD-G and POD-NN methods applied to the lid-driven cavity problem with $Re = 200$ (\emph  {top}) and $Re = 400$ (\emph  {bottom}).\relax }}{28}{figure.caption.12}}
\newlabel{fig:dc-error-vs-rank}{{7.10}{28}{Velocity (\emph {left}) and pressure (\emph {right}) error analysis for the POD-G and POD-NN methods applied to the lid-driven cavity problem with $Re = 200$ (\emph {top}) and $Re = 400$ (\emph {bottom}).\relax }{figure.caption.12}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.11}{\ignorespaces Convergence analysis with respect to the number of hidden neurons and training samples used within the POD-NN procedure to approximate the velocity field (\emph  {left}) and the pressure distribution (\emph  {right}) by means of $35$ modal functions. The Reynold's number is either $200$ (\emph  {top}) or $400$ (\emph  {bottom}).\relax }}{29}{figure.caption.13}}
\newlabel{fig:dc-nn-convergence}{{7.11}{29}{Convergence analysis with respect to the number of hidden neurons and training samples used within the POD-NN procedure to approximate the velocity field (\emph {left}) and the pressure distribution (\emph {right}) by means of $35$ modal functions. The Reynold's number is either $200$ (\emph {top}) or $400$ (\emph {bottom}).\relax }{figure.caption.13}{}}
\citation{Chen17}
\@writefile{lof}{\contentsline {figure}{\numberline {7.12}{\ignorespaces Online run times for the POD-G and the POD-NN method applied to the lid-driven cavity problem with $Re = 200$ (\emph  {left}) and $Re = 400$ (\emph  {right}). $N_{te} = 75$ test configurations are considered. For the POD-NN method, the reported times include the (sequential) evaluation of both neural networks for the velocity and pressure field.\relax }}{30}{figure.caption.14}}
\newlabel{fig:dc-time}{{7.12}{30}{Online run times for the POD-G and the POD-NN method applied to the lid-driven cavity problem with $Re = 200$ (\emph {left}) and $Re = 400$ (\emph {right}). $N_{te} = 75$ test configurations are considered. For the POD-NN method, the reported times include the (sequential) evaluation of both neural networks for the velocity and pressure field.\relax }{figure.caption.14}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.13}{\ignorespaces $\mathaccentV {widetilde}365{x}$-velocity contour at three parameter values, as computed through the FE (\emph  {top row}) and POD-NN (\emph  {bottom row}) method. For each configuration, the Reynold's number is $400$.\relax }}{30}{figure.caption.14}}
\newlabel{fig:dc-x-velocity}{{7.13}{30}{$\wt {x}$-velocity contour at three parameter values, as computed through the FE (\emph {top row}) and POD-NN (\emph {bottom row}) method. For each configuration, the Reynold's number is $400$.\relax }{figure.caption.14}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.14}{\ignorespaces Streamlines at three parameter values, as computed through the FE (\emph  {top row}) and POD-NN (\emph  {bottom row}) method. For each configuration, the Reynold's number is $400$.\relax }}{31}{figure.caption.15}}
\newlabel{fig:dc-streamlines}{{7.14}{31}{Streamlines at three parameter values, as computed through the FE (\emph {top row}) and POD-NN (\emph {bottom row}) method. For each configuration, the Reynold's number is $400$.\relax }{figure.caption.15}{}}
\@writefile{toc}{\contentsline {section}{\numberline {8}Conclusion}{31}{section.8}}
\newlabel{section:Conclusion}{{8}{31}{Conclusion}{section.8}{}}
\bibcite{Ams10}{{1}{}{{}}{{}}}
\bibcite{Bal14}{{2}{}{{}}{{}}}
\bibcite{Bar04}{{3}{}{{}}{{}}}
\bibcite{BNR00}{{4}{}{{}}{{}}}
\bibcite{Ben04}{{5}{}{{}}{{}}}
\bibcite{Bro93}{{6}{}{{}}{{}}}
\bibcite{Buf12}{{7}{}{{}}{{}}}
\bibcite{Bur06}{{8}{}{{}}{{}}}
\bibcite{Cas15}{{9}{}{{}}{{}}}
\bibcite{Cha10}{{10}{}{{}}{{}}}
\bibcite{Chen17}{{11}{}{{}}{{}}}
\bibcite{Cyb88}{{12}{}{{}}{{}}}
\bibcite{Cyb89}{{13}{}{{}}{{}}}
\bibcite{Deb78}{{14}{}{{}}{{}}}
\bibcite{Dep08}{{15}{}{{}}{{}}}
\bibcite{Dho14}{{16}{}{{}}{{}}}
\bibcite{EY36}{{17}{}{{}}{{}}}
\bibcite{Eft08}{{18}{}{{}}{{}}}
\bibcite{Hag94}{{19}{}{{}}{{}}}
\bibcite{Hag14}{{20}{}{{}}{{}}}
\bibcite{Hay05}{{21}{}{{}}{{}}}
\bibcite{Lia02}{{22}{}{{}}{{}}}
\bibcite{HSR16}{{23}{}{{}}{{}}}
\bibcite{HSZ14}{{24}{}{{}}{{}}}
\bibcite{Imam08}{{25}{}{{}}{{}}}
\bibcite{JIR14}{{26}{}{{}}{{}}}
\bibcite{Koh95}{{27}{}{{}}{{}}}
\bibcite{Kri07}{{28}{}{{}}{{}}}
\bibcite{LeM10}{{29}{}{{}}{{}}}
\bibcite{LM67}{{30}{}{{}}{{}}}
\bibcite{Mad06}{{31}{}{{}}{{}}}
\bibcite{Mar63}{{32}{}{{}}{{}}}
\bibcite{Mat16}{{33}{}{{}}{{}}}
\bibcite{MM10}{{34}{}{{}}{{}}}
\bibcite{MN16}{{35}{}{{}}{{}}}
\bibcite{Nie15}{{36}{}{{}}{{}}}
\bibcite{NMA15}{{37}{}{{}}{{}}}
\bibcite{Per02}{{38}{}{{}}{{}}}
\bibcite{Pru02}{{39}{}{{}}{{}}}
\bibcite{Qua10}{{40}{}{{}}{{}}}
\bibcite{QMN15}{{41}{}{{}}{{}}}
\bibcite{Ran99}{{42}{}{{}}{{}}}
\bibcite{Ros58}{{43}{}{{}}{{}}}
\bibcite{Sch07}{{44}{}{{}}{{}}}
\bibcite{SD13}{{45}{}{{}}{{}}}
\bibcite{Vol08}{{46}{}{{}}{{}}}
\bibcite{WH60}{{47}{}{{}}{{}}}
\providecommand\NAT@force@numbers{}\NAT@force@numbers
