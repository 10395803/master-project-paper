\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\providecommand\tcolorbox@label[2]{}
\@writefile{toc}{\contentsline {chapter}{Abstract}{iii}{figure.caption.1}}
\@writefile{toc}{\contentsline {chapter}{Sommario}{v}{figure.caption.1}}
\@writefile{toc}{\contentsline {chapter}{List of Figures}{viii}{chapter*.2}}
\citation{Kri07}
\@writefile{toc}{\contentsline {chapter}{List of Algorithms}{xii}{chapter*.3}}
\citation{Eft08}
\citation{HSR16}
\citation{JIR14}
\citation{Bro93}
\citation{Ben04}
\citation{LM67}
\citation{LeM10}
\citation{Dep08}
\citation{QMN15}
\citation{Ams10}
\citation{Chen17}
\citation{Mad06}
\citation{Bal14}
\citation{Chen17}
\citation{Vol08}
\citation{HSZ14}
\citation{HSR16}
\citation{Bal14}
\citation{Buf12}
\@writefile{toc}{\contentsline {chapter}{Introduction}{1}{chapter*.5}}
\citation{QMN15}
\citation{Bar04}
\citation{Cha10}
\citation{NMA15}
\citation{Chen17}
\citation{Cas15}
\citation{Ams10}
\citation{BNR00}
\citation{Hay05}
\citation{Kri07}
\citation{Mat16}
\citation{Nie15}
\citation{Hag14}
\citation{Hay05}
\citation{Kri07}
\citation{Kri07}
\citation{Kri07}
\citation{Hay05}
\citation{Kri07}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Biological and artificial neural networks}{5}{chapter.1}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chapter:Introduction to neural networks}{{1}{5}{Biological and artificial neural networks}{chapter.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Biological motivation}{5}{section.1.1}}
\newlabel{section:Biological motivation}{{1.1}{5}{Biological motivation}{section.1.1}{}}
\citation{SD13}
\@writefile{lof}{\contentsline {figure}{\numberline {1.1}{\ignorespaces Simplified representation of a biological neuron, with the components discussed in the text; retrieved from \cite  {Kri07}.\relax }}{6}{figure.caption.7}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:neuron}{{1.1}{6}{Simplified representation of a biological neuron, with the components discussed in the text; retrieved from \cite {Kri07}.\relax }{figure.caption.7}{}}
\citation{Hag14}
\citation{Kri07}
\citation{Kri07}
\citation{Nie15}
\citation{SD13}
\citation{Kri07}
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Artificial neural networks}{7}{section.1.2}}
\newlabel{section:Artificial neural networks}{{1.2}{7}{Artificial neural networks}{section.1.2}{}}
\newlabel{def:neural-network}{{1.1}{7}{Neural network}{definition.1.1}{}}
\citation{Kri07}
\citation{Kri07}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.1}Neuronal model}{8}{subsection.1.2.1}}
\newlabel{section:Neuronal model}{{1.2.1}{8}{Neuronal model}{subsection.1.2.1}{}}
\newlabel{eq:propagation-function}{{1.2.1}{8}{Neuronal model}{subsection.1.2.1}{}}
\newlabel{eq:weighted-sum}{{1.1}{8}{Neuronal model}{equation.1.1}{}}
\newlabel{eq:activation-function}{{1.2.1}{8}{Neuronal model}{equation.1.1}{}}
\citation{Hay05}
\citation{Hay05}
\@writefile{lof}{\contentsline {figure}{\numberline {1.2}{\ignorespaces Visualization of the generic $j$-th neuron of an artificial neural network. The neuron accumulates the weighted inputs ${\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\nulldelimiterspace \z@ \left \delimiter "42663A9 \vcenter to\@ne \big@size {}\right .$}\box \z@ }w_{s_1,j} \nobreakspace  {} y_{s_1}, \tmspace  +\thinmuskip {.1667em} \ldots  \tmspace  +\thinmuskip {.1667em} , w_{s_m,j} \nobreakspace  {} y_{s_m} {\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\nulldelimiterspace \z@ \left \delimiter "52673AA \vcenter to\@ne \big@size {}\right .$}\box \z@ }$ coming from the sending neurons $s_1, \tmspace  +\thinmuskip {.1667em} \ldots  \tmspace  +\thinmuskip {.1667em} , s_m$, and fires $y_j$, sent to the target neurons ${\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\nulldelimiterspace \z@ \left \delimiter "42663A9 \vcenter to\@ne \big@size {}\right .$}\box \z@ }r_1, \tmspace  +\thinmuskip {.1667em} \ldots  \tmspace  +\thinmuskip {.1667em} , r_n {\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\nulldelimiterspace \z@ \left \delimiter "52673AA \vcenter to\@ne \big@size {}\right .$}\box \z@ }$ through the synapsis ${\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\nulldelimiterspace \z@ \left \delimiter "42663A9 \vcenter to\@ne \big@size {}\right .$}\box \z@ }w_{j,r_1}, \tmspace  +\thinmuskip {.1667em} \ldots  \tmspace  +\thinmuskip {.1667em} , w_{j,r_n} {\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\nulldelimiterspace \z@ \left \delimiter "52673AA \vcenter to\@ne \big@size {}\right .$}\box \z@ }$. The neuron threshold $\theta _j$ is reported within its body.\relax }}{9}{figure.caption.8}}
\newlabel{fig:neural-model}{{1.2}{9}{Visualization of the generic $j$-th neuron of an artificial neural network. The neuron accumulates the weighted inputs $\big \lbrace w_{s_1,j} ~ y_{s_1}, \, \ldots \, , w_{s_m,j} ~ y_{s_m} \big \rbrace $ coming from the sending neurons $s_1, \, \ldots \, , s_m$, and fires $y_j$, sent to the target neurons $\big \lbrace r_1, \, \ldots \, , r_n \big \rbrace $ through the synapsis $\big \lbrace w_{j,r_1}, \, \ldots \, , w_{j,r_n} \big \rbrace $. The neuron threshold $\theta _j$ is reported within its body.\relax }{figure.caption.8}{}}
\newlabel{eq:net-input}{{1.2}{9}{Neuronal model}{equation.1.2}{}}
\newlabel{eq:heaviside}{{1.3}{9}{Neuronal model}{equation.1.3}{}}
\newlabel{eq:logistic}{{1.4}{9}{Neuronal model}{equation.1.4}{}}
\citation{Hay05}
\@writefile{lof}{\contentsline {figure}{\numberline {1.3}{\ignorespaces Visualization of the generic $j$-th neuron of an artificial neural network. The neuron accumulates the weighted inputs ${\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\nulldelimiterspace \z@ \left \delimiter "42663A9 \vcenter to\@ne \big@size {}\right .$}\box \z@ }w_{s_1,j} \nobreakspace  {} y_{s_1}, \tmspace  +\thinmuskip {.1667em} \ldots  \tmspace  +\thinmuskip {.1667em} , w_{s_m,j} \nobreakspace  {} y_{s_m}, \tmspace  +\thinmuskip {.1667em} -\theta _j {\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\nulldelimiterspace \z@ \left \delimiter "52673AA \vcenter to\@ne \big@size {}\right .$}\box \z@ }$ coming from the sending neurons $s_1, \tmspace  +\thinmuskip {.1667em} \ldots  \tmspace  +\thinmuskip {.1667em} , s_m, \tmspace  +\thinmuskip {.1667em} b$, respectively, with $b$ the bias neuron. The neuron output $y_j$ is then conveyed towards the target neurons ${\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\nulldelimiterspace \z@ \left \delimiter "42663A9 \vcenter to\@ne \big@size {}\right .$}\box \z@ }r_1, \tmspace  +\thinmuskip {.1667em} \ldots  \tmspace  +\thinmuskip {.1667em} , r_n {\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\nulldelimiterspace \z@ \left \delimiter "52673AA \vcenter to\@ne \big@size {}\right .$}\box \z@ }$ through the synapsis ${\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\nulldelimiterspace \z@ \left \delimiter "42663A9 \vcenter to\@ne \big@size {}\right .$}\box \z@ }w_{j,r_1}, \tmspace  +\thinmuskip {.1667em} \ldots  \tmspace  +\thinmuskip {.1667em} , w_{j,r_n} {\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\nulldelimiterspace \z@ \left \delimiter "52673AA \vcenter to\@ne \big@size {}\right .$}\box \z@ }$. Observe that, conversely to the model offered in Fig. \ref  {fig:neural-model}, the neuron threshold is now set to $0$.\relax }}{10}{figure.caption.9}}
\newlabel{fig:neural-model-bias}{{1.3}{10}{Visualization of the generic $j$-th neuron of an artificial neural network. The neuron accumulates the weighted inputs $\big \lbrace w_{s_1,j} ~ y_{s_1}, \, \ldots \, , w_{s_m,j} ~ y_{s_m}, \, -\theta _j \big \rbrace $ coming from the sending neurons $s_1, \, \ldots \, , s_m, \, b$, respectively, with $b$ the bias neuron. The neuron output $y_j$ is then conveyed towards the target neurons $\big \lbrace r_1, \, \ldots \, , r_n \big \rbrace $ through the synapsis $\big \lbrace w_{j,r_1}, \, \ldots \, , w_{j,r_n} \big \rbrace $. Observe that, conversely to the model offered in Fig. \ref {fig:neural-model}, the neuron threshold is now set to $0$.\relax }{figure.caption.9}{}}
\newlabel{eq:hyperbolic-tangent}{{1.5}{10}{Neuronal model}{equation.1.5}{}}
\newlabel{eq:output-function}{{1.2.1}{10}{Neuronal model}{equation.1.5}{}}
\citation{Ros58}
\citation{Kri07}
\@writefile{lof}{\contentsline {figure}{\numberline {1.4}{\ignorespaces Left: logistic function \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces \ref  {eq:logistic}\unskip \@@italiccorr )}} for three values of the parameter $T$; note that as $T$ decreases, the logistic function resembles the Heaviside function. Right: hyperbolic tangent \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces \ref  {eq:hyperbolic-tangent}\unskip \@@italiccorr )}}.\relax }}{11}{figure.caption.10}}
\newlabel{fig:activation-functions}{{1.4}{11}{Left: logistic function \eqref {eq:logistic} for three values of the parameter $T$; note that as $T$ decreases, the logistic function resembles the Heaviside function. Right: hyperbolic tangent \eqref {eq:hyperbolic-tangent}.\relax }{figure.caption.10}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.2}Network topologies: the feedforward neural network}{11}{subsection.1.2.2}}
\newlabel{section:Network topologies}{{1.2.2}{11}{Network topologies: the feedforward neural network}{subsection.1.2.2}{}}
\citation{Cyb88}
\citation{Cyb89}
\citation{Cyb89}
\citation{Cyb88}
\citation{Hop82}
\citation{Koh98}
\newlabel{cybenko-first-rule}{{{{(i)}}}{12}{Network topologies: the feedforward neural network}{Item.1}{}}
\newlabel{cybenko-second-rule}{{{{(ii)}}}{12}{Network topologies: the feedforward neural network}{Item.2}{}}
\citation{Kri07}
\@writefile{lof}{\contentsline {figure}{\numberline {1.5}{\ignorespaces A three-layer feedforward neural network, with $3$ input neurons, two hidden layers each one consisting of $6$ neurons, and $4$ output neurons. Within each connection, information flows from left to right.\relax }}{13}{figure.caption.11}}
\newlabel{fig:neural-network}{{1.5}{13}{A three-layer feedforward neural network, with $3$ input neurons, two hidden layers each one consisting of $6$ neurons, and $4$ output neurons. Within each connection, information flows from left to right.\relax }{figure.caption.11}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.3}Training a multi-layer feedforward neural network}{13}{subsection.1.2.3}}
\newlabel{section:Training a multi-layer feedforward neural network}{{1.2.3}{13}{Training a multi-layer feedforward neural network}{subsection.1.2.3}{}}
\citation{Kri07}
\citation{KLM96}
\citation{Heb49}
\citation{Hay05}
\citation{Hay05}
\newlabel{a}{{{{(a)}}}{14}{Training a multi-layer feedforward neural network}{Item.3}{}}
\newlabel{b}{{{{(b)}}}{14}{Training a multi-layer feedforward neural network}{Item.4}{}}
\newlabel{first-rule}{{{{(i)}}}{14}{Training a multi-layer feedforward neural network}{Item.5}{}}
\citation{Kri07}
\newlabel{second-rule}{{{{(ii)}}}{15}{Training a multi-layer feedforward neural network}{Item.6}{}}
\newlabel{first-rule}{{1.2.3}{15}{Training a multi-layer feedforward neural network}{Item.6}{}}
\newlabel{eq:hebbian-rule}{{1.6}{15}{Training a multi-layer feedforward neural network}{equation.1.6}{}}
\newlabel{eq:weight-update}{{1.2.3}{15}{Training a multi-layer feedforward neural network}{equation.1.6}{}}
\newlabel{eq:generalized-hebbian-rule}{{1.7}{15}{Training a multi-layer feedforward neural network}{equation.1.7}{}}
\newlabel{eq:performance-function}{{1.8}{15}{Training a multi-layer feedforward neural network}{equation.1.8}{}}
\citation{Hay05}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1.1}{\ignorespaces Online supervised learning algorithm; the procedure ends when all training patterns yield an error below a defined threshold.\relax }}{16}{algorithm.1.1}}
\newlabel{alg:online-learning}{{1.1}{16}{Online supervised learning algorithm; the procedure ends when all training patterns yield an error below a defined threshold.\relax }{algorithm.1.1}{}}
\newlabel{eq:accumulated-error}{{1.9}{16}{Training a multi-layer feedforward neural network}{equation.1.9}{}}
\newlabel{eq:mse}{{1.10}{16}{Training a multi-layer feedforward neural network}{equation.1.10}{}}
\newlabel{eq:accumulated-mse}{{1.11}{16}{Training a multi-layer feedforward neural network}{equation.1.11}{}}
\citation{MR86}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1.2}{\ignorespaces Offline supervised learning algorithm; the procedure to compute the accumulated error is also provided.\relax }}{17}{algorithm.1.2}}
\newlabel{alg:offline-learning}{{1.2}{17}{Offline supervised learning algorithm; the procedure to compute the accumulated error is also provided.\relax }{algorithm.1.2}{}}
\newlabel{eq:antigradient}{{1.12}{17}{Training a multi-layer feedforward neural network}{equation.1.12}{}}
\@writefile{toc}{\contentsline {subsubsection}{Backpropagation of error}{17}{section*.12}}
\newlabel{section:Backpropagation of error}{{1.2.3}{17}{Backpropagation of error}{section*.12}{}}
\citation{Kri07}
\newlabel{eq:bp-first-equation}{{1.13}{18}{Backpropagation of error}{equation.1.13}{}}
\newlabel{eq:bp-third-equation}{{1.14}{18}{Backpropagation of error}{equation.1.14}{}}
\newlabel{eq:delta}{{1.15}{18}{Backpropagation of error}{equation.1.15}{}}
\newlabel{eq:bp-weight-update}{{1.16}{18}{Backpropagation of error}{equation.1.16}{}}
\newlabel{eq:bp-fourth-equation}{{1.17}{18}{Backpropagation of error}{equation.1.17}{}}
\citation{Kri07}
\citation{WH60}
\citation{Kri07}
\newlabel{eq:bp-fifth-equation}{{1.18}{19}{Backpropagation of error}{equation.1.18}{}}
\newlabel{eq:bp-sixth-equation}{{1.19}{19}{Backpropagation of error}{equation.1.19}{}}
\newlabel{eq:bp-inner-neuron}{{1.20a}{19}{Backpropagation of error}{equation.1.20}{}}
\newlabel{eq:bp-output-neuron}{{1.20b}{19}{Backpropagation of error}{equation.1.20}{}}
\citation{Kri07}
\citation{RB93}
\citation{Fah88}
\citation{Mar63}
\citation{Hag94}
\@writefile{toc}{\contentsline {subsubsection}{Levenberg-Marquardt algorithm}{20}{section*.13}}
\newlabel{section:Levenberg-Marquardt algorithm}{{1.2.3}{20}{Levenberg-Marquardt algorithm}{section*.13}{}}
\newlabel{eq:newton}{{1.21}{20}{Levenberg-Marquardt algorithm}{equation.1.21}{}}
\newlabel{eq:accumulated-mse-bis}{{1.2.3}{20}{Levenberg-Marquardt algorithm}{equation.1.21}{}}
\citation{Hag94}
\citation{Mar63}
\citation{Mar63}
\newlabel{eq:jacobian}{{1.22}{21}{Levenberg-Marquardt algorithm}{equation.1.22}{}}
\newlabel{eq:gradient}{{1.23}{21}{Levenberg-Marquardt algorithm}{equation.1.23}{}}
\newlabel{eq:hessian}{{1.24}{21}{Levenberg-Marquardt algorithm}{equation.1.24}{}}
\newlabel{eq:newton-quadratic-function}{{1.25}{21}{Levenberg-Marquardt algorithm}{equation.1.25}{}}
\newlabel{eq:levenberg-marquardt}{{1.26}{21}{Levenberg-Marquardt algorithm}{equation.1.26}{}}
\citation{Mar63}
\citation{Hag94}
\newlabel{eq:jacobian-entry-equation}{{1.27}{22}{Levenberg-Marquardt algorithm}{equation.1.27}{}}
\newlabel{eq:levenberg-marquardt-delta}{{1.28}{22}{Levenberg-Marquardt algorithm}{equation.1.28}{}}
\newlabel{eq:jacobian-first-case}{{{{(i)}}}{22}{Levenberg-Marquardt algorithm}{Item.7}{}}
\newlabel{eq:jacobian-second-case}{{{{(ii)}}}{22}{Levenberg-Marquardt algorithm}{Item.8}{}}
\newlabel{eq:jacobian-third-case}{{{{(iii)}}}{22}{Levenberg-Marquardt algorithm}{Item.9}{}}
\newlabel{eq:levenberg-marquardt-inner-neuron}{{1.29a}{23}{Levenberg-Marquardt algorithm}{equation.1.29}{}}
\newlabel{eq:levenberg-marquardt-output-neuron}{{1.29b}{23}{Levenberg-Marquardt algorithm}{equation.1.29}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1.3}{\ignorespaces An iteration of the Levenberg-Marquardt training algorithm.\relax }}{23}{algorithm.1.3}}
\newlabel{alg:levenberg-marquardt}{{1.3}{23}{An iteration of the Levenberg-Marquardt training algorithm.\relax }{algorithm.1.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.4}Practical considerations on the design of artificial neural networks}{23}{subsection.1.2.4}}
\newlabel{section:Practical considerations on the design of artificial neural networks}{{1.2.4}{23}{Practical considerations on the design of artificial neural networks}{subsection.1.2.4}{}}
\citation{Kri07}
\citation{Mat16}
\citation{Koh95}
\citation{Mat16}
\citation{OBS}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1.4}{\ignorespaces Complete training algorithm adopted in our numerical tests.\relax }}{26}{algorithm.1.4}}
\newlabel{alg:offline-learning-complete}{{1.4}{26}{Complete training algorithm adopted in our numerical tests.\relax }{algorithm.1.4}{}}
\citation{MN16}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Reduced basis methods for nonlinear partial differential equations}{27}{chapter.2}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chapter:Reduced basis methods for nonlinear partial differential equations}{{2}{27}{Reduced basis methods for nonlinear partial differential equations}{chapter.2}{}}
\newlabel{eq:geometric-map}{{2}{27}{Reduced basis methods for nonlinear partial differential equations}{chapter.2}{}}
\citation{HSR16}
\citation{JIR14}
\citation{QMN15}
\newlabel{eq:map-continuous}{{2}{28}{Reduced basis methods for nonlinear partial differential equations}{chapter.2}{}}
\newlabel{eq:nonlinear-system-full}{{2.1}{28}{Reduced basis methods for nonlinear partial differential equations}{equation.2.1}{}}
\newlabel{eq:reduced-solution-algebraic}{{2}{28}{Reduced basis methods for nonlinear partial differential equations}{equation.2.1}{}}
\newlabel{eq:reduced-solution}{{2}{28}{Reduced basis methods for nonlinear partial differential equations}{equation.2.1}{}}
\citation{MN16}
\citation{Pru02}
\citation{Vol08}
\citation{HSZ14}
\citation{HSR16}
\citation{QMN15}
\citation{Bar04}
\citation{Cha10}
\citation{HSR16}
\citation{QMN15}
\citation{Bal14}
\newlabel{eq:nonlinear-system-reduced}{{2.2}{29}{Reduced basis methods for nonlinear partial differential equations}{equation.2.2}{}}
\citation{Ams10}
\citation{Chen17}
\citation{Haa13}
\newlabel{eq:interpolation-function}{{2}{30}{Reduced basis methods for nonlinear partial differential equations}{equation.2.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Parametrized nonlinear PDEs}{30}{section.2.1}}
\newlabel{section:Parametrized nonlinear PDEs}{{2.1}{30}{Parametrized nonlinear PDEs}{section.2.1}{}}
\citation{Qua10}
\citation{MM10}
\newlabel{eq:pde-differential-form}{{2.3}{31}{Parametrized nonlinear PDEs}{equation.2.3}{}}
\newlabel{eq:pde-variational-form}{{2.4}{31}{Parametrized nonlinear PDEs}{equation.2.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.1}Nonlinear Poisson equation}{31}{subsection.2.1.1}}
\newlabel{section:Nonlinear Poisson equation}{{2.1.1}{31}{Nonlinear Poisson equation}{subsection.2.1.1}{}}
\newlabel{eq:poisson-differential}{{2.5}{31}{Nonlinear Poisson equation}{equation.2.5}{}}
\newlabel{eq:poisson-differential-first-equation}{{2.5a}{31}{Nonlinear Poisson equation}{equation.2.5a}{}}
\citation{Qua10}
\citation{Ran99}
\newlabel{eq:poisson-weak-derivation}{{2.6}{32}{Nonlinear Poisson equation}{equation.2.6}{}}
\newlabel{eq:poisson-weak-forms}{{2.7}{32}{Nonlinear Poisson equation}{equation.2.7}{}}
\newlabel{eq:poisson-weak}{{2.8}{32}{Nonlinear Poisson equation}{equation.2.8}{}}
\citation{QMN15}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.2}Steady Navier-Stokes equations}{33}{subsection.2.1.2}}
\newlabel{section:Steady Navier-Stokes equations}{{2.1.2}{33}{Steady Navier-Stokes equations}{subsection.2.1.2}{}}
\newlabel{eq:ns-differential}{{2.9}{33}{Steady Navier-Stokes equations}{equation.2.9}{}}
\newlabel{eq:mass-conservation}{{2.9a}{33}{Steady Navier-Stokes equations}{equation.2.9a}{}}
\newlabel{eq:momentum-conservation}{{2.9b}{33}{Steady Navier-Stokes equations}{equation.2.9b}{}}
\newlabel{eq:ns-weak}{{2.10}{33}{Steady Navier-Stokes equations}{equation.2.10}{}}
\newlabel{eq:ns-weak-velocity}{{2.10a}{33}{Steady Navier-Stokes equations}{equation.2.10a}{}}
\newlabel{eq:ns-weak-pressure}{{2.10b}{33}{Steady Navier-Stokes equations}{equation.2.10b}{}}
\newlabel{eq:ns-weak-forms}{{2.11}{33}{Steady Navier-Stokes equations}{equation.2.11}{}}
\newlabel{eq:ns-weak-forms-c}{{2.11a}{33}{Steady Navier-Stokes equations}{equation.2.11a}{}}
\newlabel{eq:ns-weak-forms-a}{{2.11b}{34}{Steady Navier-Stokes equations}{equation.2.11b}{}}
\newlabel{eq:ns-weak-forms-b}{{2.11c}{34}{Steady Navier-Stokes equations}{equation.2.11c}{}}
\newlabel{eq:ns-weak-forms-d}{{2.11d}{34}{Steady Navier-Stokes equations}{equation.2.11d}{}}
\newlabel{eq:ns-weak-forms-f1}{{2.11e}{34}{Steady Navier-Stokes equations}{equation.2.11e}{}}
\newlabel{eq:ns-weak-forms-f2}{{2.11f}{34}{Steady Navier-Stokes equations}{equation.2.11f}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}From the original to the reference domain}{34}{section.2.2}}
\newlabel{section:From the original to the reference domain}{{2.2}{34}{From the original to the reference domain}{section.2.2}{}}
\citation{JIR14}
\newlabel{first-compatibility-condition}{{{{(a)}}}{35}{From the original to the reference domain}{Item.12}{}}
\newlabel{second-compatibility-condition}{{{{(b)}}}{35}{From the original to the reference domain}{Item.13}{}}
\newlabel{eq:parametrized-map}{{2.2}{35}{From the original to the reference domain}{Item.13}{}}
\newlabel{eq:pde-differential-reference}{{2.12}{35}{From the original to the reference domain}{equation.2.12}{}}
\newlabel{eq:pde-weak-reference}{{2.13}{35}{From the original to the reference domain}{equation.2.13}{}}
\newlabel{eq:parametrized-map-discrete}{{2.2}{35}{From the original to the reference domain}{equation.2.13}{}}
\citation{Rud64}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}Change of variables formulae}{36}{subsection.2.2.1}}
\newlabel{section:Change of variables formulae}{{2.2.1}{36}{Change of variables formulae}{subsection.2.2.1}{}}
\newlabel{eq:chain-rule-component}{{2.14}{36}{Change of variables formulae}{equation.2.14}{}}
\newlabel{eq:chain-rule}{{2.15}{36}{Change of variables formulae}{equation.2.15}{}}
\citation{Rud64}
\newlabel{eq:chain-rule-bis}{{2.16}{37}{Change of variables formulae}{equation.2.16}{}}
\newlabel{eq:change-of-variables}{{2.17}{37}{Change of variables formulae}{equation.2.17}{}}
\newlabel{eq:change-of-variables-vectorial}{{2.18}{37}{Change of variables formulae}{equation.2.18}{}}
\newlabel{eq:change-of-variables-first}{{2.19}{37}{Change of variables formulae}{equation.2.19}{}}
\newlabel{eq:change-of-variables-second}{{2.20}{37}{Change of variables formulae}{equation.2.20}{}}
\newlabel{eq:change-of-variables-third}{{2.21}{37}{Change of variables formulae}{equation.2.21}{}}
\newlabel{eq:change-of-variables-fourth}{{2.22}{37}{Change of variables formulae}{equation.2.22}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.2}The problems of interest}{37}{subsection.2.2.2}}
\newlabel{section:The problems of interest}{{2.2.2}{37}{The problems of interest}{subsection.2.2.2}{}}
\citation{JIR14}
\newlabel{eq:poisson-weak-reference}{{2.23}{38}{The problems of interest}{equation.2.23}{}}
\newlabel{eq:poisson-weak-forms-reference}{{2.24}{38}{The problems of interest}{equation.2.24}{}}
\newlabel{eq:poisson-weak-forms-reference-first}{{2.24a}{38}{The problems of interest}{equation.2.24a}{}}
\newlabel{eq:poisson-weak-forms-reference-second}{{2.24b}{38}{The problems of interest}{equation.2.24b}{}}
\newlabel{eq:ns-weak-reference}{{2.25}{38}{The problems of interest}{equation.2.25}{}}
\newlabel{eq:ns-weak-forms}{{2.26}{38}{The problems of interest}{equation.2.26}{}}
\newlabel{eq:ns-weak-forms-c-reference}{{2.26a}{38}{The problems of interest}{equation.2.26a}{}}
\newlabel{eq:ns-weak-forms-a-reference}{{2.26b}{38}{The problems of interest}{equation.2.26b}{}}
\newlabel{eq:ns-weak-forms-b-reference}{{2.26c}{38}{The problems of interest}{equation.2.26c}{}}
\newlabel{eq:ns-weak-forms-d-reference}{{2.26d}{38}{The problems of interest}{equation.2.26d}{}}
\newlabel{eq:ns-weak-forms-f1-reference}{{2.26e}{38}{The problems of interest}{equation.2.26e}{}}
\newlabel{eq:ns-weak-forms-f2-reference}{{2.26f}{38}{The problems of interest}{equation.2.26f}{}}
\citation{JIR14}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.3}The boundary displacement-dependent transfinite map (BDD TM)}{39}{subsection.2.2.3}}
\newlabel{section:The boundary displacement-dependent transfinite map}{{2.2.3}{39}{The boundary displacement-dependent transfinite map (BDD TM)}{subsection.2.2.3}{}}
\newlabel{eq:laplace-problem-weight-function}{{2.27}{39}{The boundary displacement-dependent transfinite map (BDD TM)}{equation.2.27}{}}
\citation{JIR14}
\citation{Rud64}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces Representation of the boundary conditions for the Laplace problems \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces \ref  {eq:laplace-problem-weight-function}\unskip \@@italiccorr )}} (\emph  {left}) and \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces \ref  {eq:laplace-problem-projection-function}\unskip \@@italiccorr )}} (\emph  {right}) stated on a exagonal reference domain $\Omega $.\relax }}{40}{figure.caption.16}}
\newlabel{fig:laplace-bc}{{2.1}{40}{Representation of the boundary conditions for the Laplace problems \eqref {eq:laplace-problem-weight-function} (\emph {left}) and \eqref {eq:laplace-problem-projection-function} (\emph {right}) stated on a exagonal reference domain $\Omega $.\relax }{figure.caption.16}{}}
\newlabel{eq:laplace-problem-projection-function}{{2.28}{40}{The boundary displacement-dependent transfinite map (BDD TM)}{equation.2.28}{}}
\newlabel{eq:weight-projection-function}{{2.29}{40}{The boundary displacement-dependent transfinite map (BDD TM)}{equation.2.29}{}}
\newlabel{eq:bddtm}{{2.30}{40}{The boundary displacement-dependent transfinite map (BDD TM)}{equation.2.30}{}}
\citation{JIR14}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces Clockwise enumeration for the edges of the reference squared domain $\Omega $ used in the numerical tests. The coordinates of the vertices are reported too.\relax }}{41}{figure.caption.17}}
\newlabel{fig:bc-square}{{2.2}{41}{Clockwise enumeration for the edges of the reference squared domain $\Omega $ used in the numerical tests. The coordinates of the vertices are reported too.\relax }{figure.caption.17}{}}
\newlabel{eq:my-bddtm}{{2.31}{41}{The boundary displacement-dependent transfinite map (BDD TM)}{equation.2.31}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Well-posedness of the test cases}{41}{section.2.3}}
\newlabel{section:Well-posedness of the test cases}{{2.3}{41}{Well-posedness of the test cases}{section.2.3}{}}
\citation{CR97}
\citation{QMN15}
\citation{ESW04}
\citation{Qua10}
\citation{Rud64}
\@writefile{toc}{\contentsline {section}{\numberline {2.4}Finite element method}{42}{section.2.4}}
\newlabel{section:Finite element method}{{2.4}{42}{Finite element method}{section.2.4}{}}
\newlabel{eq:galerkin}{{2.32}{42}{Finite element method}{equation.2.32}{}}
\newlabel{eq:continuity}{{2.33}{42}{Finite element method}{equation.2.33}{}}
\newlabel{eq:inf-sup}{{2.34}{42}{Finite element method}{equation.2.34}{}}
\citation{QMN15}
\newlabel{eq:newton-linearized-problem}{{2.4}{43}{Finite element method}{equation.2.34}{}}
\newlabel{eq:galerkin-solution}{{2.35}{43}{Finite element method}{equation.2.35}{}}
\newlabel{eq:galerkin-algebraic}{{2.4}{43}{Finite element method}{equation.2.35}{}}
\newlabel{eq:galerkin-nonlinear-system}{{2.36}{43}{Finite element method}{equation.2.36}{}}
\newlabel{eq:galerkin-nonlinear-system-equation}{{2.37}{43}{Finite element method}{equation.2.37}{}}
\newlabel{eq:galerkin-linear-system}{{2.38}{43}{Finite element method}{equation.2.38}{}}
\citation{Qua10}
\newlabel{eq:notation-1}{{2.39}{44}{Finite element method}{equation.2.39}{}}
\newlabel{eq:notation-2}{{2.40}{44}{Finite element method}{equation.2.40}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.1}Nonlinear Poisson equation}{44}{subsection.2.4.1}}
\newlabel{section:Nonlinear Poisson equation (FE)}{{2.4.1}{44}{Nonlinear Poisson equation}{subsection.2.4.1}{}}
\citation{Ran99}
\@writefile{loa}{\contentsline {algorithm}{\numberline {2.1}{\ignorespaces Newton's method applied to the nonlinear system \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces \ref  {eq:galerkin-nonlinear-system}\unskip \@@italiccorr )}}.\relax }}{45}{algorithm.2.1}}
\newlabel{alg:newton-full}{{2.1}{45}{Newton's method applied to the nonlinear system \eqref {eq:galerkin-nonlinear-system}.\relax }{algorithm.2.1}{}}
\newlabel{eq:poisson-residual-vector}{{2.41}{45}{Nonlinear Poisson equation}{equation.2.41}{}}
\newlabel{eq:poisson-jacobian}{{2.42}{45}{Nonlinear Poisson equation}{equation.2.42}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.2}Steady Navier-Stokes equations}{45}{subsection.2.4.2}}
\newlabel{section:Steady Navier-Stokes equations (FE)}{{2.4.2}{45}{Steady Navier-Stokes equations}{subsection.2.4.2}{}}
\citation{Qua10}
\citation{Per02}
\newlabel{eq:inf-sup-ns}{{2.43}{46}{Steady Navier-Stokes equations}{equation.2.43}{}}
\citation{Dep08}
\citation{HSR16}
\citation{QMN15}
\newlabel{eq:B}{{2.4.2}{47}{Steady Navier-Stokes equations}{equation.2.43}{}}
\newlabel{eq:ns-system}{{2.44}{47}{Steady Navier-Stokes equations}{equation.2.44}{}}
\newlabel{eq:ns-jacobian}{{2.4.2}{47}{Steady Navier-Stokes equations}{equation.2.44}{}}
\citation{HSR16}
\citation{Chen17}
\citation{Vol08}
\citation{HSR16}
\citation{Mad06}
\@writefile{toc}{\contentsline {section}{\numberline {2.5}POD-Galerkin RB method}{48}{section.2.5}}
\newlabel{section:POD-Galerkin reduced basis method}{{2.5}{48}{POD-Galerkin RB method}{section.2.5}{}}
\newlabel{eq:rb-solution}{{2.45}{48}{POD-Galerkin RB method}{equation.2.45}{}}
\citation{Mad06}
\citation{Buf12}
\citation{Buf12}
\citation{HSR16}
\citation{Mad06}
\citation{Dep08}
\citation{QMN15}
\newlabel{eq:kolmogorov-L-width}{{2.46}{49}{}{equation.2.46}{}}
\newlabel{eq:pde-rb}{{2.47}{50}{POD-Galerkin RB method}{equation.2.47}{}}
\newlabel{eq:pde-rb-newton}{{2.5}{50}{POD-Galerkin RB method}{equation.2.47}{}}
\newlabel{eq:rb-fe-coefficients}{{2.48}{50}{POD-Galerkin RB method}{equation.2.48}{}}
\newlabel{eq:rb-algebraic-formulation-1}{{2.49}{50}{POD-Galerkin RB method}{equation.2.49}{}}
\newlabel{eq:rb-nonlinear-system}{{2.50}{50}{POD-Galerkin RB method}{equation.2.50}{}}
\citation{QMN15}
\citation{Vol08}
\citation{Lia02}
\newlabel{eq:rb-nonlinear-system-jacobian}{{2.51}{51}{POD-Galerkin RB method}{equation.2.51}{}}
\newlabel{eq:rb-nonlinear-system-newton}{{2.52}{51}{POD-Galerkin RB method}{equation.2.52}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.1}Proper Orthogonal Decomposition}{51}{subsection.2.5.1}}
\newlabel{section:Proper Orthogonal Decomposition}{{2.5.1}{51}{Proper Orthogonal Decomposition}{subsection.2.5.1}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {2.2}{\ignorespaces Newton's method applied to the reduced nonlinear system \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces \ref  {eq:rb-nonlinear-system}\unskip \@@italiccorr )}}.\relax }}{52}{algorithm.2.2}}
\newlabel{alg:newton-rb}{{2.2}{52}{Newton's method applied to the reduced nonlinear system \eqref {eq:rb-nonlinear-system}.\relax }{algorithm.2.2}{}}
\newlabel{eq:svd}{{2.53}{52}{Proper Orthogonal Decomposition}{equation.2.53}{}}
\newlabel{eq:left-singular-vectors}{{2.54}{52}{Proper Orthogonal Decomposition}{equation.2.54}{}}
\newlabel{eq:right-singular-vectors}{{2.55}{52}{Proper Orthogonal Decomposition}{equation.2.55}{}}
\newlabel{eq:snapshot-method}{{2.56}{52}{Proper Orthogonal Decomposition}{equation.2.56}{}}
\newlabel{eq:svd-compact}{{2.5.1}{52}{Proper Orthogonal Decomposition}{equation.2.57}{}}
\citation{Vol08}
\newlabel{eq:svd-compact-matrices}{{2.58}{53}{Proper Orthogonal Decomposition}{equation.2.58}{}}
\newlabel{eq:basis-error}{{2.59}{53}{Proper Orthogonal Decomposition}{equation.2.59}{}}
\newlabel{eq:pod-optimality}{{2.60}{53}{Schmidt-Eckart-Young}{equation.2.60}{}}
\citation{Vol08}
\citation{Vol08}
\newlabel{eq:pod-error}{{2.61}{54}{Proper Orthogonal Decomposition}{equation.2.61}{}}
\newlabel{eq:pod-fe-basis-functions}{{2.62}{54}{Proper Orthogonal Decomposition}{equation.2.62}{}}
\newlabel{eq:discrete-scalar-product}{{2.63}{54}{Proper Orthogonal Decomposition}{equation.2.63}{}}
\citation{HSR16}
\citation{Bur06}
\citation{QMN15}
\newlabel{eq:relative-information-content}{{2.64}{55}{Proper Orthogonal Decomposition}{equation.2.64}{}}
\citation{Pru02}
\citation{Bar04}
\citation{Cha10}
\citation{NMA15}
\@writefile{loa}{\contentsline {algorithm}{\numberline {2.3}{\ignorespaces The POD algorithm.\relax }}{56}{algorithm.2.3}}
\newlabel{alg:pod}{{2.3}{56}{The POD algorithm.\relax }{algorithm.2.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.2}Implementation: details and issues}{56}{subsection.2.5.2}}
\newlabel{section:Implementation}{{2.5.2}{56}{Implementation: details and issues}{subsection.2.5.2}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {2.4}{\ignorespaces The offline and online stages for the POD-Galerkin (POD-G) RB method.\relax }}{56}{algorithm.2.4}}
\newlabel{alg:pod-galerkin}{{2.4}{56}{The offline and online stages for the POD-Galerkin (POD-G) RB method.\relax }{algorithm.2.4}{}}
\citation{QMN15}
\citation{Bal14}
\citation{Bur06}
\citation{Chen17}
\citation{QMN15}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.3}Application to the steady Navier-Stokes equations}{57}{subsection.2.5.3}}
\newlabel{section:Application to the steady Navier-Stokes equations}{{2.5.3}{57}{Application to the steady Navier-Stokes equations}{subsection.2.5.3}{}}
\citation{Bur06}
\citation{Bal14}
\newlabel{eq:ns-reduced-system}{{2.65}{58}{Application to the steady Navier-Stokes equations}{equation.2.65}{}}
\citation{Bal14}
\citation{Cas15}
\citation{Bar04}
\newlabel{eq:supremizer-system}{{2.66}{59}{Application to the steady Navier-Stokes equations}{equation.2.66}{}}
\newlabel{proposition:supremizer-enrichment}{{2.1}{59}{}{proposition.2.1}{}}
\newlabel{eq:inf-sup-ns-reduced}{{2.67}{59}{}{equation.2.67}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.6}A POD-based RB method using neural networks}{60}{section.2.6}}
\newlabel{section:A POD-based RB method using neural networks}{{2.6}{60}{A POD-based RB method using neural networks}{section.2.6}{}}
\newlabel{eq:good-rb-solution}{{2.68}{60}{A POD-based RB method using neural networks}{equation.2.68}{}}
\citation{Imam08}
\newlabel{eq:nonlinear-poisson-example}{{2.69}{61}{A POD-based RB method using neural networks}{equation.2.69}{}}
\newlabel{eq:high-fidelity-projected}{{2.70}{61}{A POD-based RB method using neural networks}{equation.2.70}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces The parametrized computational domain (solid line) for the Poisson problem \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces \ref  {eq:nonlinear-poisson-example}\unskip \@@italiccorr )}}.\relax }}{62}{figure.caption.18}}
\newlabel{fig:nonlinear-poisson-example-domain}{{2.3}{62}{The parametrized computational domain (solid line) for the Poisson problem \eqref {eq:nonlinear-poisson-example}.\relax }{figure.caption.18}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces Left: average relative error between the FE solution for problem \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces \ref  {eq:nonlinear-poisson-example}\unskip \@@italiccorr )}} and either its projection onto the reduced space (red squares) or the POD-Galerkin RB solution (blue circles); the errors have been evaluated on a test parameter set $\Xi _{te} \subset \mathcal  {P}$ consisting of $N_{te} = 50$ randomly picked values. Right: comparison between the online run times for the FE scheme and the POD-Galerkin method on $\Xi _{te}$, with $L = 35$ basis functions included in the reduced model.\relax }}{62}{figure.caption.19}}
\newlabel{fig:nonlinear-poisson-example}{{2.4}{62}{Left: average relative error between the FE solution for problem \eqref {eq:nonlinear-poisson-example} and either its projection onto the reduced space (red squares) or the POD-Galerkin RB solution (blue circles); the errors have been evaluated on a test parameter set $\Xi _{te} \subset \mathcal {P}$ consisting of $N_{te} = 50$ randomly picked values. Right: comparison between the online run times for the FE scheme and the POD-Galerkin method on $\Xi _{te}$, with $L = 35$ basis functions included in the reduced model.\relax }{figure.caption.19}{}}
\newlabel{eq:map-to-approximate}{{2.71}{62}{A POD-based RB method using neural networks}{equation.2.71}{}}
\citation{Ams10}
\citation{Chen17}
\@writefile{loa}{\contentsline {algorithm}{\numberline {2.5}{\ignorespaces Selection of an optimal network configuration.\relax }}{64}{algorithm.2.5}}
\newlabel{alg:podnn-training}{{2.5}{64}{Selection of an optimal network configuration.\relax }{algorithm.2.5}{}}
\newlabel{eq:pod-nn-mse}{{2.72}{65}{A POD-based RB method using neural networks}{equation.2.72}{}}
\newlabel{eq:pod-nn-solution}{{2.73}{65}{A POD-based RB method using neural networks}{equation.2.73}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.5}{\ignorespaces Three point sets randomly generate through the latin hypercube sampling. Observe the good coverage provided by the union of the sets, with only a few overlapping points.\relax }}{65}{figure.caption.20}}
\newlabel{fig:lhs}{{2.5}{65}{Three point sets randomly generate through the latin hypercube sampling. Observe the good coverage provided by the union of the sets, with only a few overlapping points.\relax }{figure.caption.20}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {2.6}{\ignorespaces The offline and online stages for the POD-NN RB method.\relax }}{66}{algorithm.2.6}}
\newlabel{alg:pod-nn}{{2.6}{66}{The offline and online stages for the POD-NN RB method.\relax }{algorithm.2.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6.1}An a priori error analysis}{66}{subsection.2.6.1}}
\newlabel{section:An a priori error analysis}{{2.6.1}{66}{An a priori error analysis}{subsection.2.6.1}{}}
\newlabel{eq:error-analysis-1}{{2.74}{66}{An a priori error analysis}{equation.2.74}{}}
\newlabel{eq:error-analysis-2}{{2.75}{66}{An a priori error analysis}{equation.2.75}{}}
\newlabel{eq:error-analysis-3}{{2.76}{67}{An a priori error analysis}{equation.2.76}{}}
\newlabel{eq:error-analysis-4}{{2.77}{67}{An a priori error analysis}{equation.2.77}{}}
\newlabel{eq:error-analysis-5}{{2.78}{68}{An a priori error analysis}{equation.2.78}{}}
\newlabel{eq:error-analysis-final}{{2.6.1}{68}{An a priori error analysis}{equation.2.78}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Numerical results}{69}{chapter.3}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chapter:Numerical results}{{3}{69}{Numerical results}{chapter.3}{}}
\newlabel{eq:podg-error}{{3.1}{69}{Numerical results}{equation.3.1}{}}
\newlabel{eq:podnn-error}{{3.2}{69}{Numerical results}{equation.3.2}{}}
\newlabel{eq:projection-error}{{3.3}{70}{Numerical results}{equation.3.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}One-dimensional Poisson equation}{71}{section.3.1}}
\newlabel{section:One-dimensional Poisson equation (results)}{{3.1}{71}{One-dimensional Poisson equation}{section.3.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.1}Linear test case}{71}{subsection.3.1.1}}
\newlabel{section:poisson1d-1}{{3.1.1}{71}{Linear test case}{subsection.3.1.1}{}}
\newlabel{eq:poisson1d-1}{{3.4}{71}{Linear test case}{equation.3.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces Left: error analysis for the POD-G RB method applied to \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces \ref  {eq:poisson1d-1}\unskip \@@italiccorr )}}; the results refer to a POD basis constructed based on $N = 25$, $50$, $100$ snapshots; for $N = 100$, the singular values of the corresponding snapshot matrix are also shown. Right: FE (solid line) and POD-G (markers) solutions for three different parameter values.\relax }}{71}{figure.caption.21}}
\newlabel{fig:poisson1d-1-fig1}{{3.1}{71}{Left: error analysis for the POD-G RB method applied to \eqref {eq:poisson1d-1}; the results refer to a POD basis constructed based on $N = 25$, $50$, $100$ snapshots; for $N = 100$, the singular values of the corresponding snapshot matrix are also shown. Right: FE (solid line) and POD-G (markers) solutions for three different parameter values.\relax }{figure.caption.21}{}}
\citation{Mol93}
\citation{GMW81}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces In respect to the approximation of the map \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces \ref  {eq:map-to-approximate}\unskip \@@italiccorr )}} for the Poisson problem \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces \ref  {eq:poisson1d-1}\unskip \@@italiccorr )}} via neural networks, comparison between three supervised algorithms - Levenberg-Marquardt (blue circles), Scaled Conjugate Gradient (orange squares), BFGS Quasi-Newton (yellow traingles) - in terms of the optimal configuration detected (\emph  {left}) and the associated (averaged) test error (\emph  {right}) for different amounts of training samples.\relax }}{72}{figure.caption.22}}
\newlabel{fig:poisson1d-1-fig2}{{3.2}{72}{In respect to the approximation of the map \eqref {eq:map-to-approximate} for the Poisson problem \eqref {eq:poisson1d-1} via neural networks, comparison between three supervised algorithms - Levenberg-Marquardt (blue circles), Scaled Conjugate Gradient (orange squares), BFGS Quasi-Newton (yellow traingles) - in terms of the optimal configuration detected (\emph {left}) and the associated (averaged) test error (\emph {right}) for different amounts of training samples.\relax }{figure.caption.22}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces Left: relative error yielded by the POD-NN RB method applied to \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces \ref  {eq:poisson1d-1}\unskip \@@italiccorr )}}; the convergence to the projection error (solid line) for $L = 10$ is analyzed in terms of both the number of neurons included in the neural network and the dimension of the training set. Right: FE and POD-NN solutions for three parameter values.\relax }}{72}{figure.caption.22}}
\newlabel{fig:poisson1d-1-fig3}{{3.3}{72}{Left: relative error yielded by the POD-NN RB method applied to \eqref {eq:poisson1d-1}; the convergence to the projection error (solid line) for $L = 10$ is analyzed in terms of both the number of neurons included in the neural network and the dimension of the training set. Right: FE and POD-NN solutions for three parameter values.\relax }{figure.caption.22}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.2}Nonlinear test case, two parameters}{73}{subsection.3.1.2}}
\newlabel{section:poisson1d-2}{{3.1.2}{73}{Nonlinear test case, two parameters}{subsection.3.1.2}{}}
\newlabel{eq:poisson1d-2}{{3.5}{73}{Nonlinear test case, two parameters}{equation.3.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.4}{\ignorespaces Error analysis for the POD-Galerkin RB method applied to the problem \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces \ref  {eq:poisson1d-2}\unskip \@@italiccorr )}}. The lower-bound provided by the projection error \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces \ref  {eq:projection-error}\unskip \@@italiccorr )}} is reported as reference.\relax }}{73}{figure.caption.23}}
\newlabel{fig:poisson1d-2-fig1}{{3.4}{73}{Error analysis for the POD-Galerkin RB method applied to the problem \eqref {eq:poisson1d-2}. The lower-bound provided by the projection error \eqref {eq:projection-error} is reported as reference.\relax }{figure.caption.23}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.5}{\ignorespaces Left: error analysis for the POD-NN RB method (dotted lines) applied to problem \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces \ref  {eq:poisson1d-2}\unskip \@@italiccorr )}} for several numbers of training samples. The solid sections represent the stepsfollowed by the Algortithm \ref  {alg:podnn-training}; the error yielded by the POD-G method using $L = 8$ or $L = 15$ basis functions are reported as reference. Right: coherence of FE and POD-NN solutions for three input vectors $\bm  {\mu }$.\relax }}{74}{figure.caption.24}}
\newlabel{fig:poisson1d-2-fig2}{{3.5}{74}{Left: error analysis for the POD-NN RB method (dotted lines) applied to problem \eqref {eq:poisson1d-2} for several numbers of training samples. The solid sections represent the stepsfollowed by the Algortithm \ref {alg:podnn-training}; the error yielded by the POD-G method using $L = 8$ or $L = 15$ basis functions are reported as reference. Right: coherence of FE and POD-NN solutions for three input vectors $\bg {\mu }$.\relax }{figure.caption.24}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.6}{\ignorespaces From left to right, top to bottom: regression plots for the first, seventh, eighth and nine scalar components of the outputs provided on $\Xi _{te}$ by a network with $H_1 = H_2 = 25$ neurons per hidden layer, trained to approximate the map \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces \ref  {eq:map-to-approximate}\unskip \@@italiccorr )}} for problem \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces \ref  {eq:poisson1d-2}\unskip \@@italiccorr )}}.\relax }}{75}{figure.caption.25}}
\newlabel{fig:poisson1d-2-fig3}{{3.6}{75}{From left to right, top to bottom: regression plots for the first, seventh, eighth and nine scalar components of the outputs provided on $\Xi _{te}$ by a network with $H_1 = H_2 = 25$ neurons per hidden layer, trained to approximate the map \eqref {eq:map-to-approximate} for problem \eqref {eq:poisson1d-2}.\relax }{figure.caption.25}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.3}Nonlinear test case, three parameters}{75}{subsection.3.1.3}}
\newlabel{section:poisson1d-3}{{3.1.3}{75}{Nonlinear test case, three parameters}{subsection.3.1.3}{}}
\newlabel{eq:poisson1d-3}{{3.6}{75}{Nonlinear test case, three parameters}{equation.3.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.7}{\ignorespaces The first $10$ POD basis functions for the (nonlinear) Poisson problem \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces \ref  {eq:poisson1d-3}\unskip \@@italiccorr )}}.\relax }}{76}{figure.caption.26}}
\newlabel{fig:poisson1d-3-fig1}{{3.7}{76}{The first $10$ POD basis functions for the (nonlinear) Poisson problem \eqref {eq:poisson1d-3}.\relax }{figure.caption.26}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {$\psi _1$}}}{76}{subfigure.7.1}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {$\psi _2$}}}{76}{subfigure.7.2}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {$\psi _3$}}}{76}{subfigure.7.3}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(d)}{\ignorespaces {$\psi _4$}}}{76}{subfigure.7.4}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(e)}{\ignorespaces {$\psi _5$}}}{76}{subfigure.7.5}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(f)}{\ignorespaces {$\psi _6$}}}{76}{subfigure.7.6}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(g)}{\ignorespaces {$\psi _7$}}}{76}{subfigure.7.7}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(h)}{\ignorespaces {$\psi _8$}}}{76}{subfigure.7.8}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(i)}{\ignorespaces {$\psi _9$}}}{76}{subfigure.7.9}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(j)}{\ignorespaces {$\psi _{10}$}}}{76}{subfigure.7.10}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.8}{\ignorespaces Convergence analysis for the POD-G and POD-NN methods applied to problem \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces \ref  {eq:poisson1d-3}\unskip \@@italiccorr )}} (\emph  {left}) and comparison between the FE and the POD-NN solutions for three parameter values (\emph  {right}). These latter results have been obtained via a neural network with $H_1 = H_2 = 35$ units per hidden layer, and considering $L = 10$ POD modes.\relax }}{76}{figure.caption.27}}
\newlabel{fig:poisson1d-3-fig2}{{3.8}{76}{Convergence analysis for the POD-G and POD-NN methods applied to problem \eqref {eq:poisson1d-3} (\emph {left}) and comparison between the FE and the POD-NN solutions for three parameter values (\emph {right}). These latter results have been obtained via a neural network with $H_1 = H_2 = 35$ units per hidden layer, and considering $L = 10$ POD modes.\relax }{figure.caption.27}{}}
\newlabel{eq:podnn-solution-full}{{3.7}{77}{Nonlinear test case, three parameters}{equation.3.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.9}{\ignorespaces Sensitivity analysis for the POD-NN method applied to problem \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces \ref  {eq:poisson1d-3}\unskip \@@italiccorr )}} with respect to the number of POD coefficients (per sample) used during the training; to avoid overfitting, $N_{tr} = 300$ learning patterns have been used.\relax }}{77}{figure.caption.28}}
\newlabel{fig:poisson1d-3-fig3}{{3.9}{77}{Sensitivity analysis for the POD-NN method applied to problem \eqref {eq:poisson1d-3} with respect to the number of POD coefficients (per sample) used during the training; to avoid overfitting, $N_{tr} = 300$ learning patterns have been used.\relax }{figure.caption.28}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Two-dimensional Poisson equation}{78}{section.3.2}}
\newlabel{section:Two-dimensional Poisson equation (results)}{{3.2}{78}{Two-dimensional Poisson equation}{section.3.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.10}{\ignorespaces The quadrilateral domain used in the simulations (\emph  {left}, solid line) and the parametrizations of its sides (\emph  {right}).\relax }}{78}{figure.caption.29}}
\newlabel{fig:quadrilateral-domain}{{3.10}{78}{The quadrilateral domain used in the simulations (\emph {left}, solid line) and the parametrizations of its sides (\emph {right}).\relax }{figure.caption.29}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.11}{\ignorespaces The stenosis geometry employed in the simulations (\emph  {left}) and the parametrizations of its sides (\emph  {right}).\relax }}{78}{figure.caption.29}}
\newlabel{fig:stenosis-domain}{{3.11}{78}{The stenosis geometry employed in the simulations (\emph {left}) and the parametrizations of its sides (\emph {right}).\relax }{figure.caption.29}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.1}Linear test case}{79}{subsection.3.2.1}}
\newlabel{section:poisson2d-1}{{3.2.1}{79}{Linear test case}{subsection.3.2.1}{}}
\newlabel{eq:poisson2d-1}{{3.8}{79}{Linear test case}{equation.3.8}{}}
\newlabel{eq:poisson2d-1-fe-system}{{3.9}{79}{Linear test case}{equation.3.9}{}}
\newlabel{eq:poisson2d-1-podg-system}{{3.2.1}{79}{Linear test case}{equation.3.9}{}}
\newlabel{eq:poisson2d-1-podg-matrices}{{3.2.1}{79}{Linear test case}{equation.3.9}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.12}{\ignorespaces Online run times for the FE, POD-G and POD-NN methods applied to problem \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces \ref  {eq:poisson2d-1}\unskip \@@italiccorr )}}, for $N_{te} = 100$ randomly generated parameter values. \relax }}{80}{figure.caption.30}}
\newlabel{fig:poisson2d-1-fig1}{{3.12}{80}{Online run times for the FE, POD-G and POD-NN methods applied to problem \eqref {eq:poisson2d-1}, for $N_{te} = 100$ randomly generated parameter values. \relax }{figure.caption.30}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.13}{\ignorespaces On the left, error analysis for the POD-G (blue dashed line) and the POD-NN (dotted lines) methods applied to problem \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces \ref  {eq:poisson2d-1}\unskip \@@italiccorr )}}. For the latter, a convergence analysis with respect to the number of training samples and hidden neurons used is provided on the right. The solid tracts denote the steps followed by the automatic training routine \ref  {alg:podnn-training}. All the results refer to neural networks exposed to $L = 15$ generalized coordinates per learning sample.\relax }}{80}{figure.caption.31}}
\newlabel{fig:poisson2d-1-fig2}{{3.13}{80}{On the left, error analysis for the POD-G (blue dashed line) and the POD-NN (dotted lines) methods applied to problem \eqref {eq:poisson2d-1}. For the latter, a convergence analysis with respect to the number of training samples and hidden neurons used is provided on the right. The solid tracts denote the steps followed by the automatic training routine \ref {alg:podnn-training}. All the results refer to neural networks exposed to $L = 15$ generalized coordinates per learning sample.\relax }{figure.caption.31}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.2}Nonlinear test case, two parameters}{81}{subsection.3.2.2}}
\newlabel{section:poisson2d-2}{{3.2.2}{81}{Nonlinear test case, two parameters}{subsection.3.2.2}{}}
\newlabel{eq:poisson2d-2}{{3.10}{81}{Nonlinear test case, two parameters}{equation.3.10}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.14}{\ignorespaces Finite element (\emph  {left}), POD-G (\emph  {center}) and POD-NN (\emph  {right}) solution to the semilinear Poisson problem \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces \ref  {eq:poisson2d-2}\unskip \@@italiccorr )}} re-stated over the reference domain $\Omega $, with $\bm  {\mu } = (0.835, \tmspace  +\thinmuskip {.1667em} 0.034)$.\relax }}{82}{figure.caption.32}}
\newlabel{fig:poisson2d-2-fig1}{{3.14}{82}{Finite element (\emph {left}), POD-G (\emph {center}) and POD-NN (\emph {right}) solution to the semilinear Poisson problem \eqref {eq:poisson2d-2} re-stated over the reference domain $\Omega $, with $\bg {\mu } = (0.835, \, 0.034)$.\relax }{figure.caption.32}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.15}{\ignorespaces Relative errors (\emph  {left}) and online run times (\emph  {right}) for the POD-G and the POD-NN methods applied to problem \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces \ref  {eq:poisson2d-2}\unskip \@@italiccorr )}} for $N_{te} = 50$ randomly picked parameter values.\relax }}{82}{figure.caption.32}}
\newlabel{fig:poisson2d-2-fig2}{{3.15}{82}{Relative errors (\emph {left}) and online run times (\emph {right}) for the POD-G and the POD-NN methods applied to problem \eqref {eq:poisson2d-2} for $N_{te} = 50$ randomly picked parameter values.\relax }{figure.caption.32}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.16}{\ignorespaces Convergence analysis for the POD-NN RB method applied to the BVP \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces \ref  {eq:poisson2d-2}\unskip \@@italiccorr )}}. The results have been obtained via three-layers perceptrons with $H_1 = H_2 \in {\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\nulldelimiterspace \z@ \left \delimiter "42663A9 \vcenter to\@ne \big@size {}\right .$}\box \z@ }15, \tmspace  +\thinmuskip {.1667em} 20, \tmspace  +\thinmuskip {.1667em} 25, \tmspace  +\thinmuskip {.1667em} 30, \tmspace  +\thinmuskip {.1667em} 35 {\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\nulldelimiterspace \z@ \left \delimiter "52673AA \vcenter to\@ne \big@size {}\right .$}\box \z@ }$ neurons per hidden layer and trained with $N_{tr} \in {\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\nulldelimiterspace \z@ \left \delimiter "42663A9 \vcenter to\@ne \big@size {}\right .$}\box \z@ }100, \tmspace  +\thinmuskip {.1667em} 200, \tmspace  +\thinmuskip {.1667em} 300 {\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\nulldelimiterspace \z@ \left \delimiter "52673AA \vcenter to\@ne \big@size {}\right .$}\box \z@ }$ learning patterns. Each pattern consists of an input vector $\bm  {\mu } \in \mathcal  {P}$ and $L = 35$ POD coefficients ${\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\nulldelimiterspace \z@ \left (\vcenter to\@ne \big@size {}\right .$}\box \z@ } \mathbf  {u}_h(\bm  {\mu }), \bm  {\psi }_i {\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\nulldelimiterspace \z@ \left )\vcenter to\@ne \big@size {}\right .$}\box \z@ }_{\math@bb  {R}^M}$, $i = 1, \tmspace  +\thinmuskip {.1667em} \ldots  \tmspace  +\thinmuskip {.1667em} , L$, as teaching inputs.\relax }}{82}{figure.caption.32}}
\newlabel{fig:poisson2d-2-fig3}{{3.16}{82}{Convergence analysis for the POD-NN RB method applied to the BVP \eqref {eq:poisson2d-2}. The results have been obtained via three-layers perceptrons with $H_1 = H_2 \in \big \lbrace 15, \, 20, \, 25, \, 30, \, 35 \big \rbrace $ neurons per hidden layer and trained with $N_{tr} \in \big \lbrace 100, \, 200, \, 300 \big \rbrace $ learning patterns. Each pattern consists of an input vector $\bg {\mu } \in \mathcal {P}$ and $L = 35$ POD coefficients $\big ( \mathbf {u}_h(\bg {\mu }), \bg {\psi }_i \big )_{\mathbb {R}^M}$, $i = 1, \, \ldots \, , L$, as teaching inputs.\relax }{figure.caption.32}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.3}Nonlinear test case, three parameters}{83}{subsection.3.2.3}}
\newlabel{section:poisson2d-3}{{3.2.3}{83}{Nonlinear test case, three parameters}{subsection.3.2.3}{}}
\newlabel{eq:poisson2d-3}{{3.11}{83}{Nonlinear test case, three parameters}{equation.3.11}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.17}{\ignorespaces FE solution (\emph  {top left}) to the Poisson problem \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces \ref  {eq:poisson2d-3}\unskip \@@italiccorr )}} with $\bm  {\mu } = (0.349, \tmspace  +\thinmuskip {.1667em} -0.413, \tmspace  +\thinmuskip {.1667em} 4.257)$, and pointwise errors yielded by either its projection onto $V_{\texttt  {rb}}$ (\emph  {top right}), or the POD-G method (\emph  {bottom left}), or the POD-NN method (\emph  {bottom right}). The results have been obtained by employing $L = 30$ POD modes.\relax }}{83}{figure.caption.33}}
\newlabel{fig:poisson2d-3-fig1}{{3.17}{83}{FE solution (\emph {top left}) to the Poisson problem \eqref {eq:poisson2d-3} with $\bg {\mu } = (0.349, \, -0.413, \, 4.257)$, and pointwise errors yielded by either its projection onto $V_{\texttt {rb}}$ (\emph {top right}), or the POD-G method (\emph {bottom left}), or the POD-NN method (\emph {bottom right}). The results have been obtained by employing $L = 30$ POD modes.\relax }{figure.caption.33}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.18}{\ignorespaces Error analysis (\emph  {left}) and online CPU time (\emph  {right}) for the POD-G and the POD-NN methods applied to problem \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces \ref  {eq:poisson2d-3}\unskip \@@italiccorr )}}. $N_{te} = 50$ randomly picked parameter values are considered. The reduced basis have been generated via POD, relying on $N = 100$ snapshots. The second plot refers to RB models including $L = 30$ modal functions; within the POD-NN framework, a neural network emboding $35$ neurons per inner layer has been used.\relax }}{84}{figure.caption.34}}
\newlabel{fig:poisson2d-3-fig2}{{3.18}{84}{Error analysis (\emph {left}) and online CPU time (\emph {right}) for the POD-G and the POD-NN methods applied to problem \eqref {eq:poisson2d-3}. $N_{te} = 50$ randomly picked parameter values are considered. The reduced basis have been generated via POD, relying on $N = 100$ snapshots. The second plot refers to RB models including $L = 30$ modal functions; within the POD-NN framework, a neural network emboding $35$ neurons per inner layer has been used.\relax }{figure.caption.34}{}}
\citation{Deb78}
\citation{Ams10}
\@writefile{lof}{\contentsline {figure}{\numberline {3.19}{\ignorespaces Convergence analysis with respect to the number of hidden neurons (\emph  {left}) and modal functions (\emph  {right}) used within the POD-NN framework applied to problem \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces \ref  {eq:poisson2d-3}\unskip \@@italiccorr )}}. The results provided in the first plot have been obtained using $L = 30$ modes; the solid tracts refer to the steps performed by Algorithm \ref  {alg:podnn-training}.\relax }}{85}{figure.caption.35}}
\newlabel{fig:poisson2d-3-fig3}{{3.19}{85}{Convergence analysis with respect to the number of hidden neurons (\emph {left}) and modal functions (\emph {right}) used within the POD-NN framework applied to problem \eqref {eq:poisson2d-3}. The results provided in the first plot have been obtained using $L = 30$ modes; the solid tracts refer to the steps performed by Algorithm \ref {alg:podnn-training}.\relax }{figure.caption.35}{}}
\newlabel{eq:podcs-error}{{3.2.3}{85}{Nonlinear test case, three parameters}{figure.caption.35}{}}
\citation{Per02}
\citation{Dho14}
\@writefile{lof}{\contentsline {figure}{\numberline {3.20}{\ignorespaces Average relative errors (\emph  {left}) and online run times (\emph  {right}) on $\Xi _{te}$ for the POD-CS and the POD-NN methods applied to problem \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces \ref  {eq:poisson2d-3}\unskip \@@italiccorr )}}. For the latter, the results refer to an MLP equipped with $H_1 = H_2 = 35$ neurons per hidden layer.\relax }}{86}{figure.caption.36}}
\newlabel{fig:poisson2d-3-fig4}{{3.20}{86}{Average relative errors (\emph {left}) and online run times (\emph {right}) on $\Xi _{te}$ for the POD-CS and the POD-NN methods applied to problem \eqref {eq:poisson2d-3}. For the latter, the results refer to an MLP equipped with $H_1 = H_2 = 35$ neurons per hidden layer.\relax }{figure.caption.36}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}The lid-driven cavity problem}{86}{section.3.3}}
\newlabel{section:The lid-driven cavity problem}{{3.3}{86}{The lid-driven cavity problem}{section.3.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.21}{\ignorespaces Computational domain (\emph  {left}) and enforced velocity at the boundaries (\emph  {right}) for the lid-driven cavity problem for the uncompressible Navier-Stokes equations.\relax }}{87}{figure.caption.37}}
\newlabel{fig:dc-domain}{{3.21}{87}{Computational domain (\emph {left}) and enforced velocity at the boundaries (\emph {right}) for the lid-driven cavity problem for the uncompressible Navier-Stokes equations.\relax }{figure.caption.37}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.22}{\ignorespaces The computational mesh $\Omega _h$ used in the simulations.\relax }}{87}{figure.caption.38}}
\newlabel{fig:dc-mesh}{{3.22}{87}{The computational mesh $\Omega _h$ used in the simulations.\relax }{figure.caption.38}{}}
\citation{Dho14}
\@writefile{lof}{\contentsline {figure}{\numberline {3.23}{\ignorespaces Velocity streamlines (\emph  {top}) and pressure distribution (\emph  {bottom}) for the lid-driven cavity problem, computed through the FE method. Three different parameter values are considered; for all configurations, the Reynold's number is $400$.\relax }}{88}{figure.caption.39}}
\newlabel{fig:dc-solutions-different-domains}{{3.23}{88}{Velocity streamlines (\emph {top}) and pressure distribution (\emph {bottom}) for the lid-driven cavity problem, computed through the FE method. Three different parameter values are considered; for all configurations, the Reynold's number is $400$.\relax }{figure.caption.39}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {$\bm {\mu } = {\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\nulldelimiterspace \z@ \left (\vcenter to\@ne \big@size {}\right .$}\box \z@ } 1, \tmspace +\thinmuskip {.1667em} \nicefrac {2}{\sqrt {3}}, \tmspace +\thinmuskip {.1667em} \nicefrac {2 \pi }{3} {\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\nulldelimiterspace \z@ \left )\vcenter to\@ne \big@size {}\right .$}\box \z@ }$}}}{88}{subfigure.23.1}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {$\bm {\mu } = {\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\nulldelimiterspace \z@ \left (\vcenter to\@ne \big@size {}\right .$}\box \z@ } 1, \tmspace +\thinmuskip {.1667em} 1, \tmspace +\thinmuskip {.1667em} \nicefrac {\pi }{2} {\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\nulldelimiterspace \z@ \left )\vcenter to\@ne \big@size {}\right .$}\box \z@ }$}}}{88}{subfigure.23.2}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {$\bm {\mu } = {\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\nulldelimiterspace \z@ \left (\vcenter to\@ne \big@size {}\right .$}\box \z@ } 1, \tmspace +\thinmuskip {.1667em} \nicefrac {2}{\sqrt {3}}, \tmspace +\thinmuskip {.1667em} \nicefrac {\pi }{3} {\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\nulldelimiterspace \z@ \left )\vcenter to\@ne \big@size {}\right .$}\box \z@ }$}}}{88}{subfigure.23.3}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(d)}{\ignorespaces {$\bm {\mu } = {\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\nulldelimiterspace \z@ \left (\vcenter to\@ne \big@size {}\right .$}\box \z@ } 1, \tmspace +\thinmuskip {.1667em} \nicefrac {2}{\sqrt {3}}, \tmspace +\thinmuskip {.1667em} \nicefrac {2 \pi }{3} {\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\nulldelimiterspace \z@ \left )\vcenter to\@ne \big@size {}\right .$}\box \z@ }$}}}{88}{subfigure.23.4}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(e)}{\ignorespaces {$\bm {\mu } = {\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\nulldelimiterspace \z@ \left (\vcenter to\@ne \big@size {}\right .$}\box \z@ } 1, \tmspace +\thinmuskip {.1667em} 1, \tmspace +\thinmuskip {.1667em} \nicefrac {\pi }{2} {\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\nulldelimiterspace \z@ \left )\vcenter to\@ne \big@size {}\right .$}\box \z@ }$}}}{88}{subfigure.23.5}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(f)}{\ignorespaces {$\bm {\mu } = {\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\nulldelimiterspace \z@ \left (\vcenter to\@ne \big@size {}\right .$}\box \z@ } 1, \tmspace +\thinmuskip {.1667em} \nicefrac {2}{\sqrt {3}}, \tmspace +\thinmuskip {.1667em} \nicefrac {\pi }{3} {\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\nulldelimiterspace \z@ \left )\vcenter to\@ne \big@size {}\right .$}\box \z@ }$}}}{88}{subfigure.23.6}}
\citation{Bal14}
\@writefile{lof}{\contentsline {figure}{\numberline {3.24}{\ignorespaces Velocity streamlines for the lid-driven cavity benchmark with $\bm  {\mu } = {\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\nulldelimiterspace \z@ \left (\vcenter to\@ne \big@size {}\right .$}\box \z@ } 1, \tmspace  +\thinmuskip {.1667em} \sqrt  {2}, \tmspace  +\thinmuskip {.1667em} \nicefrac  {\pi }{4} {\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\nulldelimiterspace \z@ \left )\vcenter to\@ne \big@size {}\right .$}\box \z@ }$ and either $Re = 200$ (\emph  {left}) or $Re = 400$ (\emph  {right}). The solutions have been computed through the FE method.\relax }}{89}{figure.caption.40}}
\newlabel{fig:dc-solutions-different-reynolds}{{3.24}{89}{Velocity streamlines for the lid-driven cavity benchmark with $\bg {\mu } = \big ( 1, \, \sqrt {2}, \, \nicefrac {\pi }{4} \big )$ and either $Re = 200$ (\emph {left}) or $Re = 400$ (\emph {right}). The solutions have been computed through the FE method.\relax }{figure.caption.40}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {$\bm {\mu } = {\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\nulldelimiterspace \z@ \left (\vcenter to\@ne \big@size {}\right .$}\box \z@ } 1, \tmspace +\thinmuskip {.1667em} \sqrt {2}, \tmspace +\thinmuskip {.1667em} \nicefrac {\pi }{4} {\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\nulldelimiterspace \z@ \left )\vcenter to\@ne \big@size {}\right .$}\box \z@ }$, $Re = 200$}}}{89}{subfigure.24.1}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {$\bm {\mu } = {\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\nulldelimiterspace \z@ \left (\vcenter to\@ne \big@size {}\right .$}\box \z@ } 1, \tmspace +\thinmuskip {.1667em} \sqrt {2}, \tmspace +\thinmuskip {.1667em} \nicefrac {\pi }{4} {\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\nulldelimiterspace \z@ \left )\vcenter to\@ne \big@size {}\right .$}\box \z@ }$, $Re = 400$}}}{89}{subfigure.24.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.25}{\ignorespaces First $5$ POD modes for the velocity (\emph  {left}), supremizers (\emph  {center}) and pressure (\emph  {right}) for the parametrized lid-driven cavity problem with $Re = 200$.\relax }}{90}{figure.caption.41}}
\newlabel{fig:dc-modal-functions}{{3.25}{90}{First $5$ POD modes for the velocity (\emph {left}), supremizers (\emph {center}) and pressure (\emph {right}) for the parametrized lid-driven cavity problem with $Re = 200$.\relax }{figure.caption.41}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.26}{\ignorespaces Velocity (\emph  {left}) and pressure (\emph  {right}) error analysis for the POD-G and POD-NN methods applied to the lid-driven cavity problem with $Re = 200$.\relax }}{91}{figure.caption.42}}
\newlabel{fig:dc-200-error-analysis}{{3.26}{91}{Velocity (\emph {left}) and pressure (\emph {right}) error analysis for the POD-G and POD-NN methods applied to the lid-driven cavity problem with $Re = 200$.\relax }{figure.caption.42}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.27}{\ignorespaces Velocity (\emph  {left}) and pressure (\emph  {right}) error analysis for the POD-G and POD-NN methods applied to the lid-driven cavity problem with $Re = 400$.\relax }}{92}{figure.caption.43}}
\newlabel{fig:dc-400-error-analysis}{{3.27}{92}{Velocity (\emph {left}) and pressure (\emph {right}) error analysis for the POD-G and POD-NN methods applied to the lid-driven cavity problem with $Re = 400$.\relax }{figure.caption.43}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.28}{\ignorespaces Convergence analysis with respect to the number of hidden neurons and training samples used within the POD-NN procedure to approximate the velocity field (\emph  {left}) and the pressure distribution (\emph  {right}) by means of $35$ modal functions. The Reynold's number is either $200$ (\emph  {top}) or $400$ (\emph  {bottom}).\relax }}{93}{figure.caption.44}}
\newlabel{fig:dc-nn-convergence}{{3.28}{93}{Convergence analysis with respect to the number of hidden neurons and training samples used within the POD-NN procedure to approximate the velocity field (\emph {left}) and the pressure distribution (\emph {right}) by means of $35$ modal functions. The Reynold's number is either $200$ (\emph {top}) or $400$ (\emph {bottom}).\relax }{figure.caption.44}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.29}{\ignorespaces Online run times for the POD-G and the POD-NN method applied to the lid-driven cavity problem with $Re = 200$ (\emph  {left}) and $Re = 400$ (\emph  {right}). $N_{te} = 75$ test configurations are considered. For the POD-NN method, the reported times include the (sequential) evaluation of both neural networks for the velocity and pressure field.\relax }}{93}{figure.caption.45}}
\newlabel{fig:dc-time}{{3.29}{93}{Online run times for the POD-G and the POD-NN method applied to the lid-driven cavity problem with $Re = 200$ (\emph {left}) and $Re = 400$ (\emph {right}). $N_{te} = 75$ test configurations are considered. For the POD-NN method, the reported times include the (sequential) evaluation of both neural networks for the velocity and pressure field.\relax }{figure.caption.45}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.30}{\ignorespaces Pressure contour at three parameter values, as computed through the FE (\emph  {top row}), POD-G (\emph  {middle row}) and POD-NN (\emph  {bottom row}) method. For each configuration, the Reynold's number is $400$.\relax }}{94}{figure.caption.46}}
\newlabel{fig:dc-pressure}{{3.30}{94}{Pressure contour at three parameter values, as computed through the FE (\emph {top row}), POD-G (\emph {middle row}) and POD-NN (\emph {bottom row}) method. For each configuration, the Reynold's number is $400$.\relax }{figure.caption.46}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.31}{\ignorespaces $\mathaccentV {widetilde}365{x}$-velocity contour at three parameter values, as computed through the FE (\emph  {top row}) and POD-NN (\emph  {bottom row}) method. For each configuration, the Reynold's number is $400$.\relax }}{95}{figure.caption.47}}
\newlabel{fig:dc-x-velocity}{{3.31}{95}{$\wt {x}$-velocity contour at three parameter values, as computed through the FE (\emph {top row}) and POD-NN (\emph {bottom row}) method. For each configuration, the Reynold's number is $400$.\relax }{figure.caption.47}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.32}{\ignorespaces $\mathaccentV {widetilde}365{y}$-velocity contour at three parameter values, as computed through the FE (\emph  {top row}) and POD-NN (\emph  {bottom row}) method. For each configuration, the Reynold's number is $400$.\relax }}{95}{figure.caption.48}}
\newlabel{fig:dc-y-velocity}{{3.32}{95}{$\wt {y}$-velocity contour at three parameter values, as computed through the FE (\emph {top row}) and POD-NN (\emph {bottom row}) method. For each configuration, the Reynold's number is $400$.\relax }{figure.caption.48}{}}
\citation{Chen17}
\@writefile{lof}{\contentsline {figure}{\numberline {3.33}{\ignorespaces Streamlines at three parameter values, as computed through the FE (\emph  {top row}) and POD-NN (\emph  {bottom row}) method. For each configuration, the Reynold's number is $400$.\relax }}{96}{figure.caption.49}}
\newlabel{fig:dc-streamlines}{{3.33}{96}{Streamlines at three parameter values, as computed through the FE (\emph {top row}) and POD-NN (\emph {bottom row}) method. For each configuration, the Reynold's number is $400$.\relax }{figure.caption.49}{}}
\@writefile{toc}{\contentsline {chapter}{Conclusion}{97}{chapter*.50}}
\@writefile{toc}{\contentsline {chapter}{Acknowledgements / Ringraziamenti}{99}{chapter*.51}}
\bibcite{Ams10}{1}
\bibcite{Bal14}{2}
\bibcite{Bar04}{3}
\bibcite{BNR00}{4}
\bibcite{Ben04}{5}
\bibcite{Bro93}{6}
\bibcite{Buf12}{7}
\bibcite{Bur06}{8}
\bibcite{Cas15}{9}
\bibcite{Cha10}{10}
\bibcite{CR97}{11}
\bibcite{Chen17}{12}
\bibcite{Cyb88}{13}
\bibcite{Cyb89}{14}
\bibcite{Deb78}{15}
\bibcite{Dep08}{16}
\bibcite{Dho14}{17}
\bibcite{Eft08}{18}
\bibcite{ESW04}{19}
\bibcite{Fah88}{20}
\bibcite{GMW81}{21}
\bibcite{Hag94}{22}
\bibcite{Hag14}{23}
\bibcite{Haa13}{24}
\bibcite{Hay05}{25}
\bibcite{Heb49}{26}
\bibcite{Lia02}{27}
\bibcite{OBS}{28}
\bibcite{HSR16}{29}
\bibcite{HSZ14}{30}
\bibcite{Hop82}{31}
\bibcite{Imam08}{32}
\bibcite{JIR14}{33}
\bibcite{KLM96}{34}
\bibcite{Koh95}{35}
\bibcite{Koh98}{36}
\bibcite{Kri07}{37}
\bibcite{LeM10}{38}
\bibcite{LM67}{39}
\bibcite{Mad06}{40}
\bibcite{Mar63}{41}
\bibcite{Mat16}{42}
\bibcite{MM10}{43}
\bibcite{MN16}{44}
\bibcite{Mol93}{45}
\bibcite{MR86}{46}
\bibcite{Nie15}{47}
\bibcite{NMA15}{48}
\bibcite{Per02}{49}
\bibcite{Pru02}{50}
\bibcite{Qua10}{51}
\bibcite{QMN15}{52}
\bibcite{Ran99}{53}
\bibcite{RB93}{54}
\bibcite{Ros58}{55}
\bibcite{Rud64}{56}
\bibcite{SD13}{57}
\bibcite{Vol08}{58}
\bibcite{WH60}{59}
